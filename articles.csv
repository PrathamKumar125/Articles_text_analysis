URL_ID,URL,Article_Title,Article
bctech2011,https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/,ML and AI-based insurance premium model to predict premium to be charged by the insurance company,"

Client Background
Client: A leading insurance firm worldwide
Industry Type: BFSI
Products & Services: Insurance
Organization Size: 10000+
The Problem
The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors.
The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape.
Key Challenges:

Data Complexity: The relevant data for predicting insurance premiums in this context is multifaceted, involving financial data, legal precedents, market trends, and individual directorial histories. Integrating and interpreting this diverse set of data poses a significant challenge.
Dynamic Risk Factors: The risk factors influencing insider trading public lawsuits are dynamic and subject to rapid changes. The models must be capable of adapting to evolving market conditions, legal landscapes, and individual company dynamics.
Fairness and Ethics: Ensuring fairness in premium calculation is critical. The models should be designed to avoid biases and discriminatory practices, considering the diverse backgrounds and contexts of Public Company Directors.
Regulatory Compliance: The insurance industry is subject to regulatory frameworks that vary across jurisdictions. The developed models need to comply with these regulations while providing accurate and reliable predictions.
Interpretability: Transparency in model predictions is crucial, especially in an industry where decisions can have significant financial implications. Ensuring that the AI and ML models are interpretable and explainable is vital for gaining the trust of stakeholders.

Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm.
Blackcoffer Solution
To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions.
Data Collection and Preprocessing:

Financial Data:

Gather financial data related to the insured companies, including revenue, profit margins, and financial stability indicators.
Incorporate stock market data and trading patterns to capture potential insider trading signals.


Legal History:

Collect historical legal cases related to insider trading lawsuits, with a focus on outcomes and financial implications.
Integrate legal precedents to understand patterns and potential future risks.


Directorial Profiles:

Compile individual profiles for each Public Company Director, including their professional history, prior legal involvements, and any relevant affiliations.


Market Trends and Regulatory Changes:

Monitor market trends and regulatory changes affecting the insurance landscape.
Incorporate external data sources for real-time updates on legal and market conditions.



Feature Engineering:

Risk Factors:

Identify key risk factors contributing to the likelihood of insider trading allegations.
Develop features that encapsulate financial stability, market conditions, and individual directorial behaviors.


Sentiment Analysis:

Implement sentiment analysis on news articles and social media to gauge public perception and potential legal scrutiny.



Machine Learning Models:

Supervised Learning:

Employ supervised learning algorithms such as Random Forests, Gradient Boosting, or ensemble models.
Train the model on historical data with labeled outcomes related to insider trading lawsuits.


Anomaly Detection:

Implement anomaly detection techniques to identify unusual patterns that may indicate potential insider trading activities.



Dynamic Risk Assessment:

Real-Time Updates:

Design the model to continuously update with real-time data to adapt to evolving risk factors.
Implement a feedback loop to capture the impact of recent legal cases and market events.


Scenario Analysis:

Develop scenario analysis capabilities to assess the impact of hypothetical events on premium calculations.



Fairness and Transparency:

Fairness Metrics:

Integrate fairness metrics to ensure unbiased predictions across diverse directorial profiles.
Regularly audit and refine the model to address any identified biases.


Explainability:

Implement model explainability tools to provide clear insights into premium calculations.
Ensure transparency in how the model arrives at its predictions.



Model Integration and Deployment:

User-Friendly Interface:

Develop a user-friendly interface for underwriters to interact with the model.
Ensure seamless integration into the existing insurance company workflow.


API Integration:

Provide API endpoints for easy integration with existing insurance systems.



Monitoring and Maintenance:

Model Monitoring:

Implement continuous monitoring to detect model drift and performance degradation.
Regularly update the model with new data and retrain it to maintain accuracy.


Scalability:

Design the solution to scale horizontally to accommodate an increasing volume of data.



By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.

Solution Architecture Diagram

Data Collection and Integration:

Data Sources: Financial records, legal databases, directorial profiles, market data.
Integration Layer: ETL processes, SQL/NoSQL databases.


Feature Engineering:

Feature Selection and Engineering Module.


Machine Learning Models:

Model Training Module: Scikit-Learn, TensorFlow, or PyTorch.
Model Evaluation Component.


Dynamic Risk Assessment:

Real-Time Data Integration Component: Apache Kafka.
Scenario Analysis Module.


Fairness and Transparency:

Fairness Metrics Integration.
Explainability Module: SHAP or Lime.


Model Integration and Deployment:

API Layer: RESTful API.
User Interface (UI).
Documentation for Integration.


Monitoring and Maintenance:

Monitoring Dashboard: Prometheus, Grafana.
Automated Model Update Pipeline: CI/CD.


General Documentation:

Model Architecture Document.
Technical User Manual.


Compliance Documentation:

Regulatory Compliance Report.
Data Privacy and Security Documentation.


Post-Implementation Support:

Support and Maintenance Plan.


Training and Knowledge Transfer:

Training Sessions.
Knowledge Transfer Documentation.


Scalability and Future-Proofing:

Scalable Infrastructure.
Flexibility for Future Enhancements.




Tools & Technology Used By Blackcoffer
Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits:
Data Collection and Preprocessing:

Python: A versatile programming language commonly used for data manipulation and preprocessing.
Pandas: A Python library for data manipulation and analysis, useful for handling structured data.
NumPy: A library for numerical operations in Python, often used for efficient array operations.
SQL/NoSQL Databases: To store and retrieve structured and unstructured data efficiently.

Feature Engineering:

Scikit-Learn: A machine learning library in Python that includes tools for feature extraction and preprocessing.
NLTK (Natural Language Toolkit): For processing and analyzing textual data, particularly for sentiment analysis.

Machine Learning Models:

Scikit-Learn: Provides various machine learning algorithms for classification tasks, including Random Forests and Gradient Boosting.
XGBoost or LightGBM: Powerful gradient boosting frameworks for improved predictive performance.
TensorFlow or PyTorch: Deep learning frameworks for building and training neural networks if the complexity of the model demands it.

Dynamic Risk Assessment:

Apache Kafka or RabbitMQ: Message brokers to facilitate real-time data streaming and updates.
Airflow: A platform to programmatically author, schedule, and monitor workflows, useful for scheduling model updates.

Fairness and Transparency:

Aequitas or Fairness Indicators: Libraries for assessing and mitigating bias in machine learning models.
SHAP (SHapley Additive exPlanations): An algorithm for model interpretability.

Model Integration and Deployment:

Flask or Django: Web frameworks for building the model deployment API.
Docker: Containerization tool for packaging the model and its dependencies.
Kubernetes: Container orchestration for deploying and managing containerized applications at scale.
RESTful API: For communication between the model and other components in the insurance company’s infrastructure.

Monitoring and Maintenance:

Prometheus: An open-source monitoring and alerting toolkit.
Grafana: A platform for monitoring and observability with beautiful, customizable dashboards.
Jenkins or GitLab CI/CD: Continuous integration and continuous deployment tools for automating model updates and deployment.
MLflow: An open-source platform to manage the end-to-end machine learning lifecycle.

General Development Environment:

Jupyter Notebooks: Interactive computing environment for exploratory data analysis and model development.
Git: Version control system for collaborative development.
VS Code or PyCharm: Integrated development environments (IDEs) for coding and debugging.

It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies.
Blackcoffer Deliverables
The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables:
1. Project Documentation:
1.1 Project Proposal:

Clearly outlines the objectives, scope, and methodology of the premium prediction model.

1.2 Requirements Document:

Specifies the functional and non-functional requirements of the model, considering the insurance company’s needs and regulatory compliance.

2. Data Collection and Preprocessing:
2.1 Data Collection Report:

Details the sources and types of data gathered, including financial records, legal cases, and directorial profiles.

2.2 Cleaned and Preprocessed Dataset:

A structured dataset ready for model training, containing relevant features and properly handled missing or inconsistent data.

3. Feature Engineering:
3.1 Feature Selection and Engineering Report:

Documents the process of selecting and creating features, highlighting their relevance to the prediction task.

4. Machine Learning Models:
4.1 Trained ML Models:

Includes the serialized models trained on historical data, such as Random Forests, Gradient Boosting, or other chosen algorithms.

4.2 Model Evaluation Report:

Evaluates the performance of the models on validation and test datasets, including metrics like accuracy, precision, recall, and F1-score.

5. Dynamic Risk Assessment:
5.1 Real-Time Integration Component:

Code or module that integrates real-time data for dynamic risk assessment.

5.2 Scenario Analysis Module:

Component allowing the assessment of premium changes based on hypothetical scenarios.

6. Fairness and Transparency:
6.1 Fairness Assessment Report:

Evaluates and mitigates bias, documenting fairness metrics and any adjustments made.

6.2 Explainability Module:

Implementation of tools or methodologies for model interpretability and explanation.

7. Model Integration and Deployment:
7.1 Deployed API:

RESTful API endpoint for seamless integration into the insurance company’s systems.

7.2 User Interface (UI):

User-friendly interface for underwriters to interact with the model, providing insights and entering necessary information.

7.3 Documentation for Integration:

Comprehensive guide on integrating the model into the existing workflow, including API documentation.

8. Monitoring and Maintenance:
8.1 Monitoring Dashboard:

Visual representation of key metrics and alerts for model performance, developed using tools like Grafana.

8.2 Automated Model Update Pipeline:

CI/CD pipeline or automated process for updating and retraining the model with new data.

9. General Documentation:
9.1 Model Architecture Document:

Detailed explanation of the model’s architecture, including components and their interactions.

9.2 Technical User Manual:

Documentation guiding technical users on deploying, maintaining, and troubleshooting the model.

10. Training and Knowledge Transfer:
10.1 Training Sessions:

Conducted for the insurance company’s staff, including underwriters and IT personnel, to ensure effective use and understanding of the model.

10.2 Knowledge Transfer Documentation:

Detailed documentation covering model usage, maintenance procedures, and troubleshooting tips.

11. Compliance Documentation:
11.1 Regulatory Compliance Report:

Ensures that the model adheres to relevant insurance regulations in the USA.

11.2 Data Privacy and Security Documentation:

Outlines measures taken to ensure the privacy and security of sensitive data.

12. Post-Implementation Support:
12.1 Support and Maintenance Plan:

Document outlining the support and maintenance plan for the model post-implementation, including response times and escalation procedures.

By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model.
Business Impacts
The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts:
1. Improved Accuracy and Risk Assessment:

Impact: Enhanced accuracy in predicting premiums based on advanced data analysis and machine learning algorithms.
Benefit: Better risk assessment leads to more precise premium calculations, reducing the likelihood of underpricing or overpricing policies.

2. Increased Competitiveness:

Impact: Utilizing cutting-edge technology to provide more accurate and dynamic premium predictions.
Benefit: Positions the insurance firm as a leader in the market, attracting more clients seeking innovative and reliable insurance solutions.

3. Tailored Coverage and Pricing:

Impact: Customizing coverage and premiums based on individual directorial profiles and evolving risk factors.
Benefit: Attracts clients with diverse risk profiles, offering tailored solutions that align with their specific needs.

4. Faster Decision-Making:

Impact: Automation of premium calculations and decision-making processes.
Benefit: Speeds up underwriting processes, enabling quicker responses to client inquiries and facilitating faster policy issuance.

5. Reduced Operational Costs:

Impact: Automation of routine tasks related to premium calculation and risk assessment.
Benefit: Decreases manual workload, leading to operational efficiency and cost savings.

6. Real-Time Adaptation to Market Changes:

Impact: Integration of real-time data for dynamic risk assessment.
Benefit: Enables the insurance firm to adapt quickly to changes in market conditions, ensuring that premiums remain reflective of current risk landscapes.

7. Enhanced Customer Satisfaction:

Impact: Accurate pricing, fair premium calculations, and transparent communication.
Benefit: Increases customer satisfaction by providing a reliable and customer-centric insurance experience.

8. Mitigation of Regulatory Risks:

Impact: Implementation of a solution that complies with insurance regulations and industry standards.
Benefit: Reduces the risk of regulatory non-compliance, protecting the firm from legal and financial repercussions.

9. Data-Driven Decision-Making:

Impact: Utilizing data-driven insights for decision-making processes.
Benefit: Empowers the firm’s leadership with actionable insights, contributing to strategic decision-making and business planning.

10. Brand Reputation and Trust:

Impact: Adoption of fairness-aware and transparent AI models.
Benefit: Builds trust among clients and stakeholders by demonstrating a commitment to fairness, transparency, and ethical AI practices.

11. Risk Mitigation for Clients:

Impact: Providing insurance coverage that reflects the evolving nature of insider trading public lawsuits.
Benefit: Assists Public Company Directors in mitigating financial risks associated with legal actions, enhancing the value proposition for clients.

12. Scalability and Future-Proofing:

Impact: Designing the solution to scale and adapt to future industry developments.
Benefit: Ensures the longevity and relevance of the insurance firm’s technology infrastructure in the face of evolving business and technological landscapes.

13. Revenue Growth:

Impact: Attracting a larger customer base and retaining existing clients through innovative and accurate insurance solutions.
Benefit: Contributes to revenue growth by expanding the firm’s market share and increasing customer loyalty.

By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA.

Summarize
Summarized: https://blackcoffer.com/
This project was done by Blackcoffer Team, a Global IT Consulting firm.

Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleRise of cybercrime and its effect by the year 2040.Next articleDatabase Discovery Tool using OpenAI Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2012,https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/,Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application,"

Client Background
Client: A leading fintech firm in the USA
Industry Type: Finance
Products & Services: Trading, Banking, Financing
Organization Size: 100+
The Problem

Integrating the Interactive Brokers API with Python.
Creating a user-friendly desktop application interface.
Managing concurrent processes and threads.
Developing the margin calculator with accurate calculations.
Handling data synchronization between TWS and the application.
Ensuring security and authentication for TWS access.
Providing real-time market data to users.
Maintaining a responsive and reliable application.
Resolving any potential compatibility issues.
Ensuring thorough documentation for users

Our Solution

Leverage Interactive Brokers API documentation and libraries.
Design an intuitive and responsive PyQT5-based desktop UI.
Implement threading and preprocessing for concurrent tasks.
Develop a robust margin calculator algorithm.
Use data synchronization mechanisms provided by TWS.
Implement secure authentication for TWS access.
Utilize the Interactive Brokers API for real-time market data.
Conduct extensive testing and quality assurance.
Address compatibility issues through rigorous testing.
Document every aspect of the project for users and developers.

Solution Architecture

Interactive Brokers API for live data and trading access.
Python-based server using Django for APIs and data storage.
PyQT5-based desktop application for trading dashboard.
PostgreSQL database for storing relevant data.
Threading and concurrency management for parallel processes.
Margin calculator component within the desktop app.
Integration with Trader Workstation (TWS).
Real-time market data feeds from TWS.
Responsive front-end using Bootstrap, HTML, and CSS.
Detailed documentation for users and developers.

Deliverables

Project Github Source Code : https://github.com/AjayBidyarthy/Sunil-Misir 

Tech Stack

Tools used

Requests
Threading and Multiprocessing
PyQT5


Language/techniques used

Python


Models used

Django ORM


Skills used

Python
Python Django 
Python Django REST Framework
PyQT5
MultiThreading and MultiProcessing


Databases used

POstgresql


Web Cloud Servers used

None



What are the technical Challenges Faced during Project Execution

Complex integration with the Interactive Brokers API.
Designing an efficient and user-friendly desktop interface.
Coordinating and managing multiple concurrent threads and processes.
Accurate implementation of the margin calculator.
Ensuring real-time data synchronization with TWS.
Handling authentication and security for TWS access.
Providing timely and reliable market data.
Resolving compatibility issues on various user machines.
Optimizing performance for a responsive application.
Documenting every aspect comprehensively.

How the Technical Challenges were Solved

Extensive research and consultation of Interactive Brokers API documentation.
User-centered design principles for the desktop interface.
Thorough testing and debugging of multi-threading scenarios.
Careful design and testing of margin calculation algorithms.
Regular data synchronization checks with TWS.
Implementation of secure authentication protocols.
Utilization of Interactive Brokers’ data streaming features.
Compatibility testing on various configurations.
Profiling and optimization of code for responsiveness.
Comprehensive documentation created throughout the development process.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEfficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application DeploymentNext articleEfficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2013,https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/,Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT
Products & Services: IT Consulting
Organization Size: 100+
The Problem

Data Complexity: Handling and integrating multiple data sources with different formats and cleaning/preprocessing them for use in a web application.
Spatial Data Integration: Managing and converting complex spatial data into a suitable format for storage and display.
User-Friendly Data Access: Providing an easy-to-use interface for users to query and visualize data efficiently.
Secure Authentication: Implementing secure user authentication to protect sensitive data and user accounts.
Deployment Considerations: Exploring the potential challenges of deploying the application on Azure.

Our Solution

Project Setup and ETL: Set up Django, developed ETL scripts, cleaned data, and loaded it into PostgreSQL.
Web Application Development: Designed user-friendly templates, implemented APIs for data display, and used session storage for queries.
User Authentication: Created login/signup pages and implemented secure user authentication.
Data Management and Integration: Ensured dynamic tables and error handling for queries, created Docker image, and documented deployment.
Spatial Data Handling: Processed and stored spatial data, integrated it with Django views, and converted data types.
API Development: Built APIs for JSON data retrieval and handled various file extensions for data extraction.
Frontend and User Interaction: Designed frontend components and implemented data upload and retrieval.
SQL Dump and Azure Deployment: Created SQL Dump template, developed a view for uploading .sql files, and explored Azure deployment options.

Solution Architecture

Backend Framework: Python Django for building the web application’s backend.
Database: PostgreSQL for storing cleaned and spatial data.
ETL Processes: Python scripts for data extraction, transformation, and loading.
Frontend: HTML templates and JavaScript for user interaction.
APIs: Custom APIs for data retrieval and spatial data handling.
Deployment: Dockerization for containerized deployment.
Authentication: Implementing user authentication using Django’s built-in features.
Spatial Data Handling: Using Python libraries to process and convert spatial data.
SQL Dump: Creating an SQL Dump feature for running PostgreSQL queries.

Deliverables

Project Resouces well be access via github Only
Github Link : https://github.com/AjayBidyarthy/Sheeban-Wasi-Full-stack.git 

Tech Stack

Tools used

Pillow
psycopg2
arcgis==1.8.2
geopandas
pyproj
pandas
numpy
matplotlib
pyshp


Language/techniques used

Python


Models used

Django ORM


Skills used

Python 
Django
ETL
Docker


Databases used

postgresql


Web Cloud Servers used

MS Azure



What are the technical Challenges Faced during Project Execution

Data Cleaning and Integration: Managing data from different sources and ensuring consistency was challenging.
Spatial Data Transformation: Converting complex spatial data into suitable database formats posed a technical hurdle.
User Authentication: Implementing secure user authentication without vulnerabilities required careful consideration.
File Handling: Handling various file extensions and extracting data from them was a technical challenge.
Deployment: Ensuring smooth deployment, especially on Azure, presented its own set of challenges.

How the Technical Challenges were Solved

Data Cleaning and Integration: Python scripts were used to clean and preprocess data, aligning it with column datatypes.
Spatial Data Transformation: Libraries were utilized to process and convert spatial data to appropriate formats.
User Authentication: Django’s built-in authentication features were leveraged for secure user management.
File Handling: Custom Python scripts were developed to handle different file extensions and extract data.
Deployment: Dockerization simplified deployment, and research on Azure ensured potential future deployment options were explored.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEffective Management of Social Media Data Extraction: Strategies for Authentication, Security, and ReliabilityNext articleStreamlined Integration: Interactive Brokers API with Python for Desktop Trading Application Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2014,https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/,"Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability","

Client Background
Client: A leading tech firm in the USA
Industry Type: IT
Products & Services: Consulting, Product & Services
Organization Size: 100+
The Problem

Handling complex authentication mechanisms for social media platforms.
Efficiently extracting data from social media profiles.
Preventing IP blocking and ensuring API reliability.
Managing and storing extracted data securely.
Abiding by social media platform policies and avoiding legal issues.
Handling rate limiting and throttling.
Providing comprehensive and up-to-date documentation.
Dealing with changes in social media platform APIs.
Optimizing API performance for rapid response.
Ensuring user privacy and data protection.

Our Solution

Implement OAuth2 or API tokens for authentication.
Utilize web scraping libraries like BeautifulSoup and Scrapy.
Employ proxy rotation and request throttling.
Use databases like MongoDB or AWS S3 for data storage.
Regularly check and update API usage against platform policies.
Implement rate limiting and queue-based processing.
Maintain versioned API documentation.
Monitor platform API changes and adapt accordingly.
Optimize code and database queries for performance.
Encrypt sensitive data and follow data protection regulations.

Solution Architecture

Authentication layer for social media logins.
API endpoints for data extraction.
Web scraping components for profile details.
Throttling and rate-limiting mechanisms.
Data storage and caching layers.
Documentation portal for API users.
Monitoring and logging infrastructure.
Error handling and alerting mechanisms.
Compliance checks and privacy safeguards.
Load balancers and auto-scaling for API servers.

Deliverables

Project Github Source Code

Tech Stack

Tools used

BeautifulSoup
Requests
Django rest Framework


Language/techniques used

Python


Models used

Django ORM


Skills used

Python
WebScraping
Python Django 
Python Django REST Framework


Databases used

SQLite Database


Web Cloud Servers used

None



What are the technical Challenges Faced during Project Execution

Frequent changes and updates to social media APIs.
Evolving security and authentication requirements.
Handling CAPTCHAs and bot detection mechanisms.
Maintaining data consistency and accuracy.
Adhering to rate limits and avoiding IP blocks.
Scaling the infrastructure to accommodate increased usage.
Dealing with diverse data formats from different platforms.
Ensuring privacy and compliance with data protection laws.
Balancing performance and cost-effectiveness.
Handling user-specific customizations and options.

How the Technical Challenges were Solved

Regularly monitoring and adapting to API changes.
Implementing robust authentication strategies.
Using CAPTCHA solving services when necessary.
Implementing data validation and cleansing routines.
Employing IP rotation and rate limiting strategies.
Utilizing cloud-based auto-scaling solutions.
Developing data parsers for various formats.
Implementing encryption and anonymization techniques.
Profiling and optimizing code for performance.
Providing configurable options for users to customize their data extraction.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleStreamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and MonitoringNext articleEfficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2015,https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/,Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring,"

Client Background
Client: A leading fintech firm in the USA
Industry Type: Finance
Products & Services: Trading, Investment, Financing
Organization Size: 100+
The Problem

Trading Operations Interface: The project aims to create a Windows-based Display Application that provides an intuitive interface for managing trading activities in MetaTrader 4 (MT4).
EA Control and Monitoring: Users need a tool to interact with and monitor the EA running in MT4, which follows predefined rules for trading.
Hedging and Configuration: The application should allow users to hedge positions, configure trading settings, close orders manually, and add new orders.
Real-time Monitoring: Real-time monitoring and control of trading activities are crucial for efficient trading.
EA Functionality: The specific functionality and rules of the EA will be defined based on pricing and requirements.

Our Solution

Display Application: Developed a Windows-based application for trading operations management, order placement, and monitoring.
EA Interaction: Created a loosely coupled system where the Display Application can control and influence the EA running on MT4.
Functionality: Implemented order placement, hedging, settings configuration, order closing, and real-time monitoring features.
Dynamic EA: The EA’s specific rules and functionality are determined based on pricing and requirements.
Communication: Established a mechanism for the Display Application to communicate with MT4, facilitating trading instructions and updates.

Solution Architecture

UI Development: UI development using Python libraries such as Kivy and Tkinter for the Windows-based application.
VPS with MT4: The client operates a Virtual Private Server (VPS) with MT4 running.
Expert Advisor (EA): An EA on MT4 executes trading operations based on predefined rules.
Communication: A mechanism for the Display Application to communicate with MT4, possibly through an API or other methods.
Dynamic EA Parameters: The exact rules and functionality of the EA will be determined based on pricing and client requirements.

Deliverables

Project Code can be accessed via this github link : https://github.com/AjayBidyarthy/Patrick-Oliveri-Applcation.git
Since, this is private Git Reporsitory , User will need permission to clone it.

Tech Stack

Tools used

TKinter
Kivy


Language/techniques used

Python


Models used

No Model Used


Skills used

Python Kivy
Python TKinter


Databases used

No Db Used


Web Cloud Servers used

No Web Services Used



What are the technical Challenges Faced during Project Execution

UI Responsiveness: Challenges in achieving responsive UI in Python libraries like Kivy and Tkinter.
Integration with MT4: Ensuring effective communication between the Display Application and MT4.
Dynamic EA Rules: Defining and integrating dynamic rules for the EA based on user requirements.
Deployment: Preparing for potential deployment but no deployment has occurred yet.
Version Control: Managing code changes and documentation using Git.

How the Technical Challenges were Solved

UI Responsiveness: The project transitioned to seek C or C# development for better responsiveness and flexibility.
Integration with MT4: A communication mechanism, possibly an API, was explored to facilitate communication between the Display Application and MT4.
Dynamic EA Rules: The exact rules for the EA were to be determined based on client requirements and pricing, ensuring flexibility.
Deployment: Deployment has not occurred yet, and it may be addressed in the future.
Version Control: Git was used to manage code changes and documentation, ensuring version control and collaboration.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEfficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and ComplianceNext articleEffective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2016,https://insights.blackcoffer.com/efficient-aws-infrastructure-setup-and-management-addressing-security-scalability-and-compliance/,"Efficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance","

Client Background
Client: A leading Consulting firm in the USA
Industry Type: IT
Products & Services: IT Consulting
Organization Size: 1000+
The Problem

Setting up and configuring AWS services.
Designing an efficient database schema.
Integrating email and calling services securely.
Ensuring data privacy and compliance.
Handling system scalability.
Managing user authentication and authorization.
Monitoring and logging system activities.
Implementing backup and recovery strategies.
Debugging and troubleshooting issues.
Balancing cost and performance.

Our Solution

Utilize AWS CloudFormation or AWS CDK for infrastructure as code.
Normalize the database schema to minimize redundancy.
Implement OAuth or JWT for secure authentication.
Encrypt data at rest and in transit.
Use AWS Auto Scaling to handle increased traffic.
Set up AWS CloudWatch for monitoring and AWS CloudTrail for auditing.
Regularly backup data to Amazon S3.
Implement comprehensive error handling and logs.
Perform unit, integration, and load testing.
Optimize AWS resource usage through cost analysis.

Solution Architecture

AWS RDS for customer and employee data storage.
AWS Lambda functions for processing calls and emails.
AWS SES and SNS for sending emails and notifications.
Amazon S3 for storing backups and static assets.
AWS Cognito for user authentication.
AWS API Gateway for managing APIs.
AWS CloudWatch and CloudTrail for monitoring and auditing.
AWS Auto Scaling for handling variable workloads.
Python codebase for application logic.
Implementing security groups and VPC for network isolation.

Deliverables

Project Github Source Code

Tech Stack

Tools used

Requests
Boto3


Language/techniques used

Python


Models used

None


Skills used

Python
AWS 


Databases used

AWS RDS


Web Cloud Servers used

Amazon Web Services (AWS)



What are the technical Challenges Faced during Project Execution

Integrating multiple AWS services.
Designing a scalable database schema.
Ensuring data security and compliance.
Handling complex user authentication and authorization.
Managing API versioning and changes.
Optimizing cost and resource usage.
Debugging and resolving performance issues.
Maintaining high availability and reliability.
Handling data synchronization between tiers.
Adapting to evolving AWS services and best practices.

How the Technical Challenges were Solved

Extensive research and leveraging AWS documentation and support.
Collaboration with experienced database architects.
Thorough security audits and compliance checks.
Implementing OAuth and fine-grained access control.
Clear versioning and documentation for APIs.
Regular cost analysis and optimization efforts.
Profiling and performance tuning of critical components.
Implementing redundancy and failover mechanisms.
Developing data synchronization algorithms.
Continuous learning and adaptation to AWS updates and community insights.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleStreamlined Equity Waterfall Calculation and Deal Management SystemNext articleStreamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2017,https://insights.blackcoffer.com/streamlined-equity-waterfall-calculation-and-deal-management-system/,Streamlined Equity Waterfall Calculation and Deal Management System,"

Client Background
Client: A leading real estate firm in the USA
Industry Type: Real Estate
Products & Services: Real Estate, Construction, Financing
Organization Size: 100+
The Problem

Calculating equity waterfalls based on CSV data.
Implementing different user roles and their permissions.
Creating a user-friendly dashboard for each user type.
Managing deal creation, invitations, and subscriptions.
Handling user invitations and registration.
Copying deals while preserving specific data.

Our Solution

Develop Python code to calculate equity waterfalls.
Implement role-based access control for admin, sponsors, and investors.
Create distinct dashboards with relevant data using ReactJS.
Design intuitive UI for deal management.
Develop invitation mechanisms and registration flows.
Implement copying deals with proper data handling.

Solution Architecture

Backend built with Django for handling data, authentication, and API endpoints.
Frontend developed using ReactJS for user interfaces.
SQLite database for data storage.
Google Cloud for application deployment.

Deliverables

Project Github Source Code : 

Tech Stack

Tools used

Pillow
Requests
GCP VM


Language/techniques used

Python
React JS 


Models used

Django ORM


Skills used

Python
Python Django 
Python Django REST Framework


Databases used

SQLite3 database


Web Cloud Servers used

GCP



What are the technical Challenges Faced during Project Execution

Equity waterfall calculations based on dynamic CSV data.
Managing user permissions and access control.
Designing and implementing complex user registration and invitation flows.
Copying deals while maintaining data integrity.
Ensuring data consistency and security.

How the Technical Challenges were Solved

Developed Python scripts to parse CSV files and perform required calculations.
Utilized Django’s built-in authentication system and implemented role-based permissions.
Designed clear and user-friendly registration and invitation processes.
Implemented a controlled deal copying mechanism.
Conducted thorough testing and used encryption for data security.

Project website url : https://stackshares.io/ 
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleAutomated Orthopedic Case Report Generation: Harnessing Web Scraping and AI IntegrationNext articleEfficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2018,https://insights.blackcoffer.com/automated-orthopedic-case-report-generation-harnessing-web-scraping-and-ai-integration/,Automated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration,"

Client Background
Client: A leading health-tech firm in the USA
Industry Type: Healthcare
Products & Services: Medical solutions, healthcare
Organization Size: 100+
The Problem

The problem is to efficiently create orthopedic case reports by extracting data from online sources, including articles, videos, and user comments.
It involves summarizing and citing relevant articles from PubMed.gov for the past 5 years related to the case.
This requires automating the extraction and summarization of data from websites, making it a time-consuming task if done manually.

Our Solution

Develops a Python tool that accepts a website URL as input and generates a case report.
Integrates web scraping to extract data from websites.
Utilizes AI, such as ChatGPT, for creating summaries and responses.
Leverages PubMed for citing and summarizing recent articles.
Provides a web application for user-friendly access to these capabilities.

Solution Architecture

Utilizes web scraping techniques to gather data from trusted medical websites.
Combines web scraping with AI, including ChatGPT, for generating case reports and responding to queries.
Utilizes PubMed for retrieving and summarizing recent articles related to the case.
Deploys a web application for user interaction and input.

Deliverables

Project Github Source Code

Tech Stack

Tools used

ChatGPT
BeautifulSoup
Requests


Language/techniques used

Python


Models used

None


Skills used

Python
WebScraping
ChatGPT prompting


Databases used

None


Web Cloud Servers used

None



What are the technical Challenges Faced during Project Execution

Accurate and reliable web scraping from diverse medical websites.
Integration of AI components for text generation and summarization.
Efficient querying and retrieval of articles from PubMed.
Handling different data formats and structures from various online sources.
Developing a user-friendly web interface for input and interaction.

How the Technical Challenges were Solved

Extensive research and testing of web scraping techniques for medical websites.
Integration of AI models and libraries for text generation.
Utilization of PubMed API for article retrieval and summarization.
Custom data parsers for handling diverse data structures.
Collaboration with medical experts for user interface design and feedback.

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleStreamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery IntegrationNext articleStreamlined Equity Waterfall Calculation and Deal Management System Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2019,https://insights.blackcoffer.com/streamlining-time-calculation-in-warehouse-management-leveraging-shiphero-api-and-google-bigquery-integration/,Streamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration,"

Client Background
Client: A leading retail firm in the USA
Industry Type: Retail
Products & Services: Retail Solutions, Supply Chain, Warehouse Management
Organization Size: 100+
The Problem

The problem was to efficiently calculate the time taken by each personnel on their shifts in a warehouse management system.
Data needed to be extracted from ShipHero API and processed to generate meaningful insights.
There was a need for a web interface to provide user-friendly access to the data and allow for data filtering.
There is a mapping issue in Python Script which occurred in December of 2022. Maybe due to the addition of another warehouse. This is an open issue and ShipHero is unable to provide any reliable solution for the same. [Issue has been highlighted below in the section of ‘’Known Issues’’]

Our Solution

Creating an API to Google BigQuery using a Python script deployed on Google Cloud.
The Python script automated data extraction from ShipHero API, transformation, and loading into Google BigQuery.
Google Data Studio was used to create a dashboard for reporting and visualization.

Solution Architecture

The solution involved two main components: a Python script and a web interface (Web App).
The Python script utilized ShipHero API to fetch data and calculate personnel shift times. It then stored the processed data in Google BigQuery.
The web interface allowed users to log in, apply filters to data tables fetched from BigQuery, and visualize the data.
Google Cloud services were used for hosting the Python script and deploying the web app.

Deliverables
[GitHub Repositories URL:

https://github.com/AjayBidyarthy/Jake-Brenner-API-to-google-big-query-to-google-data-studio.
https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/himanshu
https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/master

Tech Stack

Tools used

Google API
Beautifulsoup
Numpy and Pandas


Language/techniques used

Python 
React JS


Models used

Django ORM Model


Skills used

Python 
Python Django
React JS


Databases used

GCP BigQuery Database


Web Cloud Servers used

Google Cloud Platform (GCP)



What are the technical Challenges Faced during Project Execution

Accessing and understanding ShipHero API endpoints and data structures.
Developing and deploying the Python script to run daily on Google Cloud Scheduler.
Integrating and linking databases effectively.
Handling and automating complex data manipulation and calculations.

How the Technical Challenges were Solved

Comprehensive research and analysis of the ShipHero API and its endpoints.
The Python script was developed to handle data extraction, transformation, and loading tasks efficiently.
Google Cloud services were used to automate the script and schedule daily runs.
Collaboration and communication with the client to ensure the API data met the dashboard requirements.

Project website url
http://app.shiphero.com/
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEfficient Database Design and Management: Streamlining Access and Integration for Partner Entity ManagementNext articleAutomated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2020,https://insights.blackcoffer.com/efficient-database-design-and-management-streamlining-access-and-integration-for-partner-entity-management/,Efficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management,"

Client Background
Client: A leading IT firm in the Europe
Industry Type: IT
Products & Services: IT Services, Consulting and Automation
Organization Size: 100+
The Problem

Database designing which enables access to each related/important table data via other db table
The project required the development of a user-friendly web application for managing partner entities with diverse attributes. 
Ensuring data accuracy, security, scalability, and compliance with regulations while integrating seamlessly with a Data Warehouse posed significant technical challenges.

Our Solution

Our solution successfully addressed the technical challenges by leveraging Django’s capabilities and implementing custom solutions where needed. 
It provided a robust and scalable web application for partner entity management while ensuring data accuracy, security, and compliance. 
The dynamic attribute management system and integration with the Database facilitated efficient data handling and reporting, supporting data-driven decision-making. 
We have designed and implemented database related changes and UI related changes that Client have suggested according to which a separate db table which contains all data which will be created , updated and deleted as other table rows created, updated and deleted

Solution Architecture

Django ORM for abstracting database complexities.
Scalability through cloud resources and optimization techniques.
Security measures, including encryption and access controls for Admin Users.
Performance optimization strategies such as removing redundancy in db tables.
We have provided many database design solutions as well as User Interface solution regarding which client have given positive response.
We have successfully developed and implemented design and changes related to project after multiple discussion with client regarding database architecture design as well as database model and their related UI panel with authentication 

Deliverables

Python Django Source Code (Github Repository)

Tech Stack

Tools used

Python Django web Framework


Language/techniques used

Python 


Models used

Django Database Model and Django ORM


Skills used

Python 
Django 


Databases used

Postgresql


Web Cloud Servers used

Not Used from Side



What are the technical Challenges Faced during Project Execution

Database Complexity: Designing a comprehensive database schema to represent multiple partner entities with varying attributes posed a challenge. Each entity had unique characteristics and relationships.
Scalability: Ensuring the application’s scalability to handle a potentially large volume of partner data while maintaining performance was a significant concern.
Dynamic Attributes: Allowing users to dynamically manage entity attributes presented difficulties in database design and user interface implementation.
Data Validation: Implementing robust data validation rules to maintain data accuracy and consistency across various partner entities was complex due to the diversity of data.
Integration with Remote Database: Establishing seamless data export capabilities to feed the Database while maintaining data compatibility was a technical hurdle.
Security: Ensuring data security and compliance with relevant regulations, including encryption and access control, required careful consideration and implementation.
Performance Optimization: Optimizing the application’s performance, especially when dealing with complex queries and large datasets, was a continual challenge. 

How the Technical Challenges were Solved

Database Abstraction: Utilizing Django’s ORM (Object-Relational Mapping) allowed for an abstract representation of entities and their attributes, simplifying database management.
Scalability Planning: Employing efficient indexing and caching mechanisms to accommodate scalability and performance needs. Additionally, using cloud resources for scalability.
User Management: Implementing a flexible User management system that allowed users to Create , Read , Update and Delete other Users and their related permissions.
Data Validation Middleware: Developing custom middleware to enforce data validation rules and ensure data accuracy before database interactions.
Integration Layer: Creating a dedicated integration layer that transformed and exported data from the database to the User Interface, adhering to data compatibility standards.
Security Best Practices: Adhering to best practices for securing data, including User Authentication, Changes in Django template to Remove Other Important Database in db options and Permission Required for other User to use database table on UI panel from Admin User.
Performance Tuning: Conducting performance tuning by optimizing database model and related admin file for better fetching of db table data on UI panel.

Project website url
http://34.18.45.30:8000/api/admin/login/?next=/api/admin/ 
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleAutomated Campaign Management System: A Comprehensive Solution with LinkedIn and Email IntegrationNext articleStreamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2021,https://insights.blackcoffer.com/automated-campaign-management-system-a-comprehensive-solution-with-linkedin-and-email-integration/,Automated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration,"

Client Background
Client: A leading marketing tech firm worldwide
Industry Type: Marketing
Products & Services: Ad Tech, Marketing Automation, Lead Management
Organization Size: 100+
The Problem

Integrating with LinkedIn and Email APIs for automation.
Building a user-friendly and responsive frontend interface.
Developing a robust backend code for campaign automation.
Ensuring secure user authentication and data exchange.
Managing campaign creation, scheduling, and tracking.
Handling data storage and organization in MongoDB.
Providing comprehensive documentation for users.
Ensuring scalability and reliability in cloud hosting.
Addressing security and privacy concerns.
Maintaining ongoing support and updates.

Our Solution

Leverage LinkedIn and Email API documentation and libraries.
Implement a responsive and intuitive frontend using React.js.
Develop backend code for campaign automation with Python or Node.js.
Utilize secure authentication mechanisms like JWT.
Create user-friendly campaign creation and management interfaces.
Store and manage campaign data efficiently in MongoDB.
Produce detailed documentation for installation, usage, and troubleshooting.
Employ AWS for scalable and reliable cloud hosting.
Implement encryption and privacy measures.
Establish a support and maintenance plan for ongoing updates.

Solution Architecture

Frontend built with React.js.
Backend using Python or Node.js.
MongoDB for data storage and management.
AWS for cloud hosting and scalability.
Integration with LinkedIn and Email APIs.
User authentication and authorization layers.
Campaign creation, scheduling, and tracking features.
Responsive and user-friendly web app interface.
Documentation portal for users and developers.
Security and privacy measures integrated throughout the architecture.

Tech Stack

Tools used

LinkedIn API
Gmail API
Google Account
Selenium
BeautifulSoup
requests


Language/techniques used

Python
React JS


Models used

Django ORM


Skills used

Python
WebScraping
React JS
Selenium
Django
Django rest framework


Databases used

MongoDb


Web Cloud Servers used

AWS



What are the technical Challenges Faced during Project Execution

Complex integration with LinkedIn and Email APIs.
Designing and implementing a responsive frontend.
Developing robust automation logic for campaigns.
Ensuring secure authentication and data exchange.
Managing large datasets and data organization in MongoDB.
Creating comprehensive and user-friendly documentation.
Scaling the application for cloud hosting.
Addressing security and privacy concerns.
Handling user support requests and bug fixes.
Keeping the application up to date with API changes.

How the Technical Challenges were Solved

Thoroughly studied LinkedIn and Email API documentation.
Used React.js and responsive design practices for the frontend.
Implemented efficient campaign automation logic.
Employed JWT for secure user authentication.
Optimized data storage and retrieval in MongoDB.
Prepared detailed documentation for users and developers.
Utilized AWS services for scalable hosting.
Implemented encryption and privacy measures.
Established a support system for user inquiries.
Maintained active monitoring of API changes and regular updates.

Project website url : 

Frontend : http://35.176.216.54:3000/
Backend : http://35.176.216.54:8000/ 

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleAI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwideNext articleEfficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2022,https://insights.blackcoffer.com/ai-driven-data-analysis-ai-tool-using-langchain-for-a-leading-real-estate-and-financing-firm-worldwide/,AI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide,"

Client Background
Client: A leading real estate and financing firm worldwide
Industry Type: Real Estate
Products & Services: Infrastructure Development, Financing, Real Estate
Organization Size: 10000+
The Problem
Creating a user-friendly data analysis tool capable of interpreting natural language queries and providing insightful analyses from CSV data. The tool should facilitate seamless interaction, enabling users to gain valuable insights without the need for technical expertise. Key functionalities should include data exploration, trend identification, pattern recognition, and anomaly detection, all presented in a comprehensible format. The tool must also ensure efficient handling of CSV datasets while maintaining accuracy and reliability in its analyses.
Our Solution


Data Ingestion and Conversion:

CSV data is acquired from a source (local file system, cloud storage, etc.).
The data is then converted into a pandas DataFrame using the read_csv() function or similar methods provided by the pandas library.

Data Cleaning:

Data Cleaning operations are performed on the dataframe so that it serves as an ideal input for Pandas Agent. These may include:
Column Data type conversion.
Handling Duplicates
Handling unnecessary columns, etc.

Initialization of Langchain’s Pandas Agent:

Langchain’s Pandas Agent is initialized with the necessary parameters. These parameters include:
System prompt: A custom prompt provided by the user or defined in the application.
Temperature: A parameter controlling the randomness of the model’s outputs.
Model: The specific model or model configuration to be used by the agent.
Other relevant parameters based on the requirements and capabilities of the agent.

Integration with Pandas DataFrame:

The DataFrame created in the previous step serves as input for the Pandas Agent. It contains the structured data which will serve as input for the Pandas Agent.

Natural Language Query Interpretation:

The user interacts with the system by posing queries in natural language.
Langchain’s Pandas Agent interprets these queries using GPT-4 backend and converts them into executable commands or operations on the DataFrame.

DataFrame Operations:

The Pandas Agent executes the operations needed on the DataFrame. These operations may include:
Filtering: Selecting rows or columns based on specified criteria.
Aggregation: Computing summary statistics or aggregating data based on groups.
Transformation: Modifying data in the DataFrame (e.g., adding or removing columns, changing data types).
Joining/Merging: Combining multiple DataFrames based on common keys or indices.
Sorting: Arranging rows or columns in a specified order.
Other pandas DataFrame operations as required by the user queries.

Delivery to End User:

The processed output is delivered to the end user through the streamlit user interface.
The user can review the insights provided by the system and further refine their queries if needed.
Solution Architecture

Deliverables
Data Analysis Tool with Streamlit frontend.
Tech Stack

Tools used
Langchain, OpenAI gpt-4 API
Language/techniques used
Python
Models used
Pandas Agent, GPT-4
Skills used
Python, Streamlit, Streamlit cloud deployment, Langchain
Web Cloud Servers used
Streamlit cloud

What are the technical Challenges Faced during Project Execution
To make the tool follow the Indian standards in terms of Financial Year Quarters, currency and human readable values instead of exponential values.
How the Technical Challenges were Solved
The challenge was solved by decreasing the temperature of Pandas agent to 0 and make a custom system prompt to introduce maximum bias approximating the desirable answers.
Business Impact
The user was able get data analysis insights without expertise in python, pandas and other tools used in the process of Data Analysis in a fraction of time compared to what it would have been if the process was done manually.
Project Snapshots

Frontend Streamlit Interface




IDE Environment


Project website url
URL: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/ (Non-Functional due to the expiry of OpenAI API Key)
Project Video
Link: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63

Important Links
Video Demo: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63
URL to test App: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/
Project Success Story: https://docs.google.com/document/d/17VZukkZW6LsXVmb6IDIZWpp61sRQY_cE/edit?usp=sharing&ouid=111848530990018600604&rtpof=true&sd=true
Solution Diagram: https://drive.google.com/file/d/16T56xrxBHioAIRnoA0EmHlSdMcmzEWP3/view?usp=sharing

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleGrafana Dashboard to visualize and analyze sensors’ dataNext articleAutomated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2023,https://insights.blackcoffer.com/grafana-dashboard-to-visualize-and-analyze-sensors-data/,Grafana Dashboard to visualize and analyze sensors’ data,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT
Products & Services: IT & Consulting, Software Development, DevOps
Organization Size: 100+
The Problem
The client requires a Grafana dashboard that can fetch data from a web API providing historical data of building automation systems. The dashboard needs to allow manual entry of a target URL for individual buildings, selection of a history name from a dropdown or search bar, selectable time range for displaying history data, and the ability to choose from various chart types for visualization. Additionally, the client wants to set up alarms for certain metrics like CPU, RAM, and hard disk usage. Each user should only be able to view their own STier API data, which is controlled by their IP.
Our Solution
To meet these requirements, we will set up a Grafana dashboard using the Grafana API. We will configure the dashboard to connect to the web API and fetch data based on the user’s input for the target URL, history name, and time range. For visualization, we will implement various chart types including Bar, Line, and Scatter plot charts. To set up alarms for specific metrics, we will utilize Grafana’s built-in alerting feature.
Solution Architecture

Deliverables

A fully functional Grafana dashboard connected to the web API
Ability to manually enter a target URL for individual buildings
Selection of history name from a dropdown or search bar
Selection of time range for displaying history data
Various chart types for data visualization
Setup of alarms for specific metrics

Tech Stack

Tools used
Python
Grafana
Grafana API
Web API for historical data of building automation systems
Language/techniques used
Javascript
SQL
Skills used
Data Visualization
API Integration
User Interface Design
Databases used
Grafana Database

What are the technical Challenges Faced during Project Execution

Implementing user permissions for individual users
Setting up alarms for specific metrics

How the Technical Challenges were Solved

For connecting Grafana to the web API, we used the Grafana API and configured it to fetch data from the web API based on user input.
To implement user permissions, we used Grafana’s built-in user management feature and set up roles and permissions accordingly.
For setting up alarms, we leveraged Grafana’s built-in alerting feature and configured it to trigger alerts based on specific conditions.

Business Impact
The proposed Grafana dashboard will significantly enhance the business’s ability to monitor and manage building automation systems. By providing real-time data visualization and the ability to set alarms for specific metrics, the business can quickly identify and address potential issues, ensuring optimal system performance and efficiency. Furthermore, the user-specific permissions will ensure that sensitive data remains secure and accessible only to authorized individuals. This will not only streamline operations but also boost confidence among staff members who can now make informed decisions based on accurate and timely data. The dashboard’s flexibility in terms of selectable history names and time ranges will allow for comprehensive analysis of historical data, leading to improved decision-making processes. Overall, this solution will contribute to increased operational efficiency, reduced downtime, and improved customer satisfaction by ensuring smooth operation of building automation systems.
Project website url
https://mailhvac.postman.co/workspace/Team-Workspace~902b44a6-966b-4e59-8400-3ae02c12ce6b/collection/17767455-eb2c775e-421d-4f7c-9ec5-b4f6a73f1a5a?action=share&creator=17767455
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleMVP for a software that analyses content from audio (Pharma-based)Next articleAI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2024,https://insights.blackcoffer.com/mvp-for-a-software-that-analyses-content-from-audio-pharma-based/,MVP for a software that analyses content from audio (Pharma-based),"

Client Background
Client: A leading pharma-tech firm in the USA
Industry Type: Healthcare
Products & Services: Pharma Apps
Organization Size: 100+
The Problem
The problem lies in creating a backend model for an application that records audio responses from students and uses AI to analyze the content. The backend needs to convert audio to text, transform the text into analytics KPIs, handle login/logout operations, and manage analytics API calls. The application should also calculate the cosine similarity of the student’s response with the expected response.
Our Solution
To solve this problem, we will use Python as the primary programming language for backend development. The solution involves several steps:

Audio to Text Conversion: We will use a speech recognition library in Python such as SpeechRecognition to convert audio inputs into text.
Text Analysis: After converting the audio to text, we will apply Natural Language Processing (NLP) techniques to analyze the text. This includes sentiment analysis, readability analysis, and named entity recognition (NER). We will use libraries like NLTK and SpaCy for this purpose.
User Authentication: We will build a secure authentication system using JWT tokens for handling login and logout operations.
API Creation: We will use Flask, a lightweight Python framework, to create APIs for managing user sessions and handling analytics data.
Data Storage: We will use a relational database like PostgreSQL to store user session data, user profiles, and analytics data.
Deployment: Finally, we will deploy the application on a cloud platform like AWS or Google Cloud.

Solution Architecture

Deliverables

Backend model developed using Python
APIs for managing user sessions and analytics data
Secure user authentication system
System capable of converting audio to text
Text analysis capabilities including sentiment analysis, readability analysis, and NER
Deployed application on a cloud platform

Tech Stack

Tools used
Python
Flask
JWT
PostgreSQL
AWS/Google Cloud
Language/techniques used
Python
Models used
SpeechRecognition for audio to text conversion
NLTK and SpaCy for text analysis
Skills used
Backend development
API creation
Text Sentiment analysis – Cosine Similarity Scoring
Machine learning (Natural Language Processing)

What are the technical Challenges Faced during Project Execution

One of the main challenges faced during development was ensuring accurate audio to text conversion. Poor audio quality or heavy accents can make it difficult for speech recognition algorithms to correctly transcribe the audio.

How the Technical Challenges were Solved

To overcome this challenge, we decided to use a robust speech recognition library that supports multiple languages and dialects. Additionally, we implemented a mechanism to allow users to manually edit the transcribed text, providing them with more control over the accuracy of the transcription.

Business Impact
The implementation of this backend model will have significant business impacts:

Enhanced Student Engagement: By providing immediate feedback on student responses, the system can foster a more engaging learning environment. Students can receive instant insights into their communication style and areas of improvement, encouraging them to enhance their responses and overall academic performance.
Improved Learning Outcomes: The detailed analytics provided by the system can aid educators in understanding student learning patterns and identifying areas where students struggle. This can inform instructional strategies and curriculum adjustments, leading to improved learning outcomes.
Cost Savings: Automating the conversion of audio to text and the generation of analytics can significantly reduce manual labor costs associated with grading and feedback provision.
Scalability: The use of scalable technologies like Python and Flask allows the system to handle increasing volumes of student responses without compromising performance.
Data Insights: The system generates valuable data insights, including sentiment scores, readability metrics, and named entity recognition counts. These insights can inform strategic decisions and policy changes.
Customer Satisfaction: By providing a seamless, efficient experience for both students and educators, the system can enhance customer satisfaction, potentially leading to increased usage and positive word-of-mouth referrals.

These impacts align with the objectives of the business, making the project a high priority. The business impact analysis will ensure that the project is aligned with the organization’s strategic goals and that potential disruptions are identified and managed effectively
Project Snapshots

Project website url
Domain and SSL setup is completed :https://www.pharmacyinterns.com.au/ 
Web App is running successfully on  URL – http://34.30.224.139/ 
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleData Engineering and Management tool (Airbyte) with custom data connectors to manage CRM databaseNext articleGrafana Dashboard to visualize and analyze sensors’ data Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2025,https://insights.blackcoffer.com/data-engineering-and-management-tool-airbyte-with-custom-data-connectors-to-manage-crm-database/,Data Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database,"

Client Background
Client: A leading tech firm in Europe
Industry Type: IT
Products & Services: IT & Consulting, Software Development
Organization Size: 1000+
The Problem
Our company requires a robust, scalable, and secure data integration solution that can handle thousands of connections. We need to develop Airbyte connectors for various software applications listed in 2-nx-integration, including Join Portal, ClickUp, Coach Accountable, Hubspot, Quickbooks, Quickbooks Time, and Sales Flow. These connectors should be developed in Python and then wrapped into Docker images. The code should be housed in GitHub and automatically applied to Airbyte for execution using a CI/CD pipeline from GitHub to Airbyte. We also need a full production-ready version of Airbyte hosted on Google Cloud Platform (GCP) Kubernetes, secured via Google Sign In.
Moreover, we want to add custom features to Airbyte to control BigQuery projects/datasets. Both Airbyte and BigQuery should be monitored via Sentry, which will also be housed/hosted in the same project for all error reporting/monitoring. We also need to develop transformations to clean and transform the data from the software source to the client’s GCP Project for BigQuery. The code for these transformations should be stored in GitHub.
Our Solution
We propose to develop an instance of Airbyte that is production-ready on GCP over Kubernetes. This will be secured using Google Sign On linked to our organization. We will deploy Airbyte using the official documentation 8. To secure the Kubernetes setup, we plan to use Traefik’s ForwardAuth feature.
Next, we will code Airbyte Python integrations for our needed software list. We have already gathered the API documentation for each software application and have started coding the integrations. Once the initial integration is complete, we will document the process in ClickUp to guide future integrations.
We will use GitHub to host both the source code and Docker images of Airbyte integrations. We will also use Google Cloud’s Sentry for error reporting and monitoring.
Solution Architecture

Deliverables

Production-ready Airbyte instance on GCP Kubernetes
Secured Airbyte instance using Google Sign On
Developed Airbyte Python integrations for required software
Error reporting and monitoring setup with Sentry
Documentation of integration process in ClickUp

Tech Stack

Tools used
Airbyte
Docker
GitHub
Google Cloud Platform
Google Sign In
Traefik
Sentry
Language/techniques used
Python
Models used
Airbyte ETL
Skills used
Web Scraping
Database Management
API Connectors
Databases used
Google BigQuery

What are the technical Challenges Faced during Project Execution

One of the main challenges we anticipate is managing the scalability of the system to handle thousands of connections. Another challenge could be securing the system effectively while ensuring smooth operation.

How the Technical Challenges were Solved

To address the scalability issue, we will leverage the inherent scalability of Kubernetes and BigQuery. Kubernetes allows us to easily scale our services based on demand, while BigQuery is designed to handle large datasets and high query loads.
To ensure effective security, we will use Google Sign In for user authentication, and we will follow best practices for securing our Docker containers and GCP environment. Regular audits and penetration testing will also be conducted to identify and rectify any potential security vulnerabilities.

Business Impact
By developing a robust and scalable data integration solution using Airbyte, we aim to significantly enhance our business operations. This solution will enable us to efficiently manage and analyze data from various software applications, leading to improved decision-making processes.
Firstly, the ability to extract and load data from different software applications will allow us to centralize our data management, reducing the complexity of handling multiple data sources. This will streamline our data analysis processes and provide a unified view of our business data.
Secondly, the scalability of our solution means that it can handle a growing volume of data as our business grows. This is crucial in today’s digital age where businesses generate vast amounts of data daily.
Lastly, by securing our data integration solution with Google Sign In, we can ensure that only authorized individuals can access our sensitive business data. This adds an extra layer of security to our data management practices and helps protect against potential data breaches.
Moreover, by using Google Cloud Platform (GCP) for hosting our solution, we can take advantage of its advanced features and robust infrastructure. This will further enhance the reliability and performance of our data integration solution.
Overall, implementing this solution will enable us to harness the power of data to drive our business growth and success
Project Snapshots


Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleText Summarizing Tool to scrape and summarize pubmed medical papers Next articleMVP for a software that analyses content from audio (Pharma-based) Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2026,https://insights.blackcoffer.com/text-summarizing-tool-to-scrape-and-summarize-pubmed-medical-papers/,Text Summarizing Tool to scrape and summarize pubmed medical papers ,"

Client Background
Client: A leading medical R&D firm in the USA
Industry Type: Medical
Products & Services: R&D
Organization Size: 10000+
The Problem
An advanced AI tool designed specifically for doctors to assist them in retrieving answers to their
queries. Powered by state-of-the-art AI technologies, including web scraping and ChatGPT, The AI
Assistant aims to streamline information retrieval and provide valuable insights to professionals.
This AI Assistant leverages the capabilities of AI to facilitate seamless and efficient access to
knowledge and information. It combines web scraping techniques to gather relevant data from
trusted sources with ChatGPT and PubMed, providing accurate responses to doctors’ queries.
Query Retrieval: AI Assistant utilizes web scraping techniques to fetch information from credible
websites, academic journals, medical databases, and other trusted sources. It provides doctors with
immediate access to a vast array of knowledge and resources.
Benefits:
Time Efficiency: By quickly retrieving information and answering queries, AI Assistant saves
valuable time for doctors, allowing them to focus more on patient care and critical tasks.
Access to Knowledge: AI Assistant grants doctors easy access to a vast repository of knowledge,
ensuring they stay updated with the latest research, treatment guidelines, and best practices.
Decision Support: The tool provides valuable insights and recommendations, assisting doctors in
making informed decisions about diagnosis, treatment plans, and patient management.
Our Solution
To address this problem, we will build a web scraping tool that uses Python libraries such as BeautifulSoup, Selenium, and OpenAI’s GPT-3. The program will work as follows:

A user inputs the URL of the case report they want to extract data from.
The program sends a GET request to the webpage and parses the HTML content using BeautifulSoup.
The program then identifies the relevant sections of the webpage (such as the title, introduction, report, conclusion, and keywords) and extracts the text content.
For each reference linked in the case report, the program sends a GET request to the reference’s webpage and parses the HTML content.
The program then sends a prompt to the GPT-3 model, asking it to summarize the content of the reference, and receives a summarized response.
The program collects all the summarized references and adds them to the case report.
The program also identifies any images associated with the case report and downloads them.
Finally, the program creates a Word document and adds all the collected information (including the summarized references and downloaded images) to the document.

Solution Architecture

Deliverables

A fully functional web scraping tool that can extract data from a given webpage and generate a case report.
A detailed documentation explaining how to use the tool and what kind of data it can extract.

Tech Stack

Tools used
Python
BeautifulSoup
Selenium
OpenAI’s GPT-3
Language/techniques used
Python
Models used
OpenAI’s GPT-3
Skills used
Web Scraping
Natural Language Processing
Machine Learning

What are the technical Challenges Faced during Project Execution

Handling dynamic websites that load content via JavaScript.
Managing rate limits and CAPTCHAs imposed by the target websites.
Ensuring the accuracy and relevance of the summarized content generated by the GPT-3 model.

How the Technical Challenges were Solved

Using Selenium to interact with the JavaScript-rendered content of the target websites.
Implementing strategies to bypass rate limits and CAPTCHAs.
Fine-tuning the parameters of the GPT-3 model to improve the quality of the summarized content.

Business Impact
The implementation of our web scraping and summarization tool has had significant positive impacts on our business operations.
Firstly, it has streamlined our research process by automating the extraction of crucial information from various online sources. This has saved us considerable time and effort, allowing us to focus on more complex tasks.
Secondly, the summarization feature has improved our understanding of the information we collect. By reducing large volumes of text down to a few key points, we’ve been able to quickly grasp the main ideas and insights presented in the articles, videos, and user comments.
Thirdly, the tool has enabled us to stay up-to-date with the latest advancements in the field of orthopedics. By pulling data from recent articles on PubMed.gov, we’ve been able to stay informed about the latest research and treatments.
Finally, the tool has facilitated the creation of comprehensive case reports. These reports have been instrumental in our ability to present detailed and accurate information to our clients, thereby enhancing our reputation and credibility in the industry.
Overall, the implementation of this tool has greatly improved our efficiency and effectiveness, contributing significantly to our business success
Project Snapshots




Project Video
Link: https://www.loom.com/share/535828aad7184c1b82db707dcca8e52c?sid=c79d19b1-b963-45a1-bec5-6228cc753cc2
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous article7up7down, 10upDown, Snakes and Ladder Games built using OOPsNext articleData Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2027,https://insights.blackcoffer.com/7up7down-10updown-snakes-and-ladder-games-built-using-oops/,"7up7down, 10upDown, Snakes and Ladder Games built using OOPs","

Client Background
Client: A leading game development firm in the USA
Industry Type: Gaming Software
Products & Services: Gaming Software Development
Organization Size: 200+
The Problem
Our client sends records of millions of sports bets in real time from all over the world via an API. These bets are recorded in MySQL servers. We are tasked with processing and calculating the expected Profit and Loss (PNL) as per the bets records for each sport. Our goal is to analyze these records in real time via API and calculate PNL as per the game records history provided via API. This requires building a serverless application in Python (or similar) that reads all bets records and updates PNL in real time (within milliseconds, records need to be updated). The application should be capable of handling 10,000+ records of bets per second for numbers of different games, with PNL needing to be updated for each game separately.
Our Solution
To address this problem, we propose developing a Python-based serverless application that leverages machine learning models for real-time PNL calculation. The application will use the MySQL database to store and retrieve betting records. It will employ parallel computing techniques to ensure efficient processing of high volumes of data. The application will also utilize APIs to fetch real-time data and update PNL accordingly.
The application will follow these steps:

Connect to the MySQL database to access the betting records.
Use an API to fetch real-time betting data.
Process the data using Python scripts.
Apply machine learning models to predict the outcome of each bet.
Calculate the PNL for each bet according to the predicted outcome.
Update the PNL in the MySQL database in real time.

Solution Architecture

Deliverables

A Python-based serverless application for real-time PNL calculation.
An interface for visualizing the calculated PNL in real time.
Documentation detailing how to use and maintain the application.

Tech Stack

Tools used
Python: For writing the serverless application.
MySQL: For storing and retrieving betting records.
Machine Learning Models: For predicting the outcome of bets.
Language/techniques used
Python
Models used
OOPS
Skills used
Database Analysis & API Development: To design and optimize the MySQL database.
Python Programming: To write the serverless application.
OOPS: To make the game functioning algorithms.
Databases used
SQL

What are the technical Challenges Faced during Project Execution

One of the main challenges we faced was handling the high volume of data coming in real time. To overcome this, we employed parallel computing techniques to efficiently process the data. Another challenge was updating the PNL in the MySQL database in real time. We solved this by designing the application to update the PNL immediately after it is calculated.

How the Technical Challenges were Solved

We addressed the high volume of data challenge by using parallel computing techniques. This allowed us to process a large number of records simultaneously, ensuring efficient data handling.
To solve the real-time PNL update issue, we designed the application to update the PNL immediately after it is calculated. This ensured that the PNL was always up-to-date, meeting the requirement of real-time PNL calculation.

Business Impact
The implementation of the proposed Python-based serverless application for real-time PNL calculation had significant positive impacts on our business operations.
Firstly, the application enabled us to process and analyze millions of sports bets in real time, enhancing our decision-making capabilities and allowing for quicker responses to changes in the betting market. This improved our ability to predict outcomes and adjust our betting strategies accordingly.
Secondly, the application significantly reduced the time taken to calculate PNL, from hours to mere minutes. This resulted in faster decision-making processes and timely financial reporting, which were crucial for our clients and investors.
Lastly, the application’s ability to handle high volumes of data and provide real-time updates facilitated a more globalized betting market. With real-time data and digital platforms, geographical boundaries became less relevant, allowing bettors from around the world to place bets on any event globally, with real-time odds reflecting local nuances and dynamics. This led to increased liquidity and more competitive odds.
Overall, the successful implementation of the application led to a more efficient, accurate, and timely PNL calculation process, resulting in improved business performance and customer satisfaction.
Project Snapshots


Project website url

https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit


https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleData Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud FunctionNext articleText Summarizing Tool to scrape and summarize pubmed medical papers  Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2028,https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-using-custom-webhooks-and-google-cloud-function/,Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function,"

Client Background
Client: A leading retail firm in the USA
Industry Type: Retail
Products & Services: Retail Business, e-commerce
Organization Size: 300+
The Problem
The client needs a consolidated KPI dashboard that aggregates data from various applications and SaaS products. Currently, the data is scattered across different platforms, making it difficult to track key performance indicators (KPIs) effectively. The client wants a dashboard that automatically updates with new data, eliminating the need for manual updates. The dashboard should contain separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Additionally, the client wants to use Google Cloud Functions to sync data regularly between the Podio data app and Google Sheets.
Our Solution
The proposed solution involves the creation of a KPI dashboard in Google Sheets, which will serve as a central hub for all the client’s data. This dashboard will be populated with data from various sources, including Google Sheets and the Podio data app. The data will be organized into separate tabs, each representing a different aspect of the business. The dashboard will be designed to automatically update with new data, removing the need for manual updates.
The process begins with obtaining access to the data in Google Sheets. Once the data is accessed, a list of KPIs to be visualized will be prepared. The data from Google Sheets will then be connected to the Google Data Studio dashboard for visualization. The dashboard will be designed to align with the client’s goals, prioritizing the most important KPIs and positioning them at the top of the dashboard. The dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designated data sheets. Collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. The dashboard will be customized with brand-aligned colors and fonts to enhance its appearance and authority.
In addition to the dashboard, webhooks will be created for the Podio data app deployed as a Google Cloud Function. This will enable regular data synchronization between the Podio data app and Google Sheets, ensuring that the dashboard is always up-to-date with the latest data.
Solution Architecture

Deliverables

End-to-end data pipeline
KPI Dashboard in Google Sheets with separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
Automatic update functionality to eliminate the need for manual updates.
Webhook for the Podio data app deployed as a Google Cloud Function to sync data regularly.

Tech Stack

Tools used
Python
Google Sheets
Google Data Studio
Google Cloud Functions
Podio data app
Language/techniques used
Python
Javascript
Skills used
Data Analysis
Data Visualization
Cloud Functions
API Integration
Databases used
BigQuery

What are the technical Challenges Faced during Project Execution

One of the main challenges was ensuring that the dashboard could seamlessly integrate data from various sources and update automatically. 
Another challenge was designing the dashboard in a way that aligns with the client’s goals and presents the data in a clear and actionable manner.

How the Technical Challenges were Solved

The first challenge was addressed by connecting the data sources to Google Sheets and setting up the dashboard to automatically update with new data. This was achieved by using Google Data Studio and Google Cloud Functions. 
The second challenge was addressed by focusing on the design and organization of the dashboard, ensuring that it aligns with the client’s goals and presents the data in a clear and actionable manner. This was achieved by prioritizing the most important KPIs and positioning them at the top of the dashboard, and by presenting supporting data as charts and tables to help decision-makers make sense of the KPI

Business Impact
The implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicators (KPIs). Prior to the solution, the client was struggling with data fragmentation across different SaaS products and applications, which made it difficult to compile comprehensive insights. The KPI dashboard, now consolidated in Google Sheets, has streamlined this process, providing a unified view of the business metrics.
This solution has also automated the data update process, saving valuable time and resources that were previously spent on manual updates. The automatic update feature has allowed the client to focus on analyzing the data rather than spending hours updating it.
Additionally, the integration of the Podio data app with Google Sheets via Google Cloud Functions has improved data synchronization efficiency. Regular data synchronization ensures that the KPI dashboard is always up-to-date, providing real-time insights into the business performance.
These improvements have led to enhanced decision-making processes within the client’s organization. With accurate and timely data, managers can now set and achieve goals more effectively. The consolidation of data has also facilitated cross-departmental collaboration, as teams can now access and share data easily.
Overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planning
Project Snapshots


Project website url
https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit
https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEnd-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithmNext article7up7down, 10upDown, Snakes and Ladder Games built using OOPs Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2029,https://insights.blackcoffer.com/end-to-end-tool-to-optimize-routing-and-planning-of-field-engineers-using-googles-cvrp-tw-algorithm/,End-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm,"

Client Background
Client: A leading hardware firm in the USA
Industry Type: IT
Products & Services: IT Consulting, Support, Hardware Installations
Organization Size: 300+
The Problem
The client specializes in installing blinds and related products in customers’ homes. They are currently struggling with scheduling appointments efficiently due to a variety of factors such as location, installation duration, team member availability, and customer preferences. We need a tool that can suggest optimal schedules based on these criteria and adapt to changes as customers approve or reject proposed appointment times. The goal is to create a proof of concept for a route and job planning model that can potentially streamline our scheduling process and make a significant impact on our business operations.
Our Solution
The To address this challenge, we propose developing a proof of concept for a route and job planning model. This model will be based on the concept of Constrained Vehicle Routing Problem with Time Windows (CVRP-TW), a well-established approach in operations research and logistics. The model will take a dataset, which could be extracted from a Google sheet or converted from a CSV file, and generate optimal schedules.
The development process will involve several stages:

Understanding the data: We’ll analyze the data to identify the relevant variables and constraints. These may include the locations of installations, the duration of installations, the availability of team members, and customer preferences.
Defining the objective and constraints: The objective will be to minimize the total travel time or maximize the number of installations completed within a given time frame. The constraints will include the geographical distances between locations, the working hours of team members, and the specific requirements of each installation.
Implementing the algorithm: We’ll use an optimization algorithm, such as the Traveling Salesman Problem (TSP) solver, to find the optimal routes. The algorithm will consider all possible routes and choose the one that best meets the objectives while adhering to the constraints.
Running simulations: To ensure the feasibility of the model, we’ll run simulations using different scenarios and adjust the parameters as needed.
Saving the output: The final output will be the suggested schedules, which can then be reviewed and approved by the relevant parties.

In terms of technology, we’ll use Python, a popular language for data analysis and machine learning. We’ll also use the Anaconda distribution, which provides a powerful environment for scientific computing and data analysis.
Solution Architecture

Deliverables

A Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.

Tech Stack

Tools used
Python: The primary programming language.
Anaconda: The Python distribution used for data analysis and machine learning.
Visual Studio Code: The code editor used during development.
Google App Script for deployment integrated with Google Sheets
Language/techniques used
Python
Models used
Constrained Vehicle Routing Problem with Time Windows (CVRP-TW)
Skills used
Data Analysis
Machine Learning
Optimization Algorithms
Python Programming
Databases used
CSV, Google Sheets: The data will initially be stored in a CSV file, which can be easily imported into Python using libraries like pandas.

What are the technical Challenges Faced during Project Execution

One of the main challenges we anticipated is dealing with the complexity and variability of the data. The locations of installations, the duration of installations, the availability of team members, and customer preferences all need to be taken into account, and these factors can vary widely. Additionally, the model needs to be flexible enough to adapt to changes in the criteria as customers approve or reject appointment times.

How the Technical Challenges were Solved

To overcome these challenges, we used advanced data analysis techniques to extract meaningful insights from the data. We’ll also develop a flexible model that can handle changes in the criteria. Furthermore, we’ll thoroughly test the model under different scenarios to ensure its robustness and reliability.

Business Impact
Implementing an efficient route and job planning model had a significant positive impact on our business operations. By automating the scheduling process, we were able to reduce manual errors and streamline our workflow, resulting in quicker response times and deliveries. This not only improved our operational efficiency but also enhanced our ability to provide better service to our customers.
Moreover, the model allowed us to maximize each driver’s productivity by optimizing routes, which led to cost savings in fuel and vehicle maintenance. The automated nature of the system also enabled us to make real-time adjustments to the route in response to last-minute orders or unexpected situations, such as a driver being unavailable.
The model also provided us with valuable insights into our operations, allowing us to identify bottlenecks and areas for improvement. This helped us to proactively address potential issues and continuously enhance our processes, thereby increasing our overall business performance.
As a result of these improvements, we were able to attract more skilled workers by focusing on cutting down unskilled labor. This shift towards more automation allowed us to invest more in our workforce, leading to higher employee satisfaction and retention rates.
Lastly, the successful implementation of the route and job planning model has opened up new opportunities for our business. With the ability to efficiently cover our market and manage our resources effectively, we have been able to consider expanding our territory by entering new markets. This strategic route planning has helped us determine whether we need to acquire more vehicles or hire more operators before moving, providing a clear pathway for future growth.
Project Snapshots

Project website url
https://docs.google.com/spreadsheets/d/1kS7Em9NitvMD_49MoLCpt_KoPJGGIAGjCES_KI8rEQk/edit?userstoinvite=raymondchow%40stanbondsa.com.au#gid=766964619
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleEnd-to-end tool to predict Biofuel prices using IESO dataNext articleData Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2030,https://insights.blackcoffer.com/end-to-end-tool-to-predict-biofuel-prices-using-ieso-data/,End-to-end tool to predict Biofuel prices using IESO data,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT
Products & Services: IT Consulting, Software Development
Organization Size: 100+
The Problem
The task involves creating an end-to-end data pipeline to extract data from various reports, store it in a Google Cloud Platform (GCP) database, build a dashboard, and develop a machine learning model for price forecasting. The data is pulled from different links, each having a slightly different report layout, with some being in CSV and others in XML format. The goal is to extract data daily and hourly for the past three years. The extracted data is intended to be used for building a dashboard and training/testing a model based on user-defined inputs on the dashboard. The challenge lies in handling the varied formats of the data, ensuring accurate extraction, and maintaining the integrity of the data throughout the pipeline.
Our Solution
To solve this problem, we will use Python, along with libraries such as pandas and BeautifulSoup, to scrape data from various report links. The scraped data is stored in dataframes and then loaded into Google Cloud Storage buckets. This data is then transferred to BigQuery tables for efficient processing. The data extraction process is automated with a Cronjob/Google Cloud Scheduler.
For the machine learning part, we will build and run various machine learning models in GCP’s BigQuery to predict future fuel/energy prices. We will test LSTM univariate/multivariate, GRU for time series problems, and ANN Regressor, Random Forests regression for regression problems. The ANN regression model will provide the best results for our use case.
After modeling, we will generate a data visualization report on Google Data Studio for further insights. The report includes a pie chart about the distribution of fuel generated by each fuel type, a stacked column chart about the distribution of fuel generated each month, and a time series visualization of fuel generation during each quarter of the year.
Solution Architecture

Deliverables

End-to-end data pipeline
Data stored in Google Cloud Platform (GCP) database
Dashboard built on Google Data Studio
Machine learning model for price forecasting

Tech Stack

Tools used
Python
pandas
BeautifulSoup
Google Cloud Platform (GCP)
Google Cloud Storage
Google BigQuery
Google Data Studio
Language/techniques used
Python
Models used
LSTM
GRU
ANN Regressor
Random Forests Regression
Skills used
Web Scraping
Database Management
Data Visualization
Machine Learning Model Development
Databases used
Google BigQuery

What are the technical Challenges Faced during Project Execution

Handling varied data formats (CSV, XML)
Ensuring accurate extraction of data
Maintaining data integrity throughout the pipeline

How the Technical Challenges were Solved

Utilizing Python libraries like pandas and BeautifulSoup for web scraping and data manipulation
Automating the data extraction process using Cronjob/Google Cloud Scheduler
Testing various machine learning models to select the best fit for our use case
Using Google Cloud Platform services for storing, processing, and visualizing data.

Business Impact
The successful implementation of the end-to-end data pipeline project had several significant business impacts.
Firstly, it led to improved data quality and accessibility. The project streamlined the process of data extraction from various sources, ensuring that the data was clean, consistent, and readily available for analysis. This resulted in more reliable and accurate predictions, leading to better decision-making and strategic planning.
Secondly, the project enhanced operational efficiency. By automating the data extraction process with a Cronjob/Google Cloud Scheduler, the team saved considerable time and effort. This allowed the team to focus on more strategic tasks, thereby increasing productivity.
Thirdly, the project facilitated informed decision-making. The dashboard built on Google Data Studio provided users with real-time insights into fuel consumption patterns and energy prices. This helped stakeholders make informed decisions regarding energy usage and pricing strategies.
Lastly, the project demonstrated the company’s commitment to leveraging advanced technologies for business growth. The use of Google Cloud Platform, BigQuery, and Google Data Studio showcased the company’s ability to innovate and stay competitive in the rapidly evolving digital landscape.
Overall, the project had a positive impact on the company’s operations, decision-making processes, and reputation among stakeholders. It underscored the importance of data-driven decision making and highlighted the potential benefits of investing in advanced technologies.
Project Snapshots




Project website url
https://console.cloud.google.com/compute/instances?authuser=1&project=ieso&pli=1
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleMethodology for ETL Discovery Tool using LLMA, OpenAI, LangchainNext articleEnd-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2031,https://insights.blackcoffer.com/etl-discovery-tool-using-llma-langchain-openai/,"ETL Discovery Tool using LLMA, Langchain, OpenAI","

Client Background
Client: A leading retail firm in the USA
Industry Type: Retail
Products & Services: Retail Business, e-commerce
Organization Size: 100+
The Problem
To develop an ETL discovery tool that can answer the queries related to ETL pipelines in conversational format. The areas of the concerned queries should include Environment Analysis, Workflow Analysis, Data Source and Target Mapping, Transformation Logic, Data Volume and Velocity, Error Handling and Logging and Security and Access Control.
Our Solution
In developing our solution, we began by aggregating Open-Source Generic ETL Tool Code from various repositories on GitHub and other relevant sources. Subsequently, we meticulously fine-tuned the collected ETL tool code, organizing and saving it into distinct folders, each containing different ETL pipelines.
Following this, we implemented an OpenAI Assistant, integrating it with all the refined ETL pipelines. To facilitate communication with these pipelines, we employed the OpenAI Assistant ID within our Flask API.
For the user interface, we opted for a Streamlit front-end, providing a seamless and user-friendly interaction with our OpenAI Assistant and the integrated ETL pipelines.
Solution Architecture

ETL Discovery Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.
Step 1. Open-Source Generic ETL Tool Code:
The Open-Source Generic ETL Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.
Step 2. Data Cleaning:
Data Cleaning is a critical stage that involves cleansing and pre-processing raw data to enhance its quality and integrity. In this step the ETL understands the expected data format that is organized and cleaned for uniformity of data.
Step 3. Files/DB
Represents the storage or databases utilized for storing processed data. In this step, solutions for processed data the code files will be arranged and catalogued so that they are ready to be used by the OpenAI Assistants API.
Step 4. OpenAI Assistant Creation via API:
This step involves creating an OpenAI Assistant using the OpenAI API.

Configuring the OpenAI Assistant

Configure .env file with OpenAI API Key


We will upload the files to the Assistant for it to be added in context.

Run assistant creator.py  file for generating OpenAI Assistant ID
After Generating OpenAI Assistant id look into terminal save the generated ID into .env file


We will get the assistant ID that is to be used later.

Step 5. OpenAI Assistant:
In this step, the Assistant that is created from previous step will be queried by the API with instructions for the context accommodation.

Features and Capabilities: functionalities supported by the assistant

OpenAI Assistant will read all our ETL pipeline which is provided when we are generating the OpenAI assistant ID


Usage Guidelines/Instructions: – Guide users on interacting with the OpenAI Assistant

We are providing Instructions to our OpenAI Assistant to communicate with user 



Step 6. Django/Flask/FastAPI API:
This step involves setting up an API using popular frameworks like Django, Flask, or FastAPI.

Framework Selection: choice of the specific framework
We are using Flask API to communicate with the OpenAI Assistant


API Endpoints:  available endpoints and their functionalities

Configured the OpenAI Key in app1.py
Configured the OpenAI Assistant ID in app1.py
Store the Instruction file into variable we are using the variable below
After the Configuration of Flask file run the app1.py file to start the Flask API Local Server 


Authentication: – Used for securing the API
Handling Request and Response process

Step 7. Chat Frontend (Streamlit):
Represents the user interface for interacting with the system, built using Streamlit.

Configurations: Configurations of Streamlit frontend

Set your OpenAI API key into .env file


User Interaction: Users will be able to query based on training data.
Integration with Backend: – Frontend will be connect to the backend API.

In the main.py file Provide the Flask API url endpoint to communicate with OpenAI Assistant


Handle Request and Response from the User

Deliverables

OpenAI Assistant Flask API
Streamlit frontend

Tech Stack

Tools used
Visual Studio Code
Language/techniques used
Python, Flask, OpenAI
Models used
OpenAI Assistant 
Skills used
Python, RestAPI, OpenAI API

What are the technical Challenges Faced during Project Execution
Finding the ETL pipelines and fine tuning the ETL pipelines
How the Technical Challenges were solved
Our approach to overcoming technical challenges involved an extensive internet search focused on ETL pipelines. We scoured various online resources, eventually identifying the most effective ETL pipelines available on GitHub.
To address each challenge systematically, we created individual files for each ETL pipeline. In the process, we meticulously fine-tuned and optimized each pipeline, documenting the specific tasks and functions within the respective files. This approach allowed us to provide detailed descriptions of the work performed for every ETL pipeline, ensuring a comprehensive understanding of the solutions implemented to tackle the technical hurdles encountered.
Business Impact
The business impact was substantial as the client efficiently analysed numerous ETL tool pipelines. Instant answers in a chat format replaced the time-consuming manual work that could take Data Engineers days or weeks. This streamlined process significantly enhanced productivity and responsiveness, reflecting a tangible improvement in operational efficiency for the client.
Project Snapshots








Assistant_creator.py

Main.py

Project Video
Project Demo Video link:- https://www.loom.com/share/5ee7d0835412474ea4aa3383af5a0814?sid=999739fc-e91a-4cda-a30e-9cd02957205f
Installation Walkthrough Video:- 
Part 1 (Backend):- https://www.loom.com/share/338c4e09c90e453e83b86050d469d98b?sid=03299e7a-0699-464e-be2c-689a409ec01e
Part 2 (Frontend):- https://www.loom.com/share/8e7942f3a03e49889c6c70fba77f76b0?sid=eca0586f-b767-45fa-854d-853bca1890dc
Project GitHub Repository

GitHub Link:- https://github.com/AjayBidyarthy/Rob-Sandberg-ETL

Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleGPT/OCR APINext articleMethodology for database discovery tool using openai, LLMA, Langchain Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2032,https://insights.blackcoffer.com/gpt-ocr-api/,GPT/OCR API,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT & Consulting
Products & Services: IT Solutions, Software Development
Organization Size: 100+
The Problem
Design and develop an API as a service backend, the API should be integrated with GPT and OCR technologies to extract documents it should be hosted on Azure
Our Solution

/token – It takes username and password as a input and generate API_key/token to run the other APIs


/api/template/create-template – This is a Post request.  It stores the created json template in the database and generates a token id.


/api/document/upload – This api takes a file as an input. We can upload .pdf, .docx, .png, .jpg, .jpeg, .txt files. It has basically 2 parts. We can just upload the document or we can also provide template id to process the uploaded document according to the template id.


/api/document/process – This api takes template id and document id as an input. It fetches the template and document from the database and uses the ocr method to extract the text from the document. This extracted text and template are then processed by gpt api which generates the final output.


/api/template/all – This api fetches all the templates created by the user using create-template api.
/api/template/update-template – This api can update the created template.


/api/template/delete – This api deletes the created template by giving template id.


/api/document/all – This api shows all documents uploaded by user


/api/document/delete – This api deletes the document by document id.

Deliverables
All the APIs on the Azure server
Tools used
fastapi, gpt api, pytessaract, pypdf2
Language/techniques used
fastapi, gpt api, pytessaract, pypdf2, python
Skills used
python, Rest API development
Databases used
MS Sql 
Web Cloud Servers used
Azure
What are the technical Challenges Faced during Project Execution
Main challenge in this project extracting text from images and pdfs and generate json output according to template
How the Technical Challenges were Solved
In the apis we can upload .pdf, .docx, .png, .jpg, .jpeg, .txt files. It has basically 2 parts. We can just upload the document or we can also provide template id to process the uploaded document according to the template id.
It fetches the template and document from the database and uses the ocr method to extract the text from the document. This extracted text and template are then processed by gpt api which generates the final output..
Business Impact
This will help users to directly upload any pdf or image and extract useful information in json format. 
Project Snapshots





Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleDockerize the AWS Lambda for serverless architectureNext articleETL Discovery Tool using LLMA, Langchain, OpenAI Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2033,https://insights.blackcoffer.com/dockerize-the-aws-lambda-for-serverless-architecture/,Dockerize the AWS Lambda for serverless architecture,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT & Consulting
Products & Services: IT Solutions, Software Development
Organization Size: 100+
The Problem 
AWS Lambda, a powerful serverless compute service, faces limitations in terms of runtime customization, dependency management, and execution environment isolation.
Our Solution
To overcome the challenges mentioned above, we propose a comprehensive solution that involves Dockerizing AWS Lambda functions for improved flexibility, control, and efficiency within a serverless architecture.
Solution Architecture
Below is a high-level architecture diagram:
Key Components:

AWS Lambda Function: Contains the original Lambda function code and dependencies.
Dockerfile: Describes the steps to build the Docker image, including installing dependencies, copying Lambda function code, and setting the handler function.
Docker Image: The containerized version of the Lambda function, including its code and dependencies.
Amazon ECR Repository: Stores the Docker image. The image is tagged with the repository URI.
Updated Lambda Function: Refers to the Docker image in the ECR repository. The Lambda function configuration is updated to use this reference.

Deliverables
Some of the key deliverables: 
Dockerfile:
A Dockerfile in the root of your Lambda function project, specifying the instructions to build the Docker image. This file includes the base image, installation of dependencies, copying of Lambda function code, and setting the handler function.
Docker Image:
The Docker image built from the Dockerfile. This image encapsulates your Lambda function code and its dependencies.
Pushed Image to ECR:
The Docker image pushed to your Amazon Elastic Container Registry (ECR) repository. This involves tagging the image with the ECR repository URI and pushing it to the repository.
Updated Lambda Function Configuration:
The Lambda function configuration was updated to use the Docker image from ECR. This may involve specifying the ECR URI in the Lambda configuration.
Documentation: 
Documentation outlining the steps to Dockerize the Lambda function and push it to ECR. This documentation should include prerequisites, step-by-step instructions, and any additional considerations.
Tech Stack

Tools used
Docker
Amazon ECR
Amazon Lambda.
AWS Management Console.
Language/techniques used
NodeJS
Docker commands
Skills used
AWS services (Lambda, ECR, etc.).
Docker
Web Cloud Servers used
Amazon Web Services

What are the technical Challenges Faced during Project Execution

Dependency Management:

Challenge: AWS Lambda imposes constraints on runtime dependencies, making it challenging to manage and control library versions.

Execution Environment Isolation:

Challenge: AWS Lambda’s managed environment may lack certain runtime configurations and isolation.

Monitoring and Logging Integration:

Challenge: Efficiently capturing and analyzing performance metrics and logs from Dockerized Lambda functions.
How the Technical Challenges were solved

Dependency Management:

Solution: Use a containerization approach to package dependencies along with the Lambda function, providing better control and isolation. Implement a robust dependency management system within the Docker container.

Execution Environment Isolation:

Solution: Docker containers offer enhanced isolation. Utilize containers to encapsulate the Lambda function and its dependencies, ensuring consistent execution environments.

Monitoring and Logging Integration:

Solution: Integrate AWS CloudWatch for basic monitoring. 
Project Snapshots

Create ECR Repository:



Create directory and initialize npm:




View Docker commands:
Login to ECR and Build Docker image:




Create Lambda Function:



Testing Lambda Function: 



Project Video
Dockerizing a Lambda Function:
https://www.loom.com/share/e90438538dbb43fd884a51dab6c175e9?t=586&sid=b2e4112e-16b9-4d78-a955-77a289453e59 
Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleDesign and develop a product recommendation engine based on the features of productsNext articleGPT/OCR API Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2034,https://insights.blackcoffer.com/design-and-develop-a-product-recommendation-engine-based-on-the-features-of-products/,Design and develop a product recommendation engine based on the features of products,"

Client Background
Client: A leading retail firm in the USA
Industry Type: Retail
Products & Services: Retail Business, e-commerce
Organization Size: 100+
The Problem
Design and develop a product recommendation engine based on the features of products
Our Solution
Content-based product recommendation system has been created using Machine Learning Algorithm and Python.
Solution Architecture
Recommendation engine have six cases which are mentioned below:
Case 1:

Description: Given an object name or PAR ID, inp_prodname recommends products with the same object type as the input and ranks them based on the number of specifications matched.
Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Specifications, userDef1, userDef2, userDef3.

Case 2:

Description: Given specifications and an object type, inp_custom_spec recommends products and ranks them based on the number of specifications matched.
Input: JSON format with the following keys: Specifications, Object Type, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, userDef1, userDef2, userDef3.

Case 3:

Description: Based on compatible models, inp_prodname_comp recommends products and ranks them based on the number of compatible models matched.
Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Compatible Models, userDef1, userDef2, userDef3.

Case 4:

Description: Based on the number of specifications entered by the user, inp_spec_num creates clusters of products with the same number of specifications.
Input: JSON format with the following keys: Number of Specifications, Object Type, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: Cluster ID, Object Name, Object Types, PAR ID, specifications_grped, userDef1, userDef2, userDef3.

Case 5:

Description: Given specification attributes and an object type, inp_spec_attr creates clusters of products with the same specifications.
Input: JSON format with the following keys: Specification Attributes, Object Type, Debug Information, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: PAR ID, Object Name, Object Type, Cluster ID, Specifications, userDef1, userDef2, userDef3.

Case 6:

Description: Based on the object name or PAR ID entered by the user, inp_prodname_model creates clusters of products with similar specifications.
Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3.
Output: JSON format with the following keys: Object Name, Object Types, PAR ID, Specifications, Rank, userDef1, userDef2, userDef3.

The APIs for all the above cases have been created
Deliverables
The code of the recommendation engine and its API is been delivered.
Tools used
Python, Postman
Language/techniques used
Python, Machine Learning, Flask API, Pandas
Models used
Affinity Propagation is a clustering algorithm that does not require a predefined number of clusters. It is used to group products based on their similarities.
Skills used
Python, Logical Reasoning, Machine Learning, Data Engineering.
What are the technical Challenges Faced during Project Execution

Product has many features but there was one feature which Is a “product type” that needs to be handled differently because it was important to have the product in a cluster must have the same product type. 
Some cases can’t be solved with machine learning algorithms.

How the Technical Challenges were Solved

Handling Product Type as a Differentiating Feature: One of the challenges faced was dealing with the “product type” feature, which required special consideration. It was crucial to ensure that products with the same type were grouped together in the clustering or recommendation algorithm. This required developing a specific approach to address the uniqueness of the product type feature and incorporate it effectively into the recommendation system. Custom modifications and additional preprocessing steps were likely needed to accommodate this requirement and ensure accurate clustering based on product type.
Limitations of Machine Learning Algorithms: While machine learning algorithms are powerful tools for recommendation systems, there are cases where they may not be sufficient to solve certain challenges. During the project, it was likely discovered that some complex scenarios couldn’t be adequately addressed using traditional machine learning algorithms alone. To overcome this, alternative techniques and approaches beyond the scope of standard algorithms needed to be explored. This might involve incorporating domain-specific rules, utilizing other data analysis methods, or considering hybrid models that combine machine learning with expert knowledge to overcome the limitations and improve the recommendation system’s performance.

Business Impact
This recommendation engine can significantly enhance customers’ shopping experience by increasing the likelihood of them discovering products that perfectly align with their preferences. This personalized approach not only saves them valuable time and effort in searching for relevant items but also ensures that their unique needs and desires are met. As a result, customers are more likely to make purchases, leading to increased sales and revenue for the business.
Moreover, this recommendation engine plays a crucial role in improving customer satisfaction and fostering long-term loyalty. By suggesting products based on individual preferences and specific features, customers feel understood and valued. This tailored experience enhances their overall satisfaction, making them more inclined to return to the business for future purchases. Additionally, satisfied customers are more likely to spread positive word-of-mouth, attracting new customers and expanding their customer base.
 Project Snapshots









Summarize
Summarized: https://blackcoffer.com/
This project was done by the Blackcoffer Team, a Global IT Consulting firm.
Contact Details
This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy




Previous articleChatbot using VoiceFlowNext articleDockerize the AWS Lambda for serverless architecture Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2035,https://insights.blackcoffer.com/database-discovery-tool-using-openai/,Database Discovery Tool using OpenAI,"


Client Background
Client: A leading retail firm in the USA
Industry Type: Retail
Products & Services: Retail Business, e-commerce 
Organization Size: 100+
Problem Statement:
Organizations often face challenges in managing and understanding their vast and complex databases. As data infrastructure evolves, new databases are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. This lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration.
To address these challenges, there is a need for a Database Discovery Tool using OpenAI, aimed at providing an automated and intelligent solution for discovering, cataloging, and understanding the various databases within an organization’s ecosystem.
Key Problems to Solve:

Database Proliferation:

Challenge: The rapid growth of databases within an organization makes it challenging to keep track of all data storage systems.
Impact: Increased difficulty in managing, securing, and optimizing databases.


Data Schema Variability:

Challenge: Databases often have diverse schemas, making it hard to understand the structure of stored data.
Impact: Inefficient data integration and difficulty in ensuring data consistency across the organization.


Limited Metadata Documentation:

Challenge: Lack of comprehensive metadata documentation for databases, including information about tables, columns, relationships, and data types.
Impact: Time-consuming manual efforts for understanding data structures and dependencies.


Security and Compliance Risks:

Challenge: Inability to identify and monitor sensitive data across databases may lead to security and compliance risks.
Impact: Increased likelihood of data breaches and non-compliance with regulatory standards.


Operational Inefficiencies:

Challenge: Manual efforts required for discovering and documenting databases result in operational inefficiencies.
Impact: Increased workload for database administrators, leading to potential errors and delays.


Lack of Intelligent Insights:

Challenge: Absence of intelligent insights into database usage patterns, performance metrics, and optimization opportunities.
Impact: Missed opportunities for improving database performance and resource utilization.



Proposed Solution:
Develop an OpenAI-powered Database Discovery Tool that leverages natural language processing (NLP) and machine learning capabilities to automatically discover, catalog, and provide insights into the organization’s databases. The tool should be able to:

Automatically scan and identify databases across different environments.
Extract and catalog metadata, including schema details, relationships, and data types.
Provide intelligent insights into database usage patterns and performance metrics.
Identify and classify sensitive data for enhanced security and compliance.
Enable efficient search and navigation of the entire database landscape.
Support ongoing updates and synchronization with changes in the data infrastructure.

By addressing these challenges, the Database Discovery Tool using OpenAI aims to empower organizations with a holistic view of their data landscape, facilitating better management, security, and optimization of databases.
Solution Architecture 

Step by Step Execution
Step 1. Database Support
In this step we communicate with different types of databases, like SQL and Oracle. This means it can connect and retrieve information from a variety of database systems using Python, providing users with more flexibility and compatibility across various database environments.
Step 2. Data Extraction
In this step we are using python for our Extract, Transform, Load (ETL) processes this involves efficiently reading and extracting data from the connected databases. Python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in csv files which in turn are converted to .db files for sqlite.
Step 3. Fine-Tuning 
In this step fine-tuning mechanisms to optimize the performance and accuracy of data extraction processes. This Ensures the ETL tool finds data accurately and quickly.
Step 4. Integration with OpenAI
In this step we have utilized SQL Agent for communication with OpenAI, By communicating with OpenAI, the SQL agent get the ability to understand and respond in a more intelligent and context-aware manner.
Step 5. API Integration
In this step we made Django API endpoints for requesting and receiving data. This means that external systems or applications can interact with the SQL Agent through OpenAI by sending requests and receiving responses through these APIs.
Step 6. Streamlit Frontend
In this step we made a streamlit frontend to chat with the SQL Agent. The user can ask question about the database and receive responses in form of insights.

Video Demo








Previous articleML and AI-based insurance premium model to predict premium to be charged by the insurance companyNext articleChatbot using VoiceFlow Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2036,https://insights.blackcoffer.com/automate-the-data-management-process/,Automate the Data Management Process,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
Project Description
Businesses now have more access to data than ever before in today’s digital economy. This information is utilised to make key business choices. Businesses should invest in data management systems that increase visibility, dependability, security, and scalability to ensure that workers have the required data for decision-making. The client wanted to get the data management process automated using a tool from Python. Multiple operations like merging,sorting, filtering had to be performed on data from various resources. The data resources were mainly csv files and data from SQL queries in PostgreSQL. 
Our Solution
The project solution contained two tools that would aid in automatic efficient data storage. The first tool will concatenate all of the CSV files before merging them with the data from the SQL file. The acquired Excel file will be used as input for the second tool. The second tool will sort, filter, and lookup the Excel file received in the first tool. This tool will add columns that will be useful for the client’s analysis. The major goal is to assist the client with data management while requiring as little manual labour as possible. The files obtain the needed data in an Excel file by giving the proper input files.
Project Deliverables
The project deliverables can be divided into two parts:

 Excel Tool1: ExcelTool1 generates an Excel file that contains two sheets RSLTS IN and RSLTS OUT. The RSLTS IN is obtained by concatenating all the csv files in the Output folder. The RSLTS OUT is the result of merging the data from vwr egeas.sql query and RSLTS IN. 


Excel Tool2: Excel Tool2 creates another Excel file with one sheet RSLTS and csv files like vwr_instructions_new table, vwr proto and INST_RTR. This tool performs excel operations like lookups, arithmetic calculations and merging of data from multiple sources.

Tools used

For the whole data management and automation, we have made our own tool by python scripts.
PostgreSQL was used to merge the csv files provided by the client with the python scripts. 
The automation tool will store data in the excel sheets.

Language/techniques used

PyCharm for compiling and running the code.
The scripts for the automation tool were written in the Python programming language.
OS, glob, pandas, numpy and psycopg2 were thePython libraries used in the project.

Skills used

Configuration and Data moving using PostgreSQL.
Automation of tools
Exception Handling from Python

Databases used
Two types of databases were used: Google excel sheets and PostgreSQL.
What are the technical Challenges Faced during Project Execution
Some minor challenges were faced such as data discrepancies generated during the automation process.  
How the Technical Challenges were Solved
The challenges were solved by reworking on the automation tool and consulting with the clients for their requirements. 
Business Impact
It is critical to use appropriate data management procedures to ensure the smooth running of a firm. Furthermore, data management must be very precise, cost-effective, and completed as soon as possible. The inability to handle data can result in costly consequences and a permanent stain on the company’s image. Every company is responsible for developing a robust data management plan. The following are some of the reasons why data management is critical to the success of the firm. Instant Availability of Information: Data management makes information easily available for quick access based on company needs. Data management is also essential for accounting procedures like auditing and other strategy-based operations like company planning. The more time you spend hunting for misplaced files and missing documents, the less productive you will be. And you are aware that time is money. Keeping all of your documents structured might therefore assist to make procedures run more smoothly and quickly. Compliance: The government passed legislation requiring businesses to maintain these data. There are also periodical checks to verify that there is no manipulation. Furthermore, if a corporation is involved in a dispute, they must maintain these records for years until a solid verdict on the matter is reached. Faster Transitions to New Technology: Because technology trends change so quickly, organizations must embrace whatever comes their way. Losing information due to obsolete or outdated systems is the last thing you want for your company. Every piece of data preserved in the firm records is essential for everyday operations, managing multiple divisions, completing computations, audits, and so on. Make Right Business Decisions: Businesses use a variety of information sources for company planning, trend research, and performance management. To execute the same activity, different departments’ teams employ different sources of information. Because the legitimacy and precision of information are highly dependent on the source, analyzing several sources may have a detrimental influence on the organization. Robust data management prevents this from happening.
Project Snapshots

Fig.1: Python code of Exceltool1
Fig.2: Python code of Exceltool1

Fig.3: Python code of Exceltool2
Fig.4: Python code of csv tables
Fig.5: RSLTS_OUT worksheet in output Exceltool1
Fig.6: RSLTS worksheet in output Exceltool2

Fig.7: RSLTS worksheet in output Exceltool2

Fig.8: INST_RTR table as output from Exceltool2
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleRealtime Kibana Dashboard for a financial tech firmNext articleRise of Internet Demand and its Impact on Communications and Alternatives by the Year 2035. Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2037,https://insights.blackcoffer.com/realtime-kibana-dashboard-for-a-financial-tech-firm/,Realtime Kibana Dashboard for a financial tech firm,"

Client Background
Client: A leading fintech firm in the USA
Industry Type: Finance
Services: Financial services
Organization Size: 100+
Project Objective
Create a real-time Kibana dashboard to monitor the real-time movement and activities related to company/stock on the AWS to analyse data and get insights through dashboards to prevent due diligence. Dashboard should include visualizations of sentiments, FOIA requests, stock prices, volume, borrow rate, etc.
Project Description
Create real-time dashboards to get insights about the data and to analyse the relative change in different activities. Someone filing FOIA SEC request or FOIA FDA request and/or registering for conference calls might also have posted some negative tweets on tweeter to influence the market. Dashboard should display data of requests, sentiments, stock prices, etc on the same timeline, so that we will be able to observe the changes and relative changes with respect to time. Make separate dashboard for 2 stock symbols to analyse the activities and changes specific to that and a dashboard for all the data, eg. stocks, requests, etc. Change in sentiments effecting the price of the stock, borrow rate, trading volume, etc. should be noticeable. There is a list of names, make alert on the dashboard when the requests are filed by them on the same timeline used for other data. Also include the candlestick chart to view the stock details like open, close, high, low, volume with respect to time. 
Our Solution
For FOIA SEC and FDA requests, made a metric chart representing the total number of requests and requesters, created a date histogram to view the frequency of requests and requesters with respect to time, bar chart to view the top requester name, organization, category, pie chart to view the proportion of final disposition of requests and tag cloud for the description of the requests for the entries present in the selected time range and a search table that contains the selected columns (only relevant ones) for both SEC filings and FDA filings.
Similarly, for citation data, created a date histogram to view the frequency of citations and names of firms who posted with respect to time and bar chart to view number of citations by firm in the selected time range and a search table that contains the selected columns (only relevant ones). Index containing fail to deliver data is used to plot the date histogram in which volume failed is represented by the bar along the line representing the price at that time, bar chart where bars represents the total volume failed to deliver with respect to stock symbol and average price of the stock symbol in the selected time range by a dot size add on and tag cloud of the stock symbol as per fail to delivers.
For twitter data (short seller’s data), made a pie chart to show the proportion of polarity, metric table to show the highest 10 average retweets with respect to user name, made a date histogram to show the frequency of tweets as per time and another date histogram representing the amount of positive and negative sentiments with the help of bars as per time to leverage us to observe if change in amount of sentiments is affecting price of stock, volume in trade and fail to deliver, etc., bar chart to show the total posts and number of posts in the selected time range and another bar chart to show the count of followers and friends in the index in selected time range. A search table is made with columns like polarity, follower counts, retweets and post with timestamp to get precise info of what we have in visualizations.
For the list of names to be tracked on requests made and to make alert for them, added a annotation on the TSVB graph and added all of these along with the above visualizations on the dashboard on Kibana to make it a real-time dashboard and we can use this dashboard to do relative analysis.
For the dedicated dashboards to the stock, created and added following visualizations:

Metric to show number of requests and requesters in FOIA SEC and FDA indexes where description contains terms related to that stock symbol or product of the company.
TSVB of FOIA SEC and FDA and added annotation where the request against the stock or company is filed.
Fail to deliver and price on the same timeline to notice the relative change.
Sentiment and stock details is to be added in these but the data isn’t ready yet from the client’s end.

Project Deliverables
3 dashboards- 1 dashboard for complete data and 2 dashboards dedicatedly for one stock each. 
Tools used
Kibana and Elasticsearch
Skills used
Visualizations and analytical skills were used
Databases used
Following databases are used to:

FOIA SEC filings
FOIA FDA filings
Citations
Fail to deliver
Tweeter Short seller data
Stock price 

Web Cloud Servers used
AWS Management Console
What are the technical Challenges Faced during Project Execution
As I was using Kibana and studying the stock data for the first time, I faced challenges in making complex visualizations and understanding the terms related to stock data. Using filters while making Vega Charts to make candlestick chart with inconsistent data was displeasing.
How the Technical Challenges were Solved
Challenges related to the creation of complex visualization was solved exploring options on the Kibana and getting reference from the online sources. In order to understand the stock information and how things work, I got immense amount of knowledge from the client and from my project manager. For filtering of data in Vega charts I took help from the online sources.
Project Snapshots












Project website url

https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/33529a85d7949871c0833dab8c3b3322
https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/255f9ffe21bb76f96d1be5d49c7f75a7
https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/f8f6ad6a627f6f74ce2a775288bdbc5c

Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleHow To Secure (SSL) Nginx with Let’s Encrypt on Ubuntu (Cloud VM, GCP, AWS, Azure, Linode) and Add DomainNext articleAutomate the Data Management Process Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2038,https://insights.blackcoffer.com/data-management-etl-and-data-automation/,"Data Management, ETL, and Data Automation","

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
Project Objective
To extract the data for the given keywords from the listed websites https://www.ferguson.com/, https://www.bakerdist.com/, https://www.hajoca.com/, https://www.carrier.com/residential/en/us/, https://www.gemaire.com/, https://www.fwwebb.com/ and store the count of each keywords for each website it in an Excel File.
Project Description
A list of websites is provided from which we were supposed to find out the mentioned keywords and store their respective counts for each website in an Excel sheet with different tabs for different set of keywords.
Our Solution
We used Selenium as well as Bs4(Beautiful Soup) to extract data from the given websites. To accomplish the given task, 2 tools were developed for each website.

 Search tool was developed to search the keyword in the website’s search bar and count displayed for keywords of each category was stored in separate files.
Content tool was developed which scraped full text from each url obtained from the respective sitemaps. Along with the text visible on the page, data from meta keywords, meta description and title was also scrapped.

Extracted content from all the websites was stored in their respective text files. After that number of keywords in the text were counted using substring and count method and stored the keyword and its corresponding count in an Ordered Dictionary and then the count was transferred to a list and Excel file was created for the same. Counts received from search tool and content tool were combined and final output file was created.
Project Deliverables

Python Scripts for each website to extract the count of keywords.
Excel Sheet name HVAC_Report Test.xlsx having counts for each set of keywords for each website.

Tools used
Python Interpreter
Language/techniques used
Language Used: Python
Libraries used: BeautifulSoup, collection.OrderedDict, pandas, requests, xlsxwriter, selenium.webdriver
What are the technical Challenges Faced during Project Execution
Some of the websites cannot be accessed using Indian IP address as it was having captchas. Also, we cannot go to each and every page by clicking the results and get the count.
How the Technical Challenges were Solved
To bypass the captcha and reach the website, we need to use VPN of Singapore. And to get access to each and every page of the website, we found out sitemap for each website which includes link to every page present in it.
Project Snapshots










Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleData Management – EGEASNext articleDeploy Nodejs app on a cloud VM such as GCP, AWS, Azure, Linode Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2039,https://insights.blackcoffer.com/data-management-egeas/,Data Management – EGEAS,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
Project Objective
To extract various Reports from the given input files. Reports to be extracted are: PRODUCTION COST – ANNUAL BY UNITS REPORT, SYSTEM EMISSIONS ANNUAL REPORT, RPS CONSTRAINT – ANNUAL REPORT, RELIABILITY – ANNUAL REPORT, RESERVE – ANNUAL REPORT and CAPACITY TOTALS ANNUAL REPORT. We had to extract above mentioned reports from the given .out files and store it in the respective .csv files.
Project Description
We were given a bunch of .out files in which various Reports were available in table format. We need to extract some of the required reports from the given files and store them in their respective .csv files. A tool had to be developed in python in order to accomplish this task.
Our Solution
From each .out file its content extracted and stored in a list. Using regular expression, we searched the required report in the content. Another regular expression is used to mark as end of the table content. Content between the two given regular expressions is stored in a dataframe which is then stored into respective .csv file. 
Project Deliverables

Python Scripts for each report and a combined script which could extract all the required Reports.
Respective .csv files of the Reports 

Tools used
Python Interpreter
Language/techniques used
Language Used: Python
Libraries Used: re, pandas, os
Skills used
Programming
Project Snapshots
 









Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleDesign and develop PowerShell scriptNext articleData Management, ETL, and Data Automation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2040,https://insights.blackcoffer.com/design-and-develop-powershell-script/,Design and develop PowerShell script,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
Create a PowerShell script for the following:

check and enable auditing:- client wanted a PowerShell script that checks NTFS Rule is given to a folder or not and adds a rule to it
configuring winrm for remote windows server:- this client wanted a PowerShell script which helps us to connect to another windows remote server
check audit of windows/system32 folder and windows/inf folder of remote windows server:- this client wanted a PowerShell script which help us to connect to the remote server and check their  NTFS Rule for windows/system32 and windows/inf folder also we can add rule for those folders

Our Solution
check and enable auditing
for checking and enabling auditing of the file we used  PowerShell NTFSSecurity module

for checking the audit we used Get-NTFSAudit which is a submodule of NTFSSecurity
for adding the audit we used Add-NTFSAudit which is a submodule of NTFSSecutiry

configuring winrm for remote windows server
For this we created 2 script:

create script: this help us to create listener and open port 5986 for http as winrm uses port 5986 to connect with windows
connect script: this help us to connect with remote windows server for this purpose we used Enter-PSSession

check audit of windows/system32 folder and windows/inf folder of remote windows server
for this, we created a script that connects to the remote windows server using the Enter-PSSession command and then checks the audit for windows/system32 and windows/inf folder also we can add audit rule to windows/system32 and windows/inf folder from remote servers
Deliverables
Powershell script
Tools used

VS Code IDE
Powershell
Virtual machine 

Language/techniques used
powershell
Skills used

Powershell

BuProject Snapshots
Check audit 

Add audit

Check audit

Before running create script

Create script for winrm listner

List of listeners after running create script

Connect with remote machine

When rights are not applied

When rights are applied

Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleDesign and develop Jenkins shared libraryNext articleData Management – EGEAS Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2041,https://insights.blackcoffer.com/design-and-develop-jenkins-shared-library/,Design and develop Jenkins shared library,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
Create Jenkins shared library for the following:

validate AWS AMI creation
check if network rules exist in aws EC2
check if the security group in aws EC2

Our Solution

We created a Jenkins shared library in which we are using AWS  ec2 describe-images command with the help of aws cli if an ami don’t exist than describe-images throws error
We created a Jenkins shared library in which we are using aws ec2 describe-network-acls  for validating we were comparing input name with VPC
We created a Jenkins shared library in which we are using aws ec2  describe-instances for validating we were checking input name with SecurityGroups group

Deliverables
Jenkins Libraries
Tools used

VS Code IDE
Jenkins
AWS

Language/techniques used

Grovvy

Skills used

Jenkins
AWS Server

Web Cloud Servers used
AWS
Project Snapshots



Project Video




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.





Previous articleDesign and develop retool app for wholecell.io and Asana data using their api’sNext articleDesign and develop PowerShell script Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2042,https://insights.blackcoffer.com/design-and-develop-retool-app-for-wholecell-io-and-asana-data-using-their-apis/,Design and develop retool app for wholecell.io and Asana data using their api’s,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
Create retool app for wholecell.io and Asana data using their api’s
Our Solution
We have created two table one table contain data from wholecell.io platform and another table contain data from Assna. 
In that wholecell.io table we are providing:

Order id
Order status
Order channel
Organization 
Link of the Order

In Assna Table we are providing following details:

Id of the task 
Name of the task
Resource type
Resource_subtype
Caller
Po-id 

As client data from wholecell and Assna was linked client can search the order by PO-id in Assna table
Deliverables
App in retool
Tools used

Retool

Language/techniques used

JavaScript

Skills used

Retool
API integration
JavaScript

What are the technical Challenges Faced during Project Execution
Api was not providing all required details according to the client requirement and there were less options for data pre-processing as retool only javascript 
How the Technical Challenges were Solved

We had fetched details from one api and provide id to the other api using JavaScript this was done by using javascript promise method
We also had to do some string manipulation to get data according the client requirement

Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleDesign and develop a retool app that will show stock and crypto related information using IEX APINext articleDesign and develop Jenkins shared library Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2043,https://insights.blackcoffer.com/design-and-develop-a-retool-app-that-will-show-stock-and-crypto-related-information-using-iex-api/,Design and develop a retool app that will show stock and crypto related information using IEX API,"

Client Background
Client: A leading fintech firm in the USA
Industry Type:  Finance
Services: Crypto, financial services, banking, trading, stock markets
Organization Size: 100+
The Problem
Create a retool app that will show stock and crypto related information using IEX API
Our Solution
Created a flask web application with following features and pages:
Page 1 (Home page)– Show a Stock & Crypto Search Bar that will show the most relevant option in the IEX API via ticker search. Upon submit, user will be taken to the “Ticker Page”– List the 10 top trending stocks for each category (link click to ticker page)(logo, Stock ticker, company name, stock price, % change.Mega CapLarge CapMid CapSmall CapMicro Cap
Page 2 (Ticker Page)
-Show Company Data – (Ticker, Company Name, Logo, Market Cap, and all the other corporate data (employees, CEO, HQ, Founded, Website)-Stock Price Chart – 1 year chart, daily.-Stock Price Volume – Weekly average 20 weeks
-Recent News – list of 25 most recent articles
Deliverables
Deployed flask web application on AWS
Tools used

VS Code IDE
Nginx

Language/techniques used

Python 

Skills used

API Integration
Python
AWS Server
Nginx

Web Cloud Servers used
AWS
What are the technical Challenges Faced during Project Execution

There was lots of pre-processing required to create application as per client requirement

How the Technical Challenges were Solved

We shifted the application from retool to python flask application as python programming language allow as to pre-process the data as per our requirement

Project Snapshots








Project website url
www.stocks.bullish.studio
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleCRM (Monday.com, Make.com) to Data Warehouse to Klipfolio DashboardNext articleDesign and develop retool app for wholecell.io and Asana data using their api’s Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2044,https://insights.blackcoffer.com/crm-monday-com-make-com-to-data-warehouse-to-klipfolio-dashboard/,"CRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  IT
Services: Marketing, promotions, campaigns, consulting, business growth
Organization Size: 100+
The Problem
The client requires a dashboard for a ”week in review” and “human resources”. The dashboard should be dynamic whenever the client opens the dashboard, it should show the current week and should also have a dropdown choice option based on different time periods. So the client requires a meaningful KPI on the dashboard.
Research Objective
Taking the problem statement into consideration the following objectives are established. 
Objective 1: Getting access to the Monday.com site, Make.com, Google sheet, and Klipfolio. Objective 2: Connect Monday.com data to the Google sheet. 
Objective 3: Data Integration using make.com. 
Objective 4: Building KPIs using various calculations and formulas to get meaningful insights. 
Objective 5: Creating a dashboard from insight driven by KPIs.
Solution Architecture
1. Data Integration

Fig.3.4: Data Integration
2. Overall Architecture

Fig.3.4.2 Overall Architecture
Tools used
Klipfoliomake.com
Language/techniques used
Klip Formula
Skills used
Data IntegrationData ProcessingData Visualization
Web Cloud Servers used
Google Sheet
What are the technical Challenges Faced during Project Execution
During the project execution we faced following challenges:1. Mapping the values in make.com from Monday.com2. Whenever the update is generated on Monday.com, a new row is added to the Google sheet. 
3. Extracting insights from the data
How the Technical Challenges were Solved
To solve the technical challenges, we provided following solutions as follow:1. For mapping the values from Monday.com to make.com, we got access as admin to reach out the columns id on Monday.com.
2. On make.com, we created multiple models linking each other based on the row id in the google sheet.
3. After completing the data integration, we use calculations to extract meaningful insights from the data.
Business Impact
Using this dashboard, a client can keep track of the employee’s work process. So he can analyze employee workflow nature.
Project Snapshots




Project website url
Google Sheet:https://docs.google.com/spreadsheets/d/15ADtNWh63O7DVbg-FRH0SmWb-TemqldOVK7dq16N7Xs/edit?usp=sharing
Data Integration using make.com:https://us1.make.com/146703/scenarios?folder=all&tab=all 
Monday.com:https://primus-business-management.monday.com/ 
List Of Employees listed on Klipfolio:https://app.klipfolio.com/clients/index 
Klipfolio Dashboard:
https://app.klipfolio.com/dashboard?tab=012f404bf82f8b4e331c4a0c48d32978#:~:text=https%3A//app.klipfolio.com/dashboard/add_tab/8ca9ae6808284b158f640834f3e2afd8%3Fparam%3AstartDate%3D1671926400%26param%3ADatepickerB%3D1671753600%26param%3ADatePickerA%3D1671408000%26param%3Adropdown%3DWorking%20on%20it%26param%3AendDate%3D1672444800%26param%3AKTdate%3DFY%20to%20Last%20month%26param%3ADatePeriodq%3DThis%20Week
Project Video
Todo Board Part 1: https://www.youtube.com/watch?v=qnTV64RhGWk 
Todo Board Part 2: https://www.youtube.com/watch?v=vDyaVkNv6bU 
Todo Board part 3: https://www.youtube.com/watch?v=FciSkP-uRkM 
Census Board Part 1: https://www.youtube.com/watch?v=jpgzakxdvZw 
Census Board Part 2: https://www.youtube.com/watch?v=3y6DmUGNmTE 
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleNER Task using BERT with data in XML-formatNext articleDesign and develop a retool app that will show stock and crypto related information using IEX API Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2045,https://insights.blackcoffer.com/ner-task-using-bert-with-data-in-xml-format/,NER Task using BERT with data in XML-format,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
The goal of this task is to create and implement a workflow that annotates People/Places/Organizations and assigns them a specific number (from a normdatabase). The NER-Task should be done by using Bert (NER-German https://huggingface.co/flair/ner-german or something similar).
Our Solution
 The input to this first task is a text in XML-Format. It is important that the structuring text is not altered by the NER. This could be possible by tokenizing the XML-elements in a different/seperate way, to then run the NER with BERT and afterwards add the elements afterwards at the exact position where the initially were. The tags that were added by the NER than can be easily replaced with the required tags in the XML-format. 
Solution Architecture
Input Data 🡪 XML Text Tokenization 🡪 NER Model 🡪 Replace NER Tags with XML Tags 🡪 Final Output
Deliverables
Python tool
Documentation
Installation 
Tools used
VSCode For Python script
Language/techniques used
Python Programming Language
Models used
Named Entity Recognition (NER) 
FuzzyWuzzytqdmFlairPandas
Skills used
Data LoadingData ProcessingData Restoring
What are the technical Challenges Faced during Project Execution
During the project execution, we faced the following challenges:

Parsing of the input XML file.
Predicting the Name, Place and Organization.
Rearranging the XML file to its origin form with the predicted value.

How the Technical Challenges were Solved
To solve the technical challenges, we provided following solutions as follow:

It was not possible by the beautiful soup library. So by using the logically function start index and end index we break the sentence.
For predicting the NPO we used the flair ner-german model.
To rearrange the file we used start index and end index function which can be split with a certain condition and we place the predicted value in it.

Business Impact
The client can know easily predict the Name, Place, and Organisation from XML containing file by using our python script model.
Project Snapshots 

Fig. Input XML file

Fig. Output XML file with predicted values.
Project website url
Github: https://github.com/AjayBidyarthy/Sven-Meier-XML-tool/tree/master
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleQualtrics API integration using PythonNext articleCRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2046,https://insights.blackcoffer.com/qualtrics-api-integration-using-python/,Qualtrics API integration using Python,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
API Integration to read/write data in SQL tables from an online application.
Our Solution
To write the api between qualtrics and sql server using python programming language.
Solution Architecture

Fig. System Architecture
Deliverables
Python Software
Documentation
Tools used
PythonQualtrics
Models used
PandasRequestsnumpyZipfileiopyodbc
Skills used
Extract Transfer Load
Databases used
SQL Server
What are the technical Challenges Faced during Project Execution
During the project execution, we faced the following challenges:

After data integration, the content of the file was not readable.
Mapping the values with the required columns.

How the Technical Challenges were Solved
To solve the technical challenges, we provided the following solutions as follow:

To get the content into the CSV format after integration we used the Io module to get the text content.
To get the mapping values we created the CSV file and store the record in it and fetch that record to the SQl.

Business Impact
Using this script the client can now fetch the Qualtrics data into the SQL server automatically after every 1 hour.
Project Snapshots 

Fig. Data in CSV Format
Fig. Data in Table form

Fig. SQL data
Project website url
Github:  https://github.com/AjayBidyarthy/Richi-S-api
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleDesign and develop MLops framework for Data-centric AINext articleNER Task using BERT with data in XML-format Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2047,https://insights.blackcoffer.com/design-and-develop-mlops-framework-for-data-centric-ai/,Design and develop MLops framework for Data-centric AI,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
The task involves finding models and tools for several different tasks across various domains. The tasks include video and image capturing, working with documents such as PDF and Excel files, converting text to audio, audio capturing and transcription, translation to major languages, utilizing language models with a focus on Jina finetuner and its limitations, creative AI for generating pictures and designs, synthesizing language texts, creating Kibana dashboards and data storytelling, code creation for specific platforms like Editorjs and Nextjs, integrating Jina API inference into function blocks in Editorjs/Nextjs, UX/UI creation for the front end of Editorjs and Nextjs, transfer learning and reinforcement learning, utilizing Wikipedia for general knowledge, and utilizing an epistemic model called EPINET. To fulfill this task, you will need to search for relevant models, tools, and resources specific to each task mentioned above.
Our Solution

Jina AI Hub to deliver an ecosystem of:
Core transformer model
Distilled & Fine tuned models
OKR:s/KPI:s +domain data = “Book of knowledge” + model = AI agents 
Ensembled models = AI teams 
Also delivered functions in the marketplace
Voice interface, OpenAI Whisper transformer
Multiple data types capturing of information (DocArray)
CLIP model to mesh multiple data types into vectors
Neural Search function and 
Generative AI function
Automatic data labelling
Used weight watcher to fine tune the model quality without CPU/GPU cost

Solution Architecture

Automatic selection a model for fine tuning with data corpus (book of knowledge), given the best performance.
Add the model to an API inference
Unlike ChatGPT the model can specify when they don’t know and acknowledge it instead of making stuff up with its creative ability.
When the model knows what it doesn’t know, it can ask to go back and consult other models for joint predictions.
Add a function to select ensembled models for joint prediction when step 4 occurs.

Deliverables

Identify core transformer models
“Clean” and stabilize selected core models
Set up the process in Jina Hub
Integrate FastAPI/Jina with our Jina Hub
Integrate FastAPI/Argilla/Kibana into our Jina Hub

Tools used
Jina Hub/AI, Python, Hugging Face, Argilla, Redis stack, Kibana
Language/techniques used
Python
Models used
Epistemic Neural Nets, weight watcher, OpenAI Whisper transformer, Epinet
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleNLP-based Approach for Data TransformationNext articleQualtrics API integration using Python Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2048,https://insights.blackcoffer.com/nlp-based-approach-for-data-transformation/,NLP-based Approach for Data Transformation,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
Performing Readability and Quality testing on the text corpus from text files
Our Solution
The intention was to create a tool/system that can consume text files through a given csv file having a path for all the text files through this csv file our tool should be able to read all files one by one and could perform some tests and analysis on that text data and output the results in a csv format presenting all the metrics. 
In order to achieve this goal we created a Python-based ready-to-use code that will read all text files presented in the given csv files and perform 14 different evaluations on that text data and save the results in a excel and csv based format.
Solution Architecture

Deliverables
The final deliverable was the tool/system/code for processing and evaluation text.
Language/techniques used

Python

Natural Language processing technique used for text evaluation



Skills used
Python Programming
What are the technical Challenges Faced during Project Execution
The architecture of the solution for this project problem statement was simple, no challenges were faced during the execution of the project.
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAn ETL tool to pull data from Shiphero to Google Bigquery Data WarehouseNext articleDesign and develop MLops framework for Data-centric AI Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2049,https://insights.blackcoffer.com/an-etl-tool-to-pull-data-from-shiphero-to-google-bigquery-data-warehouse/,An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
Shiphero company is an organization providing shipping solutions to vendors. The data created by shiphero for different product picking and packing time period doesn’t provide much insight into the efficiency of ship hero employees and other aspects that are needed and useful for vendors/brands to make better decisions for their business in order words the ‘key’ data is missing.
Our Solution
The solution is an effort to create the missing data by the existing data as we came to know that the ‘key’ data can be created by involving some deep methodologies and vast logical aspects linked to it. The incoming data from shiphero company is timestamp data therefore using this sequential data we can create the missing data we need to get the required KPI’s. 
The overall architecture included getting data from shiphero through api doing some preprocessing and creating our ‘key’ through this data and populating it on Google big query. This google big query is linked to Google data studio for insights visualisation.
Solution Architecture
The data coming from Shiphero is extracted every day using a cron job scheduler. Google app engine service is used to preprocess and apply a transformation to the data.

Deliverables
Ready-to-use Google data studio Dashboard. Google app engine service-based scheduler code.
Tools used

Google App engine
Google big query 
Google data studio
Google cloud platform

Language/techniques used

Python (for preprocessing)
GraphQL (For data extraction)

Skills used

Python Programming
GraphQL querying
Statistics
Data visualization
Data Engineering
Data Science

Databases used

Google big query

Web Cloud Servers used

Google Cloud platform

What are the technical Challenges Faced during Project Execution
Initially the approach client introduced that could be able to solve the problem directly failed to give proper results and because of that we need to come up with a solution that could be able to estimate our ‘key’ column to some extent.With the way around solution using statistics and data modelling there were a series of challenges coming that were creating a question mark for us but with keen solution building and delivering the desired results we came to solution for every challenge that arose. 
How the Technical Challenges were Solved
Statistics was the only way around for the challenges we faced because it was the data which was missing and as the incoming data was in sequential format so we were able to figure out the patterns from that and the main problem of missing data for our KPI’s
Business Impact
Better insights into the business. 
Project Snapshots


Dashboards aren’t finalised but yes giving desired solutions.
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articlePlaid Financial Analytics – A Data-Driven Dashboard to generate insightsNext articleNLP-based Approach for Data Transformation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2050,https://insights.blackcoffer.com/plaid-financial-analytics-a-data-driven-dashboard-to-generate-insights/,Plaid Financial Analytics – A Data-Driven Dashboard to generate insights,"

Client Background
Client: A leading financial firm in the USA
Industry Type:  Finance
Services: Financial Services
Organization Size: 100+
The Problem
Applying automation to Financial data coming from the Plaid platform that needs to be visualized in order to get better insights and metrics from data.
Our Solution
The intention was to create an automation tool that could consume the financial csv format data and perform preprocessing on that data and could directly present the insights on visually appealing dashboard.
Initially the step was to create a tool/website that could consume the data and preprocess it and send it either directly to dashboard or into a database so the data could be safe and through the database the dashboard could be linked and updates accordingly. 
The data source for the tool was to be a manual entry therefore we created a website and hosted it on a cloud platform(Heroku) to make it available all the time for all the desired users. The processed data from this tool will be send to the Google big query database and our GBQ will be linked to the Google Data Studio for the insights presentation. Therefore as the data will keep on updating in the google big query accordingly the dashboard in our google data studio will gets updated.
Solution Architecture

Deliverables
The final deliverable was the ready-to-use dashboard and website where the preprocessing of the data happens.
Tools used

Google Cloud platform – Google Big Query (Database)
Google Data studio(Visualisation/Dashboard)
Heroku Cloud(Hosting the web application)

Language/techniques used

Python

Skills used

Python programming
Data analytics/Visualisation
Google Big Query

Databases used

Google Big Query

Web Cloud Servers used

Heroku Cloud

What are the technical Challenges Faced during Project Execution
The project was easy to implement and the architecture was simple therefore no major challenges were encountered.

Project Snapshots 

Project website url
https://plaid-conversion.herokuapp.com/
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleRecommendation Engine for Insurance Sector to Expand Business in the Rural AreaNext articleAn ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2051,https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-the-rural-area/,Recommendation Engine for Insurance Sector to Expand Business in the Rural Area,"

Client Background
Client: A leading insurance firm in the globe
Industry Type:  Insurance
Services: SaaS, Products, Insurance
Organization Size: 10000+
Project Objective

Develop the recommendation engine 
Item-based collaborative filtering based on the use case of the project
Work on Streaming data platform i.e BangDb 
Data Generation for Testing the platform

Project Description
BangDB is the platform that manages the static data stored on the cluster and also works with live streaming data as Hadoop does. Wherever the bangdb is able to manage machine learning model deployment with their inbuilt parameter and hyper tuning parameters for each model.
Streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as Insurance, loans (Business Loans and Personal Loans), Mobile recharge, UPI transactions done by their platform, etc.
 They wanted the recommendation of other services provided by them to each of their customers who are using their platform.
Our Solution
This Project Module develops according to the Clients Requirements which involves item-based collaborative filtering based on customer behaviour, Firstly classify the customers into various segments on the basis of age, location, gender, and product usage. On the basis of RFM (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaborative filtering.
We generated the synthetic data (90 Million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers.
Project Deliverables
       –   KPI of the Customers
       –   Recommendation model
       –   Graph databased model
       –   Data Generation code based on python (using copula-based on PyTorch)      
Tools used

BangDb Tool (ML, AI, NoSQL database supported)
Graph Databased
Google Colab (Data file generation)
Tableau for data visualization 

Language/techniques used

Linux cloud machine
Python
Graph Database
Data visualization tools

Models used
-K means model for clustering
-Recommendation Engine model
-Collaborative based filtering model
Skills used
– Machine learning
– NoSQL Database 
– Graph database
– Data Generation using python
– Linux 
– Data Visualization
Databases used
– BangDB
– Graph Database
– Microsoft MYSQL server
Web Cloud Servers used

AWS cloud service

What are the technical Challenges Faced during Project Execution

Decide the Recommendation Engine based on the use case
Finding the RFM score and classifying the customers into clusters
Graph Model to define the relations of customers with each service which they are using 
Synthetic data generation(90 Million events) and around 1.5 Gb structured data.

How the Technical Challenges were Solved

Item-based collaborative filtering solves the issue of recommendation because we are dealing with almost 14- 15 services.
Clustering of customers based on their similarities 
Measure the RFM score, and group and classify them based on their scores.
Graph database provides to reduce complexity and increase the processing speed.
Data generation is one of the difficult tasks and generating relational data across 29 different streams using copula and UUID python library function which is based on PyTorch.

Business Impact

It is Qualitative and Quantitative impact on economically where customers are a direct impact of these projects in their life.
It is suggesting to the customers what services they have to utilize from the provider and this is a direct impact of the product on the customers.
Product is providing the action statement of the usage of services by the customers and impacts them economically as well.
The scope impact of product service is Nationwide or statewide.
To provide these impact-full services, there is a tech team of Blackcoffer behind it

Project Snapshots 





Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleData from CRM via Zapier to Google Sheets (Dynamic) to PowerBINext articlePlaid Financial Analytics – A Data-Driven Dashboard to generate insights Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2052,https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/,Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI,"

Client Background
Client: A leading solar panel firm in the USA
Industry Type: Energy
Services: Solar Panel
Organization Size: 500+
The Problem
Solar Panel organization from America wants to keep track of sales data. They want to see the leadership dashboard of their organization in terms of sales. They also want to keep track of their campaigns and leads generated from sources of those campaigns. They want to keep track of sales data from different sources.  
Our Solution
First, we fetch the data from CRM to PowerBI. Clean the data of CRM using DAX and then perform calculations on the data. Using cleaned data, we build KPI on PowerBI.
Solution Architecture
To complete the project, we follow the following data flow pipeline:
Data from CRM 🡪 Zapier 🡪 Google Sheet (Dynamic) 🡪PowerBI 
Language/techniques used
PowerBI, DAX Language
Skills used
CRM, Zapier , PowerBI, Google Sheet
What are the technical Challenges Faced during Project Execution
Challenges Faced during the Project Execution :

Fetching the data from CRM 
Unclean Data
Merging the Data 

How the Technical Challenges were Solved
Solution:

To Fetch the data from CRM. We used Zapier. It is connector between two applications so that whenever a particular incident happen it will populate into another application. We use Zapier to connect CRM and Google sheets so that whenever a new lead will change or modified data will be stored into google sheets.
Data in google sheets was uncleaned. First, we connect the Google sheet with PowerbI then perform EDA to clean the data using DAX language.
Using merging of two tables by ONE-ON-ONE schema we solve duplicate entries of a particular lead in PowerBI.

Business Impact
Using this Dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a Sale. They can keep track of their sales leadership of employee of the month in term of sales.
Project Snapshots
CRM  
Zapier

Dashboard 

Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleData Warehouse to Google Data Studio (Looker) DashboardNext articleRecommendation Engine for Insurance Sector to Expand Business in the Rural Area Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2053,https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/,Data Warehouse to Google Data Studio (Looker) Dashboard,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products, healthcare, government, energy
Organization Size: 100+
The Problem
Our client needed a Google Data Studio dashboard for different sectors such as Oil and Gas, Government, Healthcare, and Sales analysis. They want to see an analysis of data from which they can provide insights in different domains. They want us to create visual KPIs of meaningful insights.
Our Solution
They provided us with data for different sectors. Using those data first we analyze the data and perform EDA on data for cleaning the data. After cleaning the data, we performed calculations to extract insights for KPIs. Using those KPIs we build a dashboard on Oil and Gas, Government, Healthcare, and Sales analysis.
Solution Architecture
To build the dashboard we follow the pipeline as follows:
Data 🡪 EDA(Cleaning data )🡪 Connection(GDS) 🡪 Building KPIs(Visuals)
Tools used
Google Data Studio
Skills used
EDA, Google data studio
What are the technical Challenges Faced during Project Execution
During the project execution, we faced the following challenges:

The data client provided was not cleaned.
Data was of four different sector which we have to analyse and visualize.
Extracting insights from data.

How the Technical Challenges were Solved
To solve the technical challenges, we provided following solutions as follow:

Performed EDA on data to clean it and find the missing values.
As data was from different domains, we have analysed each sector and understand the culture of each domain. We understand the pipeline and flow of work process.
After completing the case study, we use calculations to extract the meaningful insights from data.

Business Impact
Using these dashboards client can visualize the sales insights and understand the workflow. They can take crucial decisions based on these insights which will help them to make an impact on their sales.
Project Snapshots
Sales Dashboard:


Government Dashboard:


Oil and Gas Dashboard:


Hospital Analysis:


Project website url
Dashboards on Google Data Studio:
1.Government:- https://datastudio.google.com/reporting/dda94ce8-5b77-46aa-a1e0-1a57ccaef5f9
2.Oil:- https://datastudio.google.com/reporting/47c6529e-1355-4072-babf-1a96f9f842cf
3.Healthcare:- https://datastudio.google.com/reporting/b1e95a11-4380-465c-ad45-2d1995c799fb
4.Sales:- https://datastudio.google.com/reporting/36ec0e42-6b77-4fbb-9dea-760cccaa741f
Project Video












Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleCRM, Monday.com via Zapier to Power BI DashboardNext articleData from CRM via Zapier to Google Sheets (Dynamic) to PowerBI Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2054,https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/,"CRM, Monday.com via Zapier to Power BI Dashboard","

Client Background
Client: A leading solar panel firm in the USA
Industry Type:  Energy
Services: Solar Panel
Organization Size: 200+
Project Description
Mohsin has Solar Panel Company. He has setup CRMs for that. He wanted to use CRMs data and want to visualize the leads in PowerBI
Our Solution
First, we check CRMs thoroughly and understand the work culture of his company. It was not easy to fetch data into PowerBI using API key. To fetch new leads from CRMs we used Zapier. The limitation of Zapier is it cannot fetch historical data into spreadsheet. So we download data from CRMs and fetch it into spreadsheet. For new leads we created zaps for every instance. After that we connect the spreadsheet with PowerBI and clean the data accordingly. Using that data, we build KPIs according to client need.
Tools used
API , Zapier , Spreadsheet , PowerBI
Language/techniques used
 M language , DAX
Skills used
API , M language , DAX , PowerBI
What are the technical Challenges Faced during Project Execution?
First challenge was to fetch data from CRMs using API key. Data we were getting was uncleaned and were not able to fetch all data. If there were multiple pages in the CRMs we will not be able to fetch all data from the pages.
How the Technical Challenges were Solved
Technical challenge in this project was to extract data from CRMs. So for that we used Zapier connector from CRMs to spreadsheet. But there was some limitation with Zapier that it will not fetch the historical data of our CRMs. So to solve that we download all historical data from CRMs and append it to the spreadsheet we were using. We fetch new leads to our spreadsheet using Zapier. By doing this now we have all the data historical and new lead which will be pushed by Zapier.Then we fetch the data to our PowerBI and do some cleaning in data. By using cleaned data, we build the KPIs for our client according to there requirements.
Business Impact
Client will be able keep track on his company data on PowerBI and it helps them to make decisions accordingly.
Project Snapshots 
CRMs


Zapier


PowerBI Dashboard

Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleMonday.com to KPI Dashboard to manage, view, and generate insights from the CRM dataNext articleData Warehouse to Google Data Studio (Looker) Dashboard Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2055,https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/,"Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data","

Client Background
Client: A leading energy firm in the USA
Industry Type:  Energy
Services: Solar panel
Organization Size: 200+
Project Objective

Setup a dashboard on Monday.com
Fetch client CRM data onto Monday.com dashboard.

Project Description
Mohsin has CRM for his business where he has all data regarding leads of his clients. He wanted to see all his client appointments at one place. Client took subscription of Monday.com. It is an CRM where you can manage your work more easily in neat and clean user-friendly environment. We can easily track our task on Monday.com. Pipeline for Monday.com is very easy to use and also customized according to our needs.
Our Solution
The challenging part of this project was to get CRM data on Monday.com dashboard. Client also has subscription of Zapier. Zapier is a connector which connect two apps to transfer data from each other. Zapier also has limitation to fetch limited type of data from CRM. Like for Mohsin CRM we can only fetch hot lead comes on CRM. But in his CRM there are also other functions like if a customer lead comes on CRM. They manually book appointment for that client. So there is no way to get that data from CRM. Issue for client was he has attached integrated four google calendar account with CRM so whenever he confirms appointment on CRM that data fetched on google calendar. But he has check manually one by one on each calendar which was bit hard task for him. So, we advised Monday.com where he can track all his task at one place.
Tools used

Monday.com 
Zapier
Google Calendar

Databases used

Google Calendar

What are the technical Challenges Faced during Project Execution
The challenging part of this project was to get CRM data on Monday.com dashboard.There is no direct integration of CRM and Monday.com to fetch data.
How the Technical Challenges were Solved
To solve challenges, we used Zapier to get CRM data to dashboard of Monday.com. We used google calendar of client which were integrated with CRM. All the appointment confirmed leads were present on google calendar.Pipeline of Data:
  CRM 🡪 Google Calendar 🡪 Zapier 🡪 Monday.com
Business Impact
Using the Monday.com dashboard client can easily track all appointments of customers. He can track data of his team members and connect with them at one place. He will not miss any of his meeting with customer. Monday.com also has timeline and calendar view using that client can see all activity of his work.
Project Snapshots
CRM Calendar view

Monday.com

Google Calendar

Zapier
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleData Management for a Political SaaS ApplicationNext articleCRM, Monday.com via Zapier to Power BI Dashboard Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2056,https://insights.blackcoffer.com/data-management-for-a-political-saas-application/,Data Management for a Political SaaS Application,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: SaaS, Products
Organization Size: 100+
The Problem
As per the guidelines and discussion. Political Research Automated Data Acquisition (PRADA) in the following phases which included.
1. Get pics for existing EOs (Elected Officials)
2. Get new EOs and Pictures.
3. Run QA checks regularly on EOs
4. Get data from government Facebook pages.
5. Geospatial project: Create a new version of provided KML without using google earth.  Creating a nested directories which contained description and Map-URL at the designated location.
6. Get data of US States and Counties(Including Boroughs and Parishes)
By building an automated generated structured data that allows a non – programmer to create a config for each page allowing a bot to scrap and update the data.
Our Solution
We created an automated python scripts for designated phases with respective requirements. Solutions to various type of problems varied such as most of data scrapping automation was done through python developed scripts including the geospatial KML task. In addition to this different ranges of data was scrapped generated directed output for the respective tasks in the form of CSV format. So the user’s main aim requirement was achieved i.e. a non programmer could create a con-fig and initiate a bot to scrap the required data.
Solution Architecture
The majority task of project consisted of web data scraping automation so a high- level overview, and specific implementation details of project shall will be as follows:

Web Scraping Framework: Python as a coding language was used in almost all of the tasks and the framework used for data scraping included Beautiful-Soup, Selenium and Web drivers. These libraries provide tools and functionalities to navigate web pages, extract data, and handle various HTML elements.
Data Extraction and Parsing: Use the selected web scraping library to extract the desired data from the web pages provided either in the data sheet or within the websites of URLs given in the sheet. This involves locating HTML elements, applying filters or selectors, and parsing the extracted data.
Data Processing:  Followed by data extraction it was cleansed, transformed and aggregated to a structured form such as pandas’ Data Frame followed by a CSV file. In the case of geospatial task it resulted to generation of nested folders in a kml file.
Data Storage: The how and where to store the scrapped data was determined which is local file system in the form of CSV (Comma Separated Values). As it was the appropriate data storage solution according to need of the project. In addition to this the geospatial task had the output in the form of kml file as polygons inside directories of nested folders. 

Deliverables
TasksOutputs (CSV/KML/XLSX)Python Scripts Canada EOs mydata.csvScript1.pyScript2.pyGeospatial TaskElectoral Districts.kml–Facebook Scrapping of EOs EO_OUTPUT_O.csvfinal_eo_scrapping.pyFacebook Scrapping of 429 CitiesOutput_DRAFT_429_CITIES.csvFacebook_image_scrapping.pyUSA States Website URLsScreenScrapingt.csvfinal_50_states_scrapping.pyUSA Counties Website URLsUS Website_final_write.xlsxcounty_scrapping.py
Tools used

Python (Programming Language) 
Beautiful Soup 
Selenium 
Pandas 
Numpy
Simplekml 
re (regular expressions)

Language/techniques used

Python (Programming Language) – It is an interpreted language, which allows quick prototyping and interactive coding. Its versatility can be is one of the reasons for its major applications. Different libraries and tools were used in this project for various data solutions.
Beautiful Soup –  A python library used for web scraping and parsing HTML and XML documents. It provides a convenient way to extract from the said files. It eases out the work flow from parsing to data extraction and encoding handling as well.
Selenium – A python library used for web browser automation like Chrome, Firefox, Safari and others. It interacts with elements such as clicking buttons, filling out forms and selecting drop down options. In this project we used it in Chrome. Selenium web driver was used for web automation. It acted as bridge between Python code and the Web browser.
Pandas – It is Python’s versatile library that provides high performance data structure tools and it is built on top of Numpy. Data Frame is one of its key feature due to which this library was used. This key feature allows efficient manipulation, slicing, and filtering of structured data
Numpy – It is also a python library aka Numerical Python as it is a fundamental library for scientific computing in Python. 
Simplekml – It is a python package which enables you to generate KML with as little effort as possible.
 re (Regular Expressions): It is a powerful tool in python sued for pattern matching and manipulations of strings.

Skills used

Python Programming
Web Fundamentals
Web Scrapping using libraries such as BS, Selenium.
Data Cleaning and Processing
Problem- Solving and debugging.
KML structure and handling using Python’s programming.

Databases used
None. All the structured data was in the form of either python Data Frames, CSV or Excel Sheets.
What are the technical Challenges Faced during Project Execution

Firstly some of the web URLs were not accessible because they were restricted to particular range of IPs of that region
Couldn’t fetch whole data through Beautiful Soup as it couldn’t parse whole tags.
List of US Counties wasn’t provided in the given resource links

How the Technical Challenges were Solved

Used VPN for accessing Official sites which were not generally accessible.
Used Selenium Web driver to automate the direction at URLs which fetched complete html tags of the desired webpages.
Performed a search and created structure data of list of counties of each state which was used as input to gain web URLs of counties of US.

Business Impact

Enhanced Analysis: Web scraping allows businesses to gather valuable data from various websites. This information can provide insights to desired aim and objectives enabling businesses to make informed.
Real-Time Monitoring and Upgradation: Web scraping can enable business to monitor changes or updates on website in real-time. This can be useful for tracking regulatory changes. It keeps the business and it’s data updated.
Increased Efficiency: Automation eliminated the need for manual data collection, saving time and resources. With automated web scraping, business can extract large amount data quickly, accurately, improving overall operational efficiency.

Project Snapshots
Chrome driver initiated
Chrome driver visiting the directed links and accessing the image URLs
Directed to next linkKML task
Facebook Data extraction

Data of State Governments of US


Accessing links through wiki directing to counties

Nesting within the list of counties of a particular stateFinding and Extracting link of the website of County
Project website url
The GitHub repository link:- https://github.com/AjayBidyarthy/Paul-Andr-Savoie/tree/main
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleGoogle LSA Ads (Google Local Service Ads) – ETL tools and DashboardsNext articleMonday.com to KPI Dashboard to manage, view, and generate insights from the CRM data Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2057,https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/,Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards,"

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Ads, Marketing, Campaign, Consulting
Organization Size: 200+
The Problem
The client has a Google LSA Ads Manager Account with about 100+ accounts and wishes to collect data available through the Google LSA API daily. The client wishes to set up a private Databases that is automatically created for newly added accounts and stores all of the collected data (Lead and Phone Call data). Finally, all collected data must be presented through the Google Looker Studio Dashboards, with the design layouts as suggested by the client.
Our Solution
The solution involves a number of Python-based ETL tools that are responsible for fetching the data from Google’s LSA API daily and updating the same in the Google BigQuery Databases.
Two different tools run are:

MCC Data Fetching tool.
Lead Record data fetching tool. 

The fetched data is stored in BigQuery Databases on the client-provided (Google)manager account.
Carefully curated Google Looker Studio dashboards implemented with client-suggested theme layout which are updated upon client request, represent a number of KPIs and graphs indicating major data trends.
The designed dashboards have a number of data-controlling filters that filter the data account-wise and date-wise.
Solution Architecture

Deliverables

Heroku deployed Python tools
Google Looker Studio Dashboards
BigQuery Database
Maintenance service

Tools used

Python
Google BigQuery
Heroku
Google Looker Studio
Git
Heroku CLI

Language/techniques used

Python
GoogleSQL (BigQuery supported SQL)
Looker Modeling Language (Looker ML)
Git Commands

Skills used

Data Engineering skill to fetch data as per client needs.
Data Processing to make it suitable for dashboards, databases
Dashboard designing and data presentation skills
Tool Deployment
Database manipulation
Data piplining

Databases used

Google BigQuery

Web Cloud Servers used
Heroku: Cloud Application Platform
What are the technical Challenges Faced during Project Execution

Google LSA API is slow, high data fetching timelines.
BigQuery jobs fail, causing inconsistencies.

How the Technical Challenges were Solved

Entire data fetching operation requires 1-2 hrs daily, 2 separate tools run in asynchronously and populate two different databases, the data is grouped in the dashboards
Regular weekly and monthly data refreshes update any inconsistent data.

Business Impact

Business clients are able to access important KPI’s without the need to understand the complexities involved behind the scenes.
Allows clients to track their performances, responsiveness.

Project Snapshots







Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAd Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)Next articleData Management for a Political SaaS Application Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2058,https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data-studio/,Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio),"

Client Background
Client: A leading financial firm in Dubai
Industry Type:  Financial Services
Services: Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech
Organization Size: 200+
The Problem
Build dashboards unifying all the platforms in use: Google Ads, FB ads, Appsflyer, Mixpanel, etc,in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel 
Our Solution

Track the app data analytics using various platforms
Prepare the data sources – find and build data connectors for Google Data Studio.
Developed 14 pages of Dashboard reports- creating templates to importing data sources and perform various visualisations.
Maintained and tracked dashboard reports and helped the client with intelligence from these reports.

Deliverables
1Updating the iOS datasheet2Fixing the incoming data for androids3Correcting a calculation error4Finding an alternative to provide automated data update directly to google data studio for iOS.5Updates done to all the dashboards6Created new dashboards7Created a consolidated dashboard8Added required visualizations and conected to data sources9Created new data sources10Managing the consolidated dashboard with daily data monitoring11Funnel Report for consolidated dahboard12Google analytics installed on website through tag manager13Resolving errors 14work on automation for ad acccounts15Developed a new dashboard 16Ad accounts data Automated 17work towards android data automation18altering of blended data joins as per gds updates19Personalisation of dashboards20Current dashboard updated with google events and widget changes21Added Apple search ads dashboard22Firebase funnel report dashboard developed23Card topups Funnel report dashboard developed 24Porter metrics custom dashboards for trial 25Registration firebase funnel and percentage added 26Updates for all the dashboards running until now and addition of kpi to the new firebase dashboards27User info for firebase dashboard and retention report28Registration Funnel, Cardtopups, KYC funnel Dashboard 29Fixing and Updating user info firebase dashboard and began working on the tiktok dashboard30Tiktok Dashboard Developed and populated with data from porter metrics
Tools used

Google Data Studio
Google Analytics- GA4 and universal analytics
Google Tag Manager
Big Query
Firebase
Appsflyer 
Mixpanel
Google spreadsheets

Language/techniques used

Google Standard SQL dialect- bigquery
Apps script

Skills used

Analytical aptitude
Problem-solving
Communication
Knowledge about SQL
Knowledge in digital marketing and strategies
Google cloud services
Creating data pipelines.

Databases used

Bigquery
Google spreadsheets 
Firebase

Web Cloud Servers used

Google Cloud Platform

What are the technical Challenges Faced during Project Execution

Community/in-built Connectors for Appstore connect didn’t exist
Connector for apple search ads couldn’t be found 
Data tracking from google play console, due to the timezone lag in data updation.
Facebook connector issues 

How the Technical Challenges were Solved

Worked towards building the custom connector by using apple api for Appstore connect and search ads
Utilised big query to call and store 100% accurate data from google play console and be used as a connector in GDS
Made use of inhouse built facebook connector and google sheet add-on to track and keep connector inaccuracy check.

Business Impact

Helped the client to view a consolidated report of all their ad campaigns 
Calculated and executed analytics metrics which helped to track various app events and helped the business to take decisions on UX
Consulted the client and collaborated with them in marketing and ad campaign strategies- helped them cut their marketing expenses over less efficient marketing platforms
Created funnel reports and suggested insights on app traffic to take decisions on important landing pages.

Project Snapshots









Project website URL
https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAnalytical solution for a tech firmNext articleGoogle LSA Ads (Google Local Service Ads) – ETL tools and Dashboards Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2059,https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/,Analytical solution for a tech firm,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: Consulting
Organization Size: 100+
The Problem
The client’s organization had a project that matches URLs up using TF-IDF algorithm. 
The script threw some errors and resolving these errors was the immediate ask. 
The client also required us to adjust the script for better accuracy and faster computation.
Our Solution

R&D on the code developed
Find & List bugs
Solve the Bugs
Find and get the best matching algorithm implemented. 
Check and compare the existing matching algorithm implemented for accuracy. 
if not check of other solution – ngrams or fuzzy logic
Meet the expected output

Deliverables

Fully functional code
Solution & Documentation
Support

Tools used

Google spreadsheets
Microsoft Excel
Google Colaboratory

Language/techniques
Python 
Models used

TF-IDF
BERT
Ngrams
Flair Embeddings
Rapid Fuzz

Skills used

Problem-solving
Communication
Data Modelling
Data Pipelining
Python Coding

Databases used
Google spreadsheets
What are the technical Challenges Faced during Project Execution

Bugs on the model used by the client was fairly competent using pretrained libraries
The accuracy for the bug free code on the models used by the client was shaen once the model ran on a different set of data input

How the Technical Challenges were Solved

A vanilla code to execute the same logic while fine tuning the matching algorithm was written in order to over come the shortcomings of the pretrained model bugs
The data pre-processing was done manually in order to transform every instance of an input into better readable format to be able to go into the model and get best matching accuracy possible in the given timeframe of execution of the code 

Business Impact

Helped the client to perform the matching process with maximum accuracy and lowest cost on code, by implementing manually written vanilla code from scratch to utilise the matching algorithm.

Project Snapshots 










Project website url
https://colab.research.google.com/github/AjayBidyarthy/Daniel-Emery/blob/main/vanilla.ipynb#scrollTo=vPp14xj020RL
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAI solution for a Technology, Information and Internet firmNext articleAd Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio) Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2060,https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/,"AI solution for a Technology, Information and Internet firm","

Client Background
Client: A leading Technology, Information and Internet firm in India
Industry Type:  IT
Services: Emerging Technologies, 2030, and 2050
Organization Size: 10+
The Problem
The objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services.
Our Solution

Analyze each use case
Analyze product design
Analyze user type, controls per use cases
For each use case and available product design, 


provide solution or scope of the data science capabilities 
List attributes needed in each of the product design screens


List use cases are driven by the data
For each data-driven use cases      a. Research and design the data science solution

       b. List needed data
       c. List process
                     d. List models
                     e. List solution

Help product design team with data science use cases
Help product design team with data science solutions for each use case

Deliverables
Statement of Work (SoW) with a solution documentation

Data science use cases document
Data science solution for each use cases document
Data Science methodology, algorithms needed, models, recommended and more in a good documentation

Tools used

Google docs
Microsoft word
Draw.io
Excel
Google Draw

Language/techniques 
Python- Flask 
Models used

K-Nearest Neighbours
K-Means Clustering
NLTK
DeepAvlov
Spacy
Texttiling
Eclat
LSTM

Skills used

Aptitude for functionalities
Problem-solving
Communication
Data Modelling
Data Pipelining
MLOps
NLP
Recommender systems

Databases used
Amazon S3
Web Cloud Servers used
AWS EC2
Business Impact

Collaboration with the client to identify the scope and use cases for the platform
Cost Effective approach taken to document solutions 
Regressive R&D to find and document third-party solutions for certain use cases- saving cost and time.

Project Snapshots 







Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity PrincipalsNext articleAnalytical solution for a tech firm Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2061,https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-capital-and-private-equity-principals/,AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals,"

Client Background
Client: A leading Venture Capital and Private Equity Principals in the Globe
Industry Type:  Venture Capital and Private Equity Principals
Services: Private Equity, Venture Capital, Data Analysis, Fund Performance, Alternative Assets, Competitive Intelligence, Limited Partners, Customized Benchmarks, Service Providers, Fund of Funds, M&A, and Financial Services
Organization Size: 100+
The Problem

Extract funding-related data from news articles (from 1000+ websites) such as company name, funded amount, participated investors, and other details. 
create a web app to manage the extraction of funding  data

Our Solution

There were 1000+ websites from funding-related articles so we couldn’t make a crawler for each website. So we used an inbuilt web crawler provided by elasticsearch. When we have extracted articles then we need to extract funding related information company name, fund amount and investors participated etc. Then we decided to use NLP’s question-answering method in which we need to train transformers to extract funding-related information. First we have created some keywords based approaches to create labels for each field we need to extract to train models. After that we have trained distil bert model on labelled data on AWS EC2’s GPU server. We applied this approach for all the fields we need to extract. We got 90%+ accuracy for the company name field and for other fields we got 80%+ accuracy.
To manage and view all the fields of extracted funding data we created a web app using python flask. In this we created several pages to show extracted raw data by crawler,  cleaned data after applying some cleaning functions and final output which have all the fields.  We also created admin dashboard pages to show daily crawling status, how many articles processed in one day, total final output etc.

Solution Architecture



Deliverables

Flask Web app
Elasticsearch crawler

Tools used
Flask, Spacy, NLTK, pandas, numpy, transformers, elasticsearch etc. 
Language/techniques used
Question answering in NLP, web scraping, web application Flask, Python
Models used
Distil-bert model, en-core-web-sm (pre trained model of spacy)
Skills used
NLP, Data Analysis, Flask web app, Pandas, Numpy, transformers, fastapi, elasticsearch etc.
Databases used
Elasticsearch database
Web Cloud Servers used
AWS
What are the technical Challenges Faced during Project Execution

The client wanted to extract data from 1000+ different websites and if we make any crawler it only works for one website so it was not possible to create a 1000+ web crawler.
How to extract funding information from an article. It is very difficult to extract that type of information from normal python code by defining keywords because every website has different types of articles.

How the Technical Challenges were Solved

To solve web crawler-related issues we used elasticsearch web crawler which is very fast and can extract multiple websites at a time. In this we need to create an engine and add websites that we want to scrape. After that we added some keywords to extract only funding-related articles. We set up this crawler to run every hour so we can get new articles every hour.
To extract funding-related information we collected articles from different websites and created labels for each field we wanted to extract. After that we have fine-tuned the transformer’s Distil-bert model on our labeled data.  We used these models to extract funding-related information. We also created an automated python script that uses these model on every extracted article and extracts funding-related information.

Business Impact
This funding-related data would be used in two ways. From this project, companies can find suitable investors for their startups. Companies can search for investors based on industry, verticals, etc., and find investors to help their startups.
Investors can use it to find a startup in which they want to invest based on their preferences like industry, verticals, etc.
Project Snapshots (Minimum 10 Pictures)








Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAn ETL solution for an Internet Publishing firmNext articleAI solution for a Technology, Information and Internet firm Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2062,https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/,An ETL solution for an Internet Publishing firm,"

Client Background
Client: A leading internet publishing firm in Singapore and Australia
Industry Type:  Internet Publishing
Services: peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value
Organization Size: 100+
Project Objective

Fetch all call logs using zendesk api from drivelah server
Analyse call logs and  number of calls made by a particular phone number to company and fetch recent call timing  

Project Description
We need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk api.
Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server. 
Our Solution
To fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details.
After that we need to identify the number of calls made by the user to the company in the last month.  We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user’s call records. For recent dates we used python’s datetime module which can easily identify recent date time.
Project Deliverables
2 python scripts

for fetching call details and converting into table format
for identifying number of calls made and recent call timing 

Tools used
VS Code, Google Drive, and MS Excel.
Language/techniques used
Python programming language, Data Analytics with numpy and pandas, python datetime.
Skills used
Data Analytics,, Python, Mathematics
Databases used
local data from MS Excel Sheet
What are the technical Challenges Faced during Project Execution

First one was the api data in json format with other unwanted data so it was a little difficult for us to identify the number of calls and other information from direct json data.
The date format in the api data is not appropriate for us  to handle. Because the date is  stored in string format, it was difficult to compare dates with one another and identify recent ones.

How the Technical Challenges were Solved

For the first technical challenge we first took only useful details from api’s json format and converted these details in tabular format. In python we can easily handle tables with pandas dataframe and can apply whatever operation we want to collect details.
For the second one we know that it would be difficult to handle dates in string format. So we first converted dates to a proper datetime format using python’s datetime module. It has a lot of built in functionalities which can easily compare dates with one another.  So from comparison we have identified recent dates of calls.

Project Snapshots





Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAI-Based Algorithmic Trading Bot for ForexNext articleAI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2063,https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/,AI-Based Algorithmic Trading Bot for Forex,"

Client Background
Client: A leading trading firm in the USA
Industry Type:  Finance
Services: Trading, Banking, Investment
Organization Size: 100+

The Problem

Build ML/AI Model to predict next 15 min EMA cross on historical and live data by using indicators such as EMA, MACD, RSI etc.
Create a web app to show predicted EMA cross and other indicators movement

Our Solution

In stock market indicators such as EMA, MACD, RSI etc helps us to find cross by using historical price data. If we accurately predict cross earlier then it will help us in investment. So we have used 12data api to collect historical and live EUR/USD price data. We calculated EMA(12), EMA(26), MACD and RSI indicators based on price data.  After that we created labels of ema cross in historical data. When we have training data we used different classifier models for training. We predicted accuracy  with different models and the Logistic regression model gave 91% accuracy. This logistic regression is predicting the cross only for the next step. It means we will know only 15 minutes before that the cross will happen in the next 15 min but we need to know more earlier. For that we predicted the next 45 minutes price values using the LSTM model from historical price data. Based on these price values we have calculated EMA, MACD and RSI and  after that cross using logistic regression. So now we can predict the cross 1 hour earlier based on these 2 models.
To show cross and other  indicators movement we created a python flask web app and hosted it on AWS EC2 server. The process runs every 15 minutes  and checks the cross. If there is any cross in 1 hour it sends a telegram notification.

Deliverables

Flask web app
All the python code and machine learning models

Tools used
Pandas, numpy, scikit-learn, tensorflow, flask etc.
Language/techniques used
Data Analysis, Data Visualization, Machine learning, Deep learning, flask web app etc.
Models used
Logistic Regression, LSTM model
Skills used
Data Analysis, Data Visualization, Machine learning, Deep learning, flask, python etc.
Databases used
MongoDB
Web Cloud Servers used
AWS Ec2
What are the technical Challenges Faced during Project Execution

Main challenge in this project is to find the best model. Because we have time series data so we cannot change the orders to get better accuracy.
One machine learning model is only predicting the next 15 min cross but we need the ema cross 1 hour before.

How the Technical Challenges were Solved

We were using time series data so we cannot change the order to find better accuracy in every model.  So we have tried different models with the same order and evaluated the model. Only the logistic regression model worked best for the data it gave 91% accuracy on test data.
To get the next 1 hour prediction we first tried the same logistic regression to predict the next 3 steps but we failed because of poor accuracy. So we trained the LSTM model on price data and predicted the next 3 steps using the LSTM model.  After that we used logistic regression to predict ema cross.

Business Impact
It will help traders to predict the stock market earlier and get better returns from this project.  
Project Snapshots


Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleEquity Waterfalls Model-Based SaaS Application for Real Estate SectorNext articleAn ETL solution for an Internet Publishing firm Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2064,https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sector/,Equity Waterfalls Model-Based SaaS Application for Real Estate Sector,"

Client Background
Client: A leading real estate firm in the USA
Industry Type:  real estate
Services: Property business, investment, real estate 
Organization Size: 100+
Project Objective
The objective is to create software that will calculate the equity waterfalls for different cases. And there should be 3 users admin, sponsor and investor. We need to create the equity waterfall calculation according to the csv file that is shared by the client. All users have their own UI portal. 
Project Description
The project is created using python language, working on django rest framework and for frontend we use reactjs and the code deployed on google cloud app engine service. We need to create a software that will calculate the equity waterfalls. And there should be 3 users admin, sponsor and investor. We need to create the calculation according to the csv file that is shared by the client.
All users should have their own UI portal. 
Sponsors can create deals and send deal invitations to all investors or specific investors.
Investors can see all the deals that are offered by the sponsor’s. After that Investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not.
Our Solution
We have created api’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers.
Project Deliverables

Django rest framework api’s with frontend.
Github source code.
Working UI.

Tools used

Views.
Routers.
Serializers.
Serializer relations.
Settings.

Language/techniques used

Python
Django rest framework
ReactJS
JWT
SMTP

Skills used

SMTP
JWT

Databases used

Sqlite3 Database

Web Cloud Servers used
Google cloud platform
What are the technical Challenges Faced during Project Execution
The technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. And also invite the sponsors by admin or sponsors invite their investors.
How the Technical Challenges were Solved
We have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly.
Added the functionality in which admin can invite the sponsors to the website and sponsors can invite their investor through sending the invitation link to their email.
Project Snapshots


















Project website url
https://stackshares.io/dashboard/add-new-deal
Project Video



Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAI Solutions for Foreign Exchange – An Automated Algo Trading ToolNext articleAI-Based Algorithmic Trading Bot for Forex Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2065,https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/,AI Solutions for Foreign Exchange – An Automated Algo Trading Tool,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  Financial Services
Services: Trading, consulting, financial serivices
Organization Size: 100+
The Problem
Our main objective in this project was to help with setting up with given Broker API using MT4 and extracting historical data from it, and solving different tasks which are related to extracting important values from the data. And tasks assigned by the client were related to working around the data, i.e. formatting, connecting with the IG trade broker, automating the Python script and scheduling the script accordingly.
Our Solution
During the initial phase, we were assigned to set up an MT4 with given Broker API access to extract historical prices, which was delivered to the client. In the second phase, the client requested to implement Profit/Loss, Spread Direction and Time in Trade. There were minute tasks related to the R script, which was duly completed. In the third phase, the client was assigned a task related to distinguishing the tickers according to cluster types which he provided and implemented code to distinguish the sell and buy spread for the given STD. In the fourth phase, I implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code and worked on retrieving Historical prices from another Broker API and retrieving Watchlist given attributes by the client. Automated the Python script to retrieve yesterday’s market price of the given list
Deliverables
Successfully delivered set-up in MT4 for retrieving historical prices, Created logic for automating the profit and loss, Implemented code to distinguish the tickers according to the cluster type, Implemented code for distinguish the sell and buy spread for the given STD, Implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code. Automated the Python script to retrieve yesterday’s market price.
Tools used
MT4, Jupyter Notebook, Excel, IG trade, Remote Desktop setup
Language/techniques used
MQL, Python, R
Skills used
Critical thinking, Logical Thinking
What are the technical Challenges Faced during Project Execution?
While setting up MT4 platform and its configurations 
How the Technical Challenges were Solved
The above-mentioned challenges were resolved after many hours of effort and understanding. 
Project Snapshots



Project Video




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAI agent development and Deployment in Jina AINext articleEquity Waterfalls Model-Based SaaS Application for Real Estate Sector Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2066,https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/,AI agent development and Deployment in Jina AI,"

Client Background
Client: A leading tech firm in Europe
Industry Type:  IT
Services: IT and Consulting
Organization Size: 100+
The Problem
The client’s object was to create AI agents for his website, which the end-users will utilize for many tasks. The client had some recommendations on the models are utilized.
Our Solution
Created a feasible models list that complements the client’s requirement and when ahead and executed the Executor code for every model for compatibility with JinaAI deployment. After implementing Executor codes, I created a Flow to connect every executor and deployed it successfully. 
Deliverables
Successfully delivered executable deployed models in Jina Ai
Tools used
Jina AI, VSCode, HuggingFace
Language/techniques used
Python
Models used
Whisper, Stable Diffusion, GPT3, Codex, YOLO, CoquiAI, PDF Segmentor
Skills used
Python, Model APIs 
Databases used
JinaAI Cloud
What are the technical Challenges Faced during Project Execution
There were minute challenges, such as deployment issues and Execution issues
How the Technical Challenges were Solved
I resolved the issues effectively after long hours of understanding the concept because JinaAI is a new growing technology that does not have many forums to solve errors and issues. 
Project Snapshots


Project Video




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleGolden Record – A knowledge graph database approach to unfold discovery using Neo4jNext articleAI Solutions for Foreign Exchange – An Automated Algo Trading Tool Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2067,https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discovery-using-neo4j/,Golden Record – A knowledge graph database approach to unfold discovery using Neo4j,"

Client Background
Client: A leading retail firm in the USA
Industry Type:  Retail
Services: Retail business, consumer services
Organization Size: 100+
The Problem
To use data ingested into Neo4j and use the nodes and relationships with its properties to determine which nodes are actually the same person. For eg: we have Person nodes in the data, now people might enter their names in different ways. Our main aim is to identify Person nodes that may have similar data and are actually the same person. This will be represented as a perfect match between the nodes. This single-person view is referred to as the Golden Record
Our Solution
Till date, we have loaded data into Neo4j and created relationships with score property which defines match strength. We have created some criterias by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’.We have considered four properties for our criteria – full name, address, driver’s license, and passport number. We have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation.
We have also configured Graphlytics (a viz software) in the virtual machine which connects to the neo4j database and helps vizualize the nodes and relationships. 
We have also worked on some algorithms using the GDS library in neo4j to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. Other algorithms were tried as well but since all the properties are of String format it did not work on it.
We have Resolved issues neo4j is facing when deleting a Large set of data and Provided steps to recover neo4j if it fails by going OutofMemory.
We have figured out the issues with the probable and perfect match cypher queries not working as intended and proposed a solution. 
Solution Architecture
Deliverables

Created Perfect match and probable match queries.
Created queries that return the nodes (even if it does not have associated relationship) and it’s associated relationship.
A cypher query that return the result as a json object that can be mapped into a java oject.
A cypher query that will create the relationship if two node’s properties  have same value.
A cypher query that will delete one relationship from bidirectional relationship.
A python code for a sample neo4j query
Adjust the perfect and probable match queries so it would work for  current data. 

Tools used
Neo4j
Language/techniques used
Cypher Query Language
Models used
The common neighbors algorithm
Skills used
CQL
Databases used
Neo4j
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.





Previous articleAdvanced AI for Trading AutomationNext articleAI agent development and Deployment in Jina AI Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2068,https://insights.blackcoffer.com/advanced-ai-for-trading-automation/,Advanced AI for Trading Automation,"

Client Background
Client: A leading tech firm in Europe
Industry Type:  Banking & Finance
Services: Trading, and financial services
Organization Size: 100+
The Problem
Create an automated trading application with fully automated trading capabilities from selecting pair of assets to buying/selling assets. This application uses AI to decide what action to take while trading.
Our Solution
We have integrated coin_api with the application from which data is extracted. We have created the homepage for this application. We have changed the code structure of the front end to make it more fast and efficient.
Solution Architecture
An application, where the first automated top asset pair selection happens. If the coins are co-integrated, then only one indicator must be executed else trading starts based on 2 indicators.
The AI agent will take specific action to trade based on the algorithm.
Deliverables
We have removed the old API and integrated the new api with the application.
We have altered the code structure of the front end to make the code faster and more efficient.
Tools used
Visual studio code
Language/techniques used
Python
Skills used
Django
Databases used
SQlite
Web Cloud Servers used
Digital Ocean
What are the technical Challenges Faced during Project Execution
We faced an issue while integrating coin api with the application while retrieving the data. To retrieve the data using the coin api, we need to input a symbol id. This symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. There are N coins that can be retrieved using coin api. There are more than multiple exchanges, multiple symbol types, and multiple quote currencies for ONE SINGLE COIN. This makes there is a huge no. Of combinations for one single coin. This made the execution of the api integration very slow.
How the Technical Challenges were Solved
We created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. The user selects these, and in the backend, a combination is created and is sent as input to the coin api code and the data is retrieved without slowing down the process.
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleCreate a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of TruthNext articleGolden Record – A knowledge graph database approach to unfold discovery using Neo4j Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2069,https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendations-and-a-single-source-of-truth/,"Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth","

Client Background
Client: A leading tech firm in the USA
Industry Type:  Retail
Services: Retail Business
Organization Size: 100+
The Problem
The Client was using NoSql Database which was slow and did not provide real-time response for complex queries. The data had many Connections and it was difficult to represent them in NoSQL or Relational Databases.
Our Solution
Create a Knowledge Graph and Provide Real-time Analytics and Recommendations using Machine Learning.
Solution Architecture
Neo4j was Installed on a Cloud VM based on Linodes.
Deliverables
Knowledge graphs and Data Pipelines are used to Populate the Graph.
API’s to Perform CRUD operations in real-time.
Tools used

Neo4j
Postman

Language/techniques used

Python
JSON

Models used
Node-Relationship model
Skills used

Programming
Data Engineering
Data Analytics

Databases used
Neo4j
Web Cloud Servers used
Linode
What are the technical Challenges Faced during Project Execution
Integration of Firestore with Neo4j without any native integration method or driver.
How the Technical Challenges were Solved
The challenge was solved by using api to retrieve data from Firestore.
Project Snapshots 




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAdvanced AI for Thermal Person DetectionNext articleAdvanced AI for Trading Automation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2070,https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/,Advanced AI for Thermal Person Detection,"

Client Background
Client: A leading tech firm in the Middle East
Industry Type:  Security
Services: Security services
Organization Size: 100+
The Problem
Detect a Person from thermal image and videos. Why this model was created was not told to us by the client.
Our Solution
Use Deeplearning Computer Vision to train the model on custom dataset and get the results.
Solution Architecture

Linux 22.04
Nvidiva RTX 3080

Deliverables
Trained model
Tools used

Labelimg
Yolov7
COCO2JSON

Language/techniques used
Python
Models used
Yolov7
Skills used

Deeplearning
Computer vision
Programming

Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAdvanced AI for Road Cam Threat DetectionNext articleCreate a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2071,https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/,Advanced AI for Road Cam Threat Detection,"

Client Background
Client: A leading tech firm in the Middle East
Industry Type:  Security
Services: Security services
Organization Size: 100+
The Problem
Detect the threat level of accidents between a Pedestrian and a Car.
Our Solution
Use Deeplearning Computer vision and logic to detect the threat level as defined by the Client.
Solution Architecture
Linux 22.04
Deliverables

Program which detects the threat level.
Pretrained model.

Tools used

Yolov7
DEEPSORT
Opencv

Language/techniques used
Python
Models used
Yolov7
Skills used

Programming
Computer Vision
Deep learning

What are the technical Challenges Faced during Project Execution

Integration of Object tracking algorithm with Object detection algorithm.
Writing of logic to detect the threat level.

How the Technical Challenges were Solved
The technical challenge was sorted by testing, experimenting and later on finding and modifying an already existing repository to use as a baseline for our code for integration.
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAdvanced AI for Pedestrian Crossing SafetyNext articleAdvanced AI for Thermal Person Detection Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2072,https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/,Advanced AI for Pedestrian Crossing Safety,"

Client Background
Client: A leading tech firm in the Middle East
Industry Type:  Security
Services: Security services
Organization Size: 100+
The Problem
Traffic Signals are inefficient because even if there are no cars or no pedestrians on the road it still works on a timer and stops the traffic or pedestrian unnecessarily. 
Our Solution
We provide a Computer vision-logic to Manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal.
Solution Architecture

Yolov7 pose estimation
Opencv

Deliverables

The program Detects Pedestrians and Gives alerts to traffic Signals to turn Red or stay Green.
Yolov7 pose model weights

Tools used

Yolov7
Opencv

Language/techniques used

Python
Computer Vision

Models used

Yolov7 Pose Estimation

Skills used

Programming
Computer Vision
Deep Learning

What are the technical Challenges Faced during Project Execution
There was no existing solution and we had to create the logic from scratch.
How the Technical Challenges were Solved
Researching Computer Vision. Learning new Techniques and Experimentation.
Project Snapshots

Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleAdvanced AI for Handgun DetectionNext articleAdvanced AI for Road Cam Threat Detection Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2073,https://insights.blackcoffer.com/handgun-detection-using-yolo/,Advanced AI for Handgun Detection,"

Client Background
Client: A leading tech firm in the Middle East
Industry Type:  Security
Services: Security services
Organization Size: 100+
The Problem
Detecting Handguns in images and videos.
Our Solution
We use Yolov7 instance segmentation model to detect and provide coordinates for handguns.
Solution Architecture

Linux 22.04
Yolo

Deliverables
Trained model of yolov7 instance segmentation
Tools used

Openimages
Roboflow
Yolov7

Language/techniques used
Python
Models used
Yolov7_mask
Skills used

Deeplearning
Programming

What are the technical Challenges Faced during Project Execution
Retrieving handgun images in bulk from opensource.
How the Technical Challenges were Solved
Found Openimages dataset with good amount of required images
Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.
 




Previous articleUsing Graph Technology to Create Single Customer View.Next articleAdvanced AI for Pedestrian Crossing Safety Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2074,https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/,Using Graph Technology to Create Single Customer View.,"

Client Background
Client: A leading retail firm in Newzealand
Industry Type:  Retail
Services: Retail business
Organization Size: 100+
The Problem
Companies face issue of having a Single customer under various rows with slightly different information in the same database. This causes unwanted duplication and inaccurate statistics. It also results in inaccurate ad targeting and financial loss.
Our Solution
We leverage graph technology to create a single customer view by using Complex cypher queries  and Graph Algorithms. 
Solution Architecture
We have an Azure VM on which we have installed the Neo4j Database. Deployment architecture is a single Instance because of using the Community version of the software.
Deliverables

Populated Neo4j Database. 
Required Cypher Queries.

Tools used

Neo4j
Graphlytics

Language/techniques used

Java
Cypher Query

Models used
Node-Relationship model
Skills used
Data Analytics
Data Engineering
Data Science
Databases used
Neo4j
Web Cloud Servers used
AZURE
What are the technical Challenges Faced during Project Execution
Only 1 Difficulty was faced in this Project and that was to migrate data from Elasticsearch to Neo4j.
How the Technical Challenges were Solved
Research and Experimentation.
Project Snapshots









Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleCar Detection in Satellite ImagesNext articleAdvanced AI for Handgun Detection Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2075,https://insights.blackcoffer.com/car-detection-in-satellite-images/,Car Detection in Satellite Images,"

Client Background
Client: A leading retail firm in the USA
Industry Type:  Retail
Services: Retail business
Organization Size: 100+
Project Objective
The objective of this project was to detect cars in satellite images and highlight them using a bounding box.
Project Description
The client, Steffen Schneider, approached us with a requirement to develop a Python project that dealt in the field of computer vision. The main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. To achieve this, we decided to use the Darknet model and train it on Yolov4 dataset of cars in satellite images.
Our Solution
 We used Google Colab for coding and training the Darknet model. Kaggle was used to download the Yolov4 dataset of cars in satellite images. We preprocessed the dataset and trained the model on it. Once the model was trained, we tested it on sample satellite images and it worked perfectly fine. Finally, we created a script that detected the cars in an image and highlighted them using a bounding box.
Project Deliverables
The final deliverable was a ipython Notebook presented on Google Colab.
Tools used
Google Colab, Kaggle, Slack(For Communication)
Language/techniques used
Python
Models used
Darknet(CV Model)
Skills used
Python programming, AI/ML.
What are the technical Challenges Faced during Project Execution
The main challenge we faced was related to the pre-processing of the Yolov4 dataset of cars in satellite images. The dataset was large and had to be cleaned and formatted before it could be used for training the model.
How the Technical Challenges were Solved
We used Python programming skills and developed a script that automated the pre-processing of the dataset. This saved us a lot of time and allowed us to focus on training the model.
Business Impact
The project was a success and the client was very happy with the final product. The car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time.
Project website url
https://colab.research.google.com/drive/1AoeHdZdpi0lWLf3X2G800J0VT_7wJtnE
Project Video




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.com
Skype: asbidyarthy
WhatsApp: +91 9717367468
Telegram: @asbidyarthy 
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleBuilding a Physics-Informed Neural Network for Circuit EvaluationNext articleUsing Graph Technology to Create Single Customer View. Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2076,https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/,Building a Physics-Informed Neural Network for Circuit Evaluation,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  Retail
Services: Consulting
Organization Size: 100+
Project Objective
The objective of this project was to build a Physics Informed Neural Network (PINN) using TensorFlow, which could evaluate circuits based on the parameters provided through a MATLAB simulation.
Project Description
Mohamed provided us with a dataset generated from a MATLAB simulation of a circuit, consisting of various input parameters and the corresponding circuit performance outputs. We were tasked with developing a machine learning model that could accurately predict circuit performance based on the input parameters, while also incorporating the underlying physics principles that govern circuit behavior.
Our Solution
 Our team utilized Jupyter Notebook, Google Colab, Octave, and MATLAB to build the PINN. We used TensorFlow models to build the neural network and Microsoft Excel to clean and preprocess the data. Our team employed Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. We did not use any databases for this project, nor did we use any web/cloud servers.
Project Deliverables
The final deliverable was a functional PINN capable of evaluating circuits based on the provided parameters.
Tools used
Our team used Jupyter Notebook, Google Colab, Octave, MATLAB, and Microsoft Excel.
Language/techniques used
The primary languages and techniques we used were Python programming, TensorFlow, and MATLAB.
Models used
We used TensorFlow models to build the neural network for the PINN.
Skills used
Our team utilized Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN.
Databases used
We did not use any databases for this project.
Web Cloud Servers used
We did not use any web/cloud servers for this project.
What are the technical Challenges Faced during Project Execution
The project was very challenging since our team did not have a background in electrical engineering. It was difficult to understand the physics behind the circuit evaluation, and we faced issues when using MATLAB to provide data for the project.
How the Technical Challenges were Solved
We worked with the client to gain a better understanding of the physics behind the circuit evaluation. We also worked with MATLAB experts to help us better understand how to provide data for the project.
Business Impact
The PINN we built for Mohamed Zamil allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process.
Project website url
https://colab.research.google.com/drive/1HX37MP4Jcb39SWJgkE_5z5n1gQwqWmV9
Project Video




Contact Details
Here are my contact details:
Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy
For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.




Previous articleConnecting MongoDB Database to Power BI Dashboard: Dashboard AutomationNext articleCar Detection in Satellite Images Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2077,https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automation/,Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation,"

Client Background
Client: A leading tech firm in Newzealand
Industry Type:  Retail
Services: Retail business
Organization Size: 100+
Project Objective
Brodie Johnco had a MongoDB Database that he wanted to connect to a Power BI Dashboard. However, ODBC connectors were not working for his level of subscription, so he needed a cheaper workaround.
Project Description
Brodie Johnco had a MongoDB Database containing a large amount of data that he wanted to visualize in a Power BI Dashboard. He initially tried to use ODBC connectors to connect his database to Power BI, but ran into issues due to his level of subscription. We were brought in to help find a cheaper workaround.
Our solution involved using Python to extract the relevant data from Brodie’s MongoDB Database. We used the Pandas library to create Dataframes, which we then uploaded to Azure Blob Storage as tables. We set up an Azure pipeline that ran a Python script every 30 minutes to update the tables with new data from the database.
Our Solution
 We used Brodie’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added them as tables to Azure Blob Storage and set up a Python script to an Azure pipeline that refreshed every 30 minutes. This allowed us to keep the data in sync and provide Brodie with up-to-date information for his Power BI Dashboard.
Project Deliverables
The final deliverable was a readable CSV file that contained the converted data from the original JSON format.
Tools used
Jupyter Notebook, Google Colab, Power BI, MongoDB Compass, Microsoft Excel, Azure Blob Storage
Language/techniques used
Python, Pandas, Azure Cloud Storage
Skills used
Python programming, Azure Cloud Storage, data extraction and manipulation
Databases used
MongoDB Database
Web Cloud Servers used
Azure Blob Storage
What are the technical Challenges Faced during Project Execution
The main challenge we faced was finding a way to connect Brodie’s MongoDB Database to his Power BI Dashboard without using ODBC connectors. We overcame this challenge by using Python and Azure Blob Storage to extract and store the relevant data.
How the Technical Challenges were Solved
We solved the issue by using the client’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added these dataframes as tables to Azure Blob Storage and set the Python script to an Azure pipeline that refreshed every 30 minutes. This allowed the client to access the data in Power BI without the need for ODBC connectors.
Business Impact
Our solution allowed Brodie to visualize his data in a Power BI Dashboard without having to pay for expensive ODBC connectors. The Azure Blob Storage solution we implemented was much more cost-effective and provided him with up-to-date information every 30 minutes.
Project website url
https://github.com/AjayBidyarthy/Brodie-Johnco




Previous articleData TransformationNext articleBuilding a Physics-Informed Neural Network for Circuit Evaluation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2078,https://insights.blackcoffer.com/data-transformation/,Data Transformation,"

Client Background
Client: A leading tech firm in the USA
Industry Type:  Retail
Services: Retail business
Organization Size: 100+
Project Objective
The objective of this project was to convert dirty JSON data present in a CSV file to a readable CSV file. The CSV file contained data in JSON format, which was split into columns in an Excel file, making it hard to read. The client wanted the data to be extracted and converted into a readable format to perform further analysis on it.
Project Description
Our client had provided us with a CSV file that contained data in JSON format, which was split into columns in an Excel file. The data was hard to read and understand, making it difficult to perform any analysis on it. Our objective was to extract the data, convert it to a readable format, and validate the JSON file to ensure that it was in a correct format. Finally, we had to convert the JSON data into a CSV file that could be easily read and analyzed.
Our Solution
 To extract the data, we used Python programming language and Pandas library. We extracted every piece of text present in the Excel sheet using Pandas and converted it into a readable text format. We then validated the JSON file with a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas again to convert the JSON data into a CSV file that could be easily read and analyzed.
To perform the conversion, we used Jupyter Notebook, Json Validator, and Microsoft Excel.
Project Deliverables
The final deliverable was a readable CSV file that contained the converted data from the original JSON format.
Tools used
Jupyter Notebook, Json Validator, and Microsoft Excel.
Language/techniques used
Python programming language and Pandas library.
Skills used
Python programming and Pandas data manipulation.
What are the technical Challenges Faced during Project Execution
The main technical challenge we faced during the project was dealing with dirty JSON data present in a CSV file that was split into columns in an Excel file. This made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format.
How the Technical Challenges were Solved
We solved the technical challenges by using Python programming language and Pandas library to extract and manipulate the data. We validated the JSON data using a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas to convert the JSON data into a readable CSV file that could be easily analyzed.
Business Impact
The business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand.
Project website url
https://colab.research.google.com/drive/1yWDj8_HXu6hOYatrzWQ3ezqBxsUON3JY

Here are my contact details:
Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy
For project discussions and daily updates, would you like to use Slack, or Skype or Whatsapp? Please recommend, what would work best for you.




Previous articleE-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…Next articleConnecting MongoDB Database to Power BI Dashboard: Dashboard Automation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2079,https://insights.blackcoffer.com/e-commerce-store-analysis-purchase-behavior-ad-spend-conversion-traffic-etc/,"E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…","

Client Background
Client: A leading retail firm in the USA
Industry Type:  Retail
Services: Retail business
Organization Size: 100+
Project Objective
To create a well-designed and informative dashboard for Symbiome e-commerce website using data sourced from Bigquery Database, Google Ads, Google Analytics, and Facebook Ads.
Project Description
Our client, Arik Oganesian, approached us with a requirement to create a dashboard for his friend’s e-commerce website, Symbiome. The dashboard needed to be visually appealing and provide comprehensive insights into the website’s performance. We sourced data from various sources such as Bigquery Database, Google Ads, Google Analytics, and Facebook Ads. To create the dashboard, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. The client specifically asked for cohort retention and cohort revenue charts to be included in the dashboard. With our expertise in data analytics, we were able to fulfill the client’s requirements and provide a dashboard that helped the client make data-driven decisions.
Our Solution
We used Google Data Studio to create the dashboard and Google Sheets to link the data sources. To extract data from Bigquery Database, we used SQL language. We created a set of charts including cohort retention and cohort revenue charts to fulfill the client’s requirements.
Project Deliverables
Symbiome E-commerce Dashboard
Tools used
Google Data Studio and Google Sheets
Language/techniques used
SQL for Bigquery
Skills used
Data analytics
Databases used
Bigquery Database
What are the technical Challenges Faced during Project Execution
One of the major challenges we faced was extracting data from Bigquery Database using SQL language. However, we were able to overcome this challenge by using our expertise in data analytics.
How the Technical Challenges were Solved
To solve this issue, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. By using these tools, we were able to integrate the data from different sources and create a single comprehensive dashboard that met the client’s requirements.
Business Impact
The dashboard we created provided a clear view of the website’s performance and helped the client to make data-driven decisions. This resulted in an increase in website traffic and revenue.
Project Snapshots 




Project website url
https://lookerstudio.google.com/u/1/reporting/c25c55ae-8052-4166-b363-347a2f8059da/page/SI6uC
Project Video







Previous articleKPI Dashboard for AccountantsNext articleData Transformation Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2080,https://insights.blackcoffer.com/kpi-dashboard-for-accountants/,KPI Dashboard for Accountants,"

Client Background
Client: A leading accounting firm in the USA
Industry Type:  Finance and Accouting
Services: Accounting and financial services
Organization Size: 100+
Project Objective
The objective of the project was to create a simple and easy-to-use dashboard for the accounting firm Tech 4 Accountants to track their highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
Project Description
Our client, Andrew Lassise, wanted a KPI dashboard for Tech 4 Accountants that would help them track their business performance easily. The dashboard needed to have various charts and tables that would display important KPIs in a visually appealing manner.
Our Solution
To achieve our client’s objectives, we used Google Data Studio and Google Sheets to create a visually appealing and easy-to-use KPI dashboard. We created various charts and tables that displayed the KPIs that our client wanted to track. We used Google Sheets to store the data and created visualizations using Data Studio.
Project Deliverables
We delivered a KPI dashboard for Tech 4 Accountants that included charts and tables for tracking the highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
Tools used
Google Data Studio and Google Sheets
Skills used
Data Analytics
What are the technical Challenges Faced during Project Execution
There were no major technical challenges faced during the project execution as the data was stored in Google Sheets, and Data Studio allowed us to easily create visualizations using the data.
How the Technical Challenges were Solved
No major technical challenges were encountered, and the project was completed smoothly.
Business Impact
The KPI dashboard that we created for Tech 4 Accountants allowed them to track their business performance easily and make informed decisions. The dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly.
Project Snapshots






Project website url

https://lookerstudio.google.com/u/1/reporting/fbf7879a-be79-4cb9-b7d4-783bf7447902/page/Hmg2C

Project Video







Previous articleReturn on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and DashboardNext articleE-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc… Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2081,https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-analytics-using-etl-and-dashboard/,Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard,"

Client Background
Client: A leading ad firm in India
Industry Type:  Ads
Services: Ads, Marketing, and Promotions
Organization Size: 100+
The Problem
The main problem that was addressed in this project was the manual calculation of Return on Advertising Spend (ROAS) due to the lack of a centralized platform for running ads. The client’s ads were spread across multiple revenue generating platforms, including Google Adsense, Adx, and Ezoic, while the spending was managed through the Google Ads Platform. At that time, the client lacked a centralized dashboard or website that could effectively calculate ROAS by integrating revenue and cost streams. This fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. Therefore, a comprehensive solution was developed and implemented, providing a centralized platform for calculating ROAS, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments. 
Our Solution
We developed a comprehensive solution to address the challenges faced by the client in calculating Return on Advertising Spend (ROAS) and centralizing their advertising data. The solution involved collecting data from four different APIs: Google Ads API for spending data, Google Adsense API, Ad Manager API, and Ezoic data for revenue data. To ensure compatibility, we utilized an Extract, Transform, Load (ETL) tool to convert the data received from each API, which was in different formats, into a standardized format storing them Pandas Dataframe for both revenue and spending data.
The transformed data was then stored in a Postgres database for easy access and management. To automate the data extraction process, we implemented an ETL script that runs twice daily via cronjob on a Digital Ocean VM, ensuring the latest data is always available.
Moreover, we designed a backend API using the Flask framework. This API fetched the required data from the Postgres DB, allowing users to retrieve relevant information efficiently.
Finally, we implemented a ROAS Dashboard frontend to display the calculated ROAS using the fetched values. The dashboard provided a visually appealing and intuitive interface for users to track and monitor their advertising performance. With our solution in place, the client could now easily monitor ROAS over time, access consolidated data, and make informed decisions regarding their advertising investments.
Solution Architecture
The solution architecture involved a multi-step process to address the challenges faced by the client in calculating ROAS and centralizing their advertising data. Data was collected from various APIs, including Google Ads API, Google Adsense API, Ad Manager API, and Ezoic data, and transformed into a standardized format using an ETL tool. 
The transformed data was stored in a Postgres database, and a backend API was developed using the Flask framework to fetch the required data. The calculated ROAS was then displayed on a Next Js Dashboard, providing users with an intuitive interface to track and analyze their advertising performance.
Deliverables

ETL Tool
Deployment on Digital Ocean
Backend API
Next js backend/ frontend
ROAS Dashboard

Tools used

Google Ads API
Google AdSense API
Adx API
Ezoic API
Python 3.9
Jupyter Notebook
Flask
Digital Ocean Droplet
Next Js frontend/backend Stack
Vuexy Template for ROAS Dashboard

Language/techniques used
Python 3.9
Flask API
DigitalOcean Droplet
Functional Programming in Python
ETL Tool
Skills used
Python
Git
Deployment
Data Engineering
Web Development using Next js
Databases used
We used PostgreSQL database for the project.
Web Cloud Servers used
Digital Ocean Droplet
What are the technical Challenges Faced during Project Execution
Some of the technical challenges encountered were:

Ensuring data integrity during the transformation process.
Deployment of Docker image on VM
Setting up an automated ETL pipeline.
Adding SSL certificate to backend API.

How the Technical Challenges were Solved
1. Ensuring data integrity: Implemented checks, cleansing, and validation to maintain the accuracy and reliability of the data.
2. Docker image deployment on VM: Configured VM to support Docker Image for ETL and deployed the image for seamless execution.
3. Setting up automated ETL pipeline: Automated data extraction, transformation, and loading processes for efficient data management via cronjob.
4. Adding SSL certificate to backend API: Secured backend API with SSL certificate, enabling encrypted communication for enhanced data protection.
Business Impact
The implemented solution had a significant positive impact on the client’s business. By providing a centralized platform for calculating ROAS and integrating data from multiple revenue-generating platforms, the client gained valuable insights into the effectiveness of their advertising campaigns. The availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. The user-friendly interface of the RAOS Dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. Overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcomes.
Project Snapshots 
Here are the project snapshots:

Login Screen



Landing page with first selected campaign in the list:



Using Date Picker



Search Functionality




Revenue Breakdown by Platform



Show/Hide Left Sidebar



Switching Site’s theme to Light Mode



Settings/Log Out Menu



Change Email/Password


Project Website URL:
https://roasing.com/
Project Video







Previous articleGrafana Dashboard – Oscar AwardsNext articleKPI Dashboard for Accountants Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2082,https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/,Ranking customer behaviours for business strategy,"

Client Background
Client: A Leading Retail Firm in the USA
Industry Type: Retail
Services: Retail Business
Organization Size: 100+
The Problem
Create an API service that will parse text, include comments, analyse the remarks, assign a score based on sentiment or other criteria, etc. Feed it comments, and it should analyse the syntax and sentiment of the comments as well as extract key terms to add to the extended meta data of that model. In order for us to know a user’s behaviour, personal information, and more meta data about their interests
Our Solution
Created a flask API, that will take comments as input and will textual analysis as follows:
Spell and Grammar Check: We have used language tool python for this , LanguageTool is an open-source grammar tool, also known as the spellchecker for OpenOffice. This library allows you to detect grammar errors and spelling mistakes through a Python script or through a command-line interface. Sentimental Analysis: For Sentimental Analysis we used FLAIR, Flair is a pre-trained embedding-based model. This means that each word is represented inside a vector space. Words with vector representations most similar to another word are often used in the same context. This allows us, to, therefore, determine the sentiment of any given vector, and therefore, any given sentence. Keywords Extraction: For keywords extraction we used SPACY which is newer than NLTK or Scikit-Learn, is aimed at making deep learning for text data analysis as simple as possible. The following are the procedures involved in extracting keywords from a text using spacy.
Split the input text content by tokensExtract the hot words from the token list.Set the hot words as the words with pos tag “PROPN“, “ADJ“, or “NOUN“. (POS tag list is customizable)Find the most common T number of hot words from the list
Solution Architecture

Deliverables
CommentScoringAPI that will take comments/reviews as input, and do the textual analysis on the given comment and will return the Comment Score based on counts of spell and grammar errors, sentiments, hot keywords.
Tools used
Numpy, pandas, flask, NLTK, Spacy (Keyword Extraction), language tool python (spell and grammar check), flair (Sentimental Analysis)
Language/techniques used
Python 
Business Impact
Client have a user schema that contain all the information of users that have visited there platform, and he/she want to build a Script that will take all the reviews of a certain User as input and than will do textual analysis on all the comments of the user , by textual analysis we mean Spell and Grammar Check, Sentimental Analysis, and Keywords extraction. Based on these factors our Script scored each user and helped Client to understand his/her users well. 




Previous articleAlgorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.Next articleRise of Chatbots and its impact on customer support by the year 2040 Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2083,https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/,"Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.","

Client Background
Client: A Leading Trading Firm in the USA
Industry Type: Finance
Services: Trading, Consulting, Software
Organization Size: 100+
The Problem
A Trading site will have all the required features, allowing users to trade in multiple commodities markets, like Forex, Agriculture, Metals, Energy etc. 
Our Solution
Designed the website with technical indicators, and the ability to trade in live market, plus allows the user to create his/her own strategy to backtest. Functionalities like all types of technical indicators:
Trend followingmean reversionrelative strengthvolumemomentum.
Strategies are specific scripts, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. Backtesting is the process of recreating the work of your strategies on historical data, essentially all of your past strategic work. Forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data.
Solution Architecture

Deliverables
A Fully functional trading platform that lets you customize technical indicators, create charts, and analyse financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators:
Trend followingmean reversionrelative strengthvolumemomentum.
Tools used
Numpy
pandas
Language/techniques used
Python 
Business Impact
Clients want a social media network, analysis platform, and mobile app for traders and investors. So we designed a website with all the client’s requirements, where traders, investors, educators, and market enthusiasts can connect to share ideas and talk about the market. By actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to backtest. A Fully functional trading platform that lets you customize technical indicators, create charts and analyze financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators
Project Snapshots






Previous articleTrading Bot for FOREXNext articleRanking customer behaviours for business strategy Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2084,https://insights.blackcoffer.com/trading-bot-for-forex/,Trading Bot for FOREX,"

Client Background
Client: A Leading Trading Firm in the USA
Industry Type: Finance
Services: Trading, Consulting
Organization Size: 100+
The Problem
Automate trading on the MT4 terminal for forex when certain conditions are met, and end trade at the best exit point.Save mt4 forex data for a instrument live for every tick.
Our Solution
Use PyTrader to log into trading system (mt4) for 2 brokers.Use live prices to identify when prices diverge.Buy one currency on broker 1, sell currency on broker 2.Hold until prices come back together.Coded a MQL4 script that will save tick data (bid, ask, open, high, low, close) for any instrument when active
Solution Architecture

Deliverables
Python Script to Automate the two Meta Trader 4 terminals, and trade when some conditions are true and break the trade at a exit point.A MQL4 Sript that will Save the Live tick data (Bid, Ask, Spread, Open, High, Low, Close) in a CSV file. 
Tools used
PyTrader
numpy
pandas
Language/techniques used
Python (Automation)
Mql4 (To save tick data)
Business Impact
Client requirements were  to automate his forex trading strategy  on Meta Trader4 terminal, so that he doesn’t have to bother trading anymore, the Python script we designed to not only do it, plus it offers a safe exit point for Ongoing Trades, that saved the client’s money and time.




Previous articlePython model for the analysis of sector-specific stock ETFs for investment purposesNext articleAlgorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc. Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2085,https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-investment-purposes%ef%bf%bc/,Python model for the analysis of sector-specific stock ETFs for investment purposes,"

Client Background
Client: A Leading Investment Firm in the USA
Industry Type: Finance
Services: Investment, Consulting
Organization Size: 100+
The Problem
Have an existing Python model that has been built for the analysis of sector-specific stock ETFs for investment purposes. Need to update the existing selection criteria to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameters of the selection criteria to test different variables.
Our Solution
The 2 in 4 Fundamental model screens a fundamental ranking of stock market sectors, picks the top ranked holding and continues to hold that sector as long as it remains in the top four rankings.  The model holds two positions at a time.  The sector ranking data is in the wcm5.xlxs file.  We input data from the PRICES.CSV file to pull up monthly returns.  When I go to run the program, I use the 2_in_4_New.py and that give me the current rankings for both the fundamental and technical rankings.
Sometimes a sector is ranked as being fundamentally attractive because it has become cheaper because of problems going on within an industry.  What I would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period.  Here is what the new model would do.  
Screen for a the specific number of sectors, probably between three and five, based upon the fundamental ranking over an average time period (currently 3 weeks)Choose either three, four, or five holdingsExclude the holding that has the weakest performance over a specify lookback period, let’s start with 52 weeks, but I would like to be able to adjust this variablecompare the performance of various combinations, seeing the return on an annual basis if possible, as well as showing the maximum drawdown
Solution Architecture

Deliverables
An Updated, Optimised Python script that will filter and return Technical and Financial holdings, with a Price filter that will do price analysis on a certain lookback period.
Tools used
Numpy
pandas
itertools, 
combinations 
permutations
Language/techniques used
Python 
Business Impact
The client now can get more than 2 Financial and technical holdings , up to maximum 5 holdings for both Technical and Financial, plus the holdings were more accurate because of the new added Price Filter that will Exclude the holding that has the weakest performance over a specify lookback period, default 52 weeks. It boosted the Client’s profit because of the more accurate and optimised functional filters.
Project Snapshots







Previous articleRise of e-health and its impact on humans by the year 2030Next articleTrading Bot for FOREX Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2086,https://insights.blackcoffer.com/medical-classification/,Medical Classification,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT Consulting
Services: Software, Consulting
Organization Size: 100+
Project Objective
Classify the medical research paper into 0 if the medical research paper cannot be used in future medical research and 1 if the medical research paper can be used in research based on some research-related phrases.Train an ML/DL model on classified data.
Project Description 
We have given an excel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. If the phrase is not present in a research paper then it will not be used for research. After annotation, we need to find the best ML/DL model to train research data and evaluate the model on test data.
Our Solution
We have created a python script that can compare all medical research paper text to research phrases and annot 0 if research phrases are not present in a medical research paper and 1 if research phrases present in medical research paper. 
After annotation we have trained different machine learning and deep learning models like Bert base uncased using Tensorflow, bert large, XGBoost Classifier, Random Forest Classifier and Logistic Regression. Among these models we have chosen the best accuracy  parameters model. In our case the bert-base model performed good and gave 95% test accuracy. 
Project Deliverables
ML/DL model which is trained on medical research classification data to classify other medical research papers.
Tools used
Google Colab notebooks, Tensorflow, PyTorch, Transformers, MS Excel
Language/techniques used
Python, Machine learning, Deep learning, Data Science, Natural Language Processing (NLP).
Models used
Tensorflow-Bert model, PyTorch LSTM model, Random Forest Classifier, XGBoost Classifier, Logistic Regression.  
Skills used
Machine Learning, Deep learning, NLP, Python programming. 
Databases used
used ms excel data
What are the technical Challenges Faced during Project Execution
There are various technical challenges faced during project execution:
The research paper has a huge amount of text data so the model was giving space errors in colab notebooks.Find the best threshold value which gives best test accuracy. 
How the Technical Challenges were Solved
To solve space error we have trained the model with lower batch size so this solved the error.To find the best threshold value we created the ROC AUC curve and Precision  Recall curve and checked best points where accuracy will be higher.




Previous articleDesign & Develop BERT Question Answering model explanations with visualizationNext articlePlaystore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

  

 
"
bctech2087,https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visualization/,Design & Develop BERT Question Answering model explanations with visualization,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT Consulting
Services: Software, Consulting
Organization Size: 100+
Project Description
We need to use a pre-trained bert question answering model and create a notebook that has explanations of model’s working with some visuals of bertviz, allennlp and gradient values.
Our Solution
We created a notebook first and explained the model with model view and head view visuals of bertviz library. It gives similarity between words so we can easily find related words.We used the allennlp library and created bar charts and heatmaps to show higher and lower attention words. It means when it finds question related words in the context it gives higher value to those words and if words are not related it gives lower values. We used a gradient based method to show higher and lower gradient values word according to question text and created bar charts and text color charts to show higher gradient values.
Project Deliverables
A notebook which has an explanation of the bert question answering model using some visualization.
Tools used
Google colab notebooks, Tensorflow, Bertviz, Allennlp, Transformers
Language/techniques used
Python programming language, Deep learning, NLP, Data Visualization
Models used
Pretrained bert-base-uncased model and distilbert model (both trained on squad2 dataset) 
Skills used
Data visualization, Deep learning, NLP, python 
What are the technical Challenges Faced during Project Execution
We need to use the best pre-trained model which can give good results on different questions and answers.We were working on text data so we need to use charts which can clearly show differences between higher attention and lower attention value words.  
How the Technical Challenges were Solved
For best pretrained we tried different Bert’s pretrained models like distilbert(trained on squad dataset), distilbert(trained on squad2), bert base uncased, bert large and roberta base.
Among these models we kept the best one. 
For solving charts related issues we used heatmap chart, bar chart with dark and light colors and text coloring method.
Project Snapshots














Previous articleDesign and develop solution to anomaly detection classification problemsNext articleMedical Classification Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2088,https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-problems/,Design and develop solution to anomaly detection classification problems,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT Consulting
Services: Software, Consulting
Organization Size: 100+
Project Description
We need to create a notebook with solutions to binary classification-related anomaly detection problems. We need to use machine learning and deep learning models which have greater than 90% accuracy. 
Our Solution
We created a notebook for anomaly detection. We used 10 to 15 machine learning and deep learning models but only  3 different types of auto encoder models that were giving greater than 90% accuracy. We trained all 3 models on one classification data which have anomalies and evaluated trained models on test data.
Project Deliverables
A notebook that has solutions for anomaly detection related classification problems and accuracy should be above 90%.
Tools used
Google colab notebooks, Tensorflow, Google drive
Language/techniques used
Python programming language, Machine learning, Deep learning, Data analysis and Data visualization.
Models used
Auto Encoder and Variational Auto Encoder
Skills used
Python, Data Analysis, Data visualization, Machine learning, Deep learning.
Databases used
MS Excel
What are the technical Challenges Faced during Project Execution
Most of the anomaly detection models work with regression type data and this problem was classification problem so we need to deal with classification data.Getting high accuracy is also a tough challenge for us because there are only a few models which work well on anomaly detection related classification problems.
How the Technical Challenges were Solved
So we have limited models for this problem so we used only classification models like Autoencoders, Isolation forest and one class svm. Only Autoencoder was giving high accuracy so we worked with different types of autoencoders like variational autoencoder and normal autoencoder.
Project Snapshots














Previous articleAn ETL Solution for Currency Data to Google Big QueryNext articleDesign & Develop BERT Question Answering model explanations with visualization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2089,https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/,An ETL Solution for Currency Data to Google Big Query,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT Consulting
Services: Software, Consulting
Organization Size: 100+
Project Objective
Fetch currency data from Pure-clear API and store it to Google cloud BigQuery.Create a Google cloud function to automate the above process.
Project Description 
We have given a pure-clear API and a google cloud account. We need to fetch currency data from that pure-clear API using python and need to store fetched data in Google Cloud Bigquery.
We also need to automate the above process like the process runs on a daily basis and update the currency data on Bigquery.
Our Solution
We have created a python program that can fetch pure-clear API data. The API data was in JSON format but we needed table format so we used python package pandas. We converted json data to tabular format using pandas. After that, we connected python code to google cloud using google’s authentication module and then stored data frame (table) directly to BigQuery using the “.to_gbq” method.
 We also need to run the above process daily to update new data in BigQuery. For this Google cloud provides a “Cloud function” tool. In this, we can create a function and set up their running process. So we created a function and attached the above code to that function and set up a cloud function to run daily. 
Project Deliverables
A Google cloud function that runs daily and updates data on Google BigQuery
Tools used
Cloud function, BigQuery of Google Cloud, Google Colab notebook, Python programming, Pandas
Language/techniques used
Python language and pandas module
Skills used
Python programming, Data handling, Google Cloud
Databases used
Google Cloud BigQuery
Web Cloud Servers used
Google Cloud Server
What are the technical Challenges Faced during Project Execution
Connecting google cloud to python code is challenging because Its credentials should be in a specified format otherwise it shows an authentication error.
How the Technical Challenges were Solved
To tackle this challenge we created a dictionary format (key-value pair) and stored all the authentication variables in the dictionary as a key value pair. Then we used google’s authentication library “google.auth” and passed a dictionary to the service_account method and stored it in different variables so we can store data from pandas dataframe to Google BigQuery.
Project Snapshots 





Previous articleETL and MLOps Infrastructure for Blockchain AnalyticsNext articleDesign and develop solution to anomaly detection classification problems Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2090,https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/,ETL and MLOps Infrastructure for Blockchain Analytics,"

Client Background
Client: A Leading Blockchain Tech Firm in the USA
Industry Type: AR/VR
Services: Metaverse, NFT, Digital Currency
Organization Size: 100+
Project Objective
Code for extraction of the price of cryptocurrency Required real-time data of cryptocurrency and this is extracted from the cryptocurrency URL Forecast code for prediction of the priceBuilt FastApi to reduce interaction complexity for the user 
Project Description
ETL and MLOps Infrastructure for Blockchain Analytics this entire project completes in 4 outlines and stages. In the first segment data scraping for the price of the cryptocurrency. The second stage is, Loading the data into the Microsoft MYSQL server and Transforming data into the required shape for the automated process data Load into the Amazon RDS tool management service which knows as the Amazon relational database service, and creating DB instances (DB instance class – db.t3.small).
In the fourth stage, built the FastAPI for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding.
Our Solution
           This Project Module develops according to the Client’s Requirements which involves Data extraction of Cryptocurrency data from a given URL by the Client, it also changes the data format, and attributes nomenclature according to the requirements. After extracting the data its loads into Microsoft MYSQL Server for the transformation of data and for full automation process, used Amazon RDS and built the FastAPI.
Project Deliverables
–  Data Scraping code using Python 
–  ETL code for extracting, Transform and Loading into Microsoft MYSQL server
–  AWS RDS (db.t3.samll) instances for storing data and for deployment 
–  Built FastAPI for getting the price of cryptocurrency  
Tools used
       – VC code and Google Collab    
– Microsoft MYSQL server
– AWS RDS services 
Language/techniques used
Data Scraping using Python ETL process to extract, load, and transform the dataFastAPI using PythonAmazon Cloud services 
Skills used
 – Data scraping using python
– ETL setup
– Aws web services
– FastAPI using Python
Databases used
– Microsoft MYSQL server
– Aws RDS (Amazon Relational Database services)
Web Cloud Servers used
 -AWS RDS services
What are the technical Challenges Faced during Project Execution
Data scraping speed does not meet the expected speed (events/sec)API calls have their own limitation in requesting calls per secStoring the huge amount of data
How the Technical Challenges were Solved
Get the Premium service of API calls (20 calls/sec)Used the AWS RDS for storing the data and for faster execution 
Business Impact
This Project impact is directly responsible to the investors of the cryptocurrency.To get the prices of cryptocurrency on fingers tips and use it for buying and investing money in the right corner of the cap market of finance.It clearly impacts financially to the investors and helps them for investing purposes.The scope impact of product service is worldwide for purchasing any cryptocurrency in the world.To provide these impactful services, there is a tech team of Blackcoffer behind it. 
Project Snapshots
 


Project website URL 
127.0.0.1:62190
Project Video
https://www.youtube.com/watch?v=xDeL5YggxDw&ab_channel=Blackcoffer




Previous articleAn agent-based model of a Virtual Power Plant (VPP)Next articleAn ETL Solution for Currency Data to Google Big Query Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2091,https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/,An agent-based model of a Virtual Power Plant (VPP),"

Client Background
Client: A Leading Energy Firm in the USA
Industry Type: Energy
Services: Power, Energy, Distribution
Organization Size: 5000+
Project Objective
To create an agent based model of a virtual power plant in Netlogo. To see the function of multiple such power plants that worked simultaneously. These power plants created and supplied energy based on a demand parameter that can be controlled by the observer
Project Description
The client defined specific requirements as to how he wanted the model to be. The requirements were divided into 4 parts. Each successive part increased in complexity and required the model to be adjusted or configured to fit that part into itThe entire model when completed contained all the four parts defined by the client in the Statement of work. 
Our Solution
Created the model according to requirements. The clustering of multiple agents and their position is decided mathematically based on the total number of agents and the sum of their energies. The agents form a cluster based on the condition that the sum of their power is a figure that is above a certain threshold amount, the threshold amount is also decided by the observer.
Project Deliverables
https://github.com/AjayBidyarthy/Shingi-Samudzi-Build-Netlogo-ABM-for-simulating-Virtual-Power-Grid-economicsAbove is the github link to every state of the model that was delivered to the client. The uploads start from a basic model with only clustering of the agentsThe final upload is a model that contains the full representation of a VPP for simulation. 
Tools used
-Netlogo
– python 
Language/techniques used
Netlogo uses a specific language that resembles the logo language but has it’s unique syntax and variations in the way variables are stored and how a list is parsed 
Models used
Clustering
Skills used
Netlogo programming 
What are the technical Challenges Faced during Project Execution
The major challenge was controlling the behavior of each agent in the model. The lack of understanding of the language and the available resources about it made it challenging to figure out the actual behavior of the agents and the overall model. The decision to decide where exactly each agent will cluster on the grid was difficult primarily because each agent spawned on a random patch of the screen. This meant that each agent would have to be given a spot to land on and form a cluster with other agents. The next challenge was deciding the condition on which the agents will cluster as their relative distance to each other couldn’t be used as a parameter as it wasn’t relevant to the model’s purpose. 
How the Technical Challenges were Solved
The technical challenges were solved by extensive research and referring to several forums over the span of 2 months. 
Project Snapshots


Project Video
https://www.youtube.com/watch?v=1fzCUzZ0q0Q&ab_channel=Blackcoffer





Previous articleTransform API into SDK library and widgetNext articleETL and MLOps Infrastructure for Blockchain Analytics Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2092,https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/,Transform API into SDK library and widget,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT
Services: Consulting, Marketing, Healthtech
Organization Size: 500+
Project Objective
Convert API documentation into SDK library and widget. Expected deliverables are SDK library and widgets for
Web appsiOS appsAndroid Apps
Project Description
API documentation is available for a tool that allows customers to type in their medication and find the cheapest price near them. For partners who want to have it on their own site, currently using the API documentation but would like to ultimately be able to send them an embeddable widget that incorporates the tool on their site
Our Solution
We created a flutter widget that uses  SDK libraries that allows the customer to type their medication and find the cheapest price near them.
This widget can be embedded in their web, android and IOS applications
Project Deliverables
1)SDK Library/Widget
2)Sample flutter application
Tools used
Flutter
Language/techniques used
Dart
Skills used
1)Knowledge of dart language
2)flutter app developing
What are the technical Challenges Faced during Project Execution
1 )Problems while fetching details of drugs and pharmacies
2) Showing details of drugs and pharmacies in the widget
How the Technical Challenges were Solved
All technical challenges are solved by proper communication with the client and by logical analyzing of data
Project Snapshots





Project Video
https://www.youtube.com/watch?v=MyNK_DPtsKA&ab_channel=Blackcoffer




Previous articleIntegration of a product to a cloud-based CRM platformNext articleAn agent-based model of a Virtual Power Plant (VPP) Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

  

 
"
bctech2093,https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/,Integration of a product to a cloud-based CRM platform,"

Client Background
Client: A Leading Logistics Firm Worldwide
Industry Type: Logistics
Services: Import, Export, Supply Chain, Logistics, Trades
Organization Size: 500+
Project Description
The main challenge faced by the team was the integration of the two systems themselves.
Since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automation using APIs.
Our Solution
The challenge was divided into two milestones and sub-tasks for each.
1. First was the ingestion of existing data into the cloud-based CRM platform.
2. Second was the question of automating the process of adding newer records to the cloud platform. 
Project Deliverables
The client has been provided with python scripting handling bulk data ingestion to CRM and also the script to handle daily synchronization of data.
Tools used
– Python
– MySQL Database
– Postman
– TeamViewer
Language/techniques used
– Automation
– 3rd party APIs
– Authentication methods
– Multi-Threading of function calls
– bat Scripts for easier running of scripts for the client
Models used
Python Frameworks like requests to build own custom client for consumption of APIs.
Skills used
Python Programming, Mult-threading, APIs 
Databases used
The client provided a MySQL instance.
Web Cloud Servers used
Zoho 
What are the technical Challenges Faced during Project Execution?
– Writing own client-side API-consumption code handling API calls from Authentication and Other Operations as per task requirements.
– Debugging of API responses was messy.
How the Technical Challenges were Solved
– Multiple alternatives were discussed and implemented in python like conditional refreshing of API tokens.
– Automation of daily synchronization handled by use of time deltas.
– Logging of all operations to efficiently handle errors in the future.
Business Impact
– Automated workflow of the client
– No need for dull tasks like data entry to CRM modules everything is taken care of using logic.

URL
https://www.exportgenius.in/




Previous articleA web-based dashboard for the filtered data retrieval of land recordsNext articleTransform API into SDK library and widget Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2094,https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-records/,A web-based dashboard for the filtered data retrieval of land records,"

Client Background
Client: A Leading Real Estate Firm in the USA
Industry Type: Real Estate
Services: Land, Infrastructure, Real Estate, Investment
Organization Size: 100+
Project Description
The client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish.
Our Solution
Created the web application as per client needs.
Added user functionality to handle signup/logins and added authorization middlewares to protect routes from unwanted access.
Transformed raw data into a meaningful NoSQL-based database with a proper schema being served as an instance on a cloud service named 
‘ MongoDB Atlas ‘.
Project Deliverables
Pushed code to the required GitHub repository.
Tools used
– Vanilla javascript
– Javascript Frameworks ( Nodejs, express , cors )
– Postman
Language/techniques used
– JavsScript
– Backend Service setup ( express, cors , js )
– Fronted logic setup ( HTML , CSS , JavaScript , Jquery )
Models used
Backend: An API service created to handle land records database and queries made by users.
Frontend: A frontend client is available as a web application where users can signup and access land records. 
Skills used
JavaScript Programming, APIs, JavaScript Frameworks ( NodeJS, Express  , cors ) , Web Design, NoSQL querying in MongoDB.
Databases used
MongoDB (NoSQL)
Web Cloud Servers used
MongoDB Atlas
What are the technical Challenges Faced during Project Execution
– UI component creation
– User authorization middleware creation
– Querying data in NoSQL
How the Technical Challenges were Solved
– Created and extended UI components to handle filters like owners, date fields, and area ranges on land records.
– API and Frontend are separately built for easier team management of tasks.
– Using a cloud-based MongoDB instance provided support for teams to work without any problems with accessibility.
Business Impact
– Created a platform for clients’ business.
– Transformed his raw data into meaningful business applications.




Previous articleIntegration of video-conferencing data to the existing web appNext articleIntegration of a product to a cloud-based CRM platform Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2095,https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/,Integration of video-conferencing data to the existing web app,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT & Consulting
Services: Software, Business Solutions, Consulting
Organization Size: 200+
Project Description
Integration of 3rd party APIs to client’s platform.Client required meeting/conference data from sites like gotomeeting/zoom.
Our Solution
Using APIs fetched data from different platform and rendered data into client’s application.
Modifed web application with a UI to handle form data accepting dates as a timeframe – which then makes a request to the API being handled at server end and returns the meeting data from the required source.
Project Deliverables
Pushed code to client’s github repository.
Tools used
– Python
– Postman
Language/techniques used
– Automation
– 3rd party APIs
– Authenication methods
– Multi-Threading of function calls ( authentication of api client )
– UI component design to get dates from user-end
Models used
Python Framework- Django , requests
Skills used
Python Programming, APIs , Multi-threading , Web Developement
Databases used
Default project postgreSQL
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
– UI creation for handling form data
– Managing and Validating form data to process request at server end
How the Technical Challenges were Solved
– Created autmated functions as views in django to handle requests made to video-conferencing platform.
– Which then returns meeting data as per user’s wish.
Business Impact
– Instead of extracting meeting data and adding it to all users
any authorized user can get meeting data as his wish.
Project website url
https://www.codanalytics.net/




Previous articleDesign & develop an app in retool which shows the progress of the added videoNext articleA web-based dashboard for the filtered data retrieval of land records Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2096,https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-added-video/,Design & develop an app in retool which shows the progress of the added video,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT & Consulting
Services: Software, Business Solutions, Consulting
Organization Size: 200+

Project Description
The objective was to develop a progress bar that can help costumes to estimate the analytics of the video. 
Our Solution
The client wanted a progress bar with the following filters:Date filter: – Update the progress bar and count of the videos according to the date selectedCategory filter: – Update the progress bar and the count of the videos according to the selected categoryWe have created a SQL query for getting a count of the videos from the full video table according to the filter selected in the appIn added video table some columns were missing to solve this we created a SQL query for joining the added video table to the other tables and return the count of the video according to the filter selected
Project Deliverables
App in retool
Tools used
Retool
Language/techniques used
SQL 
Skills used
SQL
Databases used
SQL Database
What are the technical Challenges Faced during Project Execution
Client wanted date filter and a video category filter but this data was not there in added video table
How the Technical Challenges were Solved
We had to join multiple data so that we can get category column and date column for applying filter 
Project Snapshots




Project Video







Previous articleRise of Electric Vehicles and its Impact on Livelihood by 2040Next articleIntegration of video-conferencing data to the existing web app Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

  

 
"
bctech2097,https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/,"Auvik, Connectwise integration in Grafana","

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT & Consulting
Services: Software, Business Solutions, Consulting
Organization Size: 200+
Project Objective
Get statistics such as uptime,  availability, cpu throughput etc. from Auvik and Connectwise and make a dashboard from it in Grafana.
Project Description
Unlike many technologies for which plugins are readily available in Grafana, there are none for auvik and Connectwise. So our task was to device a solution through which all the data from Auvik and Connectwise can be fed to Grafana. This data then would be used to plot graphs in Grafana.
Our Solution

Setup Postgres on linux
Create appropriate databases, tables and users in it.
Use python to get data from Auvik and Connectwise and perform necessary preprocesing.
In the same python file, Connect to our postgres database.
Ingest this data into postgres database.
Setup Grafana.
Connect Grafana to postgres using the postgres plugin.
Query our postgres database in Grafana to get desired results.
Plot multiple graphs according to client’s requirement and make a dashboard from it

Project Deliverables

Setup Postgres
Setup Postrges in Grafana
Write Python code to get data from Auvik and Connectwise into Postrges
Plot graphs into Grafana according to client’s requirement
Make dashboards for all the graphs

Tools used
Grafana
Postgres
Vs Code
AWS
Postman
Language/techniques used
Python
bash
Skills used
Python
networking
Data visualisation
Databases used
Postgres
Web Cloud Servers used
Amazon Web Services (AWS)
What are the technical Challenges Faced during Project Execution
Since, the data received from Auvik was in Json fromat, our first approach was to use Grafana’s built-in Json plugin. But this wasn’t working since, the data received from Auvik was multi-dimensional when the Json plugin required One dimensional data. 
How the Technical Challenges were Solved
The above challenge was addressed by transforming the multi- dimensional data into one dimensional when it was store in a python variable. This transformed data was then inserted into Postgres.
Project Snapshots









Project website url
https://github.com/AjayBidyarthy/Henry-Pardo
Project Video

https://www.youtube.com/watch?v=7CcbdfjkBzc&ab_channel=Blackcoffer





Previous articleData integration and big data performance using ElasticsearchNext articlePortfolio: Website, Dashboard, SaaS Applications, Web Apps Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2098,https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/,Data integration and big data performance using Elasticsearch,"

Client Background
Client: A Leading Tech Firm in the USA
Industry Type: IT & Consulting
Services: Software, Business Solutions, Consulting
Organization Size: 200+
Project Objective
Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.
Project Description
The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.
Our Solution
Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.
Project Deliverables
Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipeline
Tools used
Elasticsearch
Postman
Kibana
Logstash
Python
Javascript
Amazon Web Services
Postgres
Docker
Git Bucket
Github
Language/techniques used
Javascript
Json
Domain-Specific Language for elasticsearch
bash
Skills used
Elasticsearch query knowledge
Postgres query knowledge
Networking
Javascript
Backend web stack
Databases used
Postgres
Elasticsearch
Web Cloud Servers used
Amazon Web Services (AWS)
What are the technical Challenges Faced during Project Execution
Sometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.
How the Technical Challenges were Solved
To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.
Business Impact
Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.
Project Snapshots 












Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2099,https://insights.blackcoffer.com/web-data-connector/,Web Data Connector,"

Client Background
Client: A Leading Marketing Tech Firm in Australia
Industry Type: Marketing
Services: Marketing Solutions
Organization Size: 50+
Project Objective
To make a software code that takes data from a source and ingests it into a database present on a server. The scripts should automatically execute after regular intervals of time. 
Project Description
The client had several data sources that were updated with new data regularly. The client wanted software that triggers itself automatically and takes data from those data sources and ingests it into a database that is hosted on a Linode server. Also, the date parameters in the query should be changed dynamically using the current date. Further, we had to assist in setting up the Tableau BI tool on the client’s PC and connect the Postgres database to the tableau. 
Our Solution
We setup a linux server on linode.Install Postgres on this linux server.Create a database and create a new user. Grant this new user all privileges on the database.Create a table within the database. This table has columns with datatypes as specified by the client.Write a python script that makes GET request to the client specified data source and store the response in json format.Inside the python script itself, establish  a connection to our postgres database using the pscopg2 module and user credentials.Ingest the data into postgres using INSERT query in python script.Write code to get the today’s date using the datetime module. Using this, calculate yesterday’s date. Now we can use these as parameters inside our query to the data source.Move these python files to our server.Install and setup Cron on our server. Add the task to run specified python files at regular intervals to Cron.Repeat steps 4 to 11 for every new data source.
Project Deliverables
Python ScriptWorking linode server with cron installedTableau installation and connection to postgresProject Documentation
Tools used
Linode server
VS Code 
Language/techniques used
Python 
Bash
PSQL.
Skills used
Python programming
Postgres SQL 
Linux scripting
Databases used
Postgres
Web Cloud Servers used
Linode
What are the technical Challenges Faced during Project Execution
Avoiding duplicates was a challenge. Since Client was living in Australia all the timezone (on server and in code) were changed to AEDT. 
How the Technical Challenges were Solved
Used uniqueid Column to check for duplicates. Used pytz module to change timezones.
Business Impact
This solution helps in maintaining a copy of all data sources inside our Postgres database. Also, the data is 24/7 available. Since data inside the Postgres is updated regularly, graphs in the tableau are also up to date.
Project Snapshots










Project website url
https://github.com/X360pro/Web-connector-for-tableu







Previous articleAn app for updating the email id of the user and stripe refund tool using retoolNext articleData integration and big data performance using Elasticsearch Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2100,https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool-using-retool/,An app for updating the email id of the user and stripe refund tool using retool,"

Client Background
Client: A Leading Healthcare Tech Firm in the USA
Industry Type: Healthcare
Services: Healthcare Solutions
Organization Size: 200+
Project Description
The client needed two apps in retool
Update the email id of the customerStripe refund app with two options full payment and partial payment
Our Solution
We create the following two apps in retool
Takes the old email id of the user and new email id of the user when the update email id is clicked then the old email id is updated with the new email id. For updating email id we have used stripe APIThe user has to select the email id of the user and payment id of the user from the table the user get two options for a refundFull payment: – This option refunds the whole amount to the customerPartial payment: – This option refunds the partial amount entered by the user
Project Deliverables
Apps in retool
Tools used
RetoolStripe
Language/techniques used
JavaScript
Models used
We have not used any models
Skills used
API 
Databases used
Stripe database
What are the technical Challenges Faced during Project Execution
The main challenge was creating a full payment option using stripe API. If the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amount
How the Technical Challenges were Solved
To solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in retool
Business Impact
Using this apps it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment 
Project Snapshots



Project website url







Previous articleAn AI ML-based web application that detects the correctness of text in a given videoNext articleWeb Data Connector Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2101,https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text-in-a-given-video/,An AI ML-based web application that detects the correctness of text in a given video,"

Client Background
Client: A Design & Media firm in the USA
Industry Type: Marketing
Services: Consulting, Software, Marketing Solutions
Organization Size: 100+
Project Objective
Create a python web application that detects the text and checks the spelling of written text in the videos and prints the count of wrong spelling in the end
Project Description
Developing a dockerized Django web application for detecting the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on google cloud
Our Solution
We have created a python web application with Django framework when user uploads the video the application run keras-ocr model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. For correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words.
Project Deliverables
Deployed dockerized web application on google cloud which generate video with bounding box around texts
Tools used
DockerRedis ServerDjango Celery NginxOpencv NLTK Moviepy
Language/techniques used
PythonHtmlCSSJavaScript
Models used
We have used keras-ocr model for detecting the text form the video and creating the bounding box around the words
Skills used
Natural language processing,Machine learning,Image processing,Web development,Python programming
Databases used
Django Sqlite3, Redis Server
Web Cloud Servers used
Google cloud
What are the technical Challenges Faced during Project Execution
Running model on each frame of the video Show progress bar for the progress of the work
How the Technical Challenges were Solved
For running the model on each frame of the video we have used celery it runs the model in the backend of the applicationWe have used celery backend progressrecorder and updated it every time when model has detected the text from the frame of the video
Project Snapshots




Project website url
http://34.68.134.64/







Previous articleWebsite Tracking and Insights using Google Analytics, & Google Tag ManagerNext articleAn app for updating the email id of the user and stripe refund tool using retool Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2102,https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-manager/,"Website Tracking and Insights using Google Analytics, & Google Tag Manager","

Client Background
Client: A leading marketing firm in the USA
Industry Type: Marketing
Services: Consulting, Software, Marketing Solutions
Organization Size: 400+
Project Objective
The project objectives are as follows: 
Assisting the businesses with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.Setup pixels of Social Media platforms like LinkedIn and Facebook which assists users in tracking conversions.Providing monthly insights on their website performance to analyse the businesses’ strengths and opportunities for growth.
Project Description
This project includes assisting businesses with digital analysis for their marketing.Digital analytics allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. The importance of digital analytics is that it allows for a data-driven approach to marketing, and as such it can produce better results.
The primary objective of the project is to help the businesses in knowing their target audience, understanding the trends in digital marketing, and providing insights on the analytics part of their website performance. Use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. As the picture of the customer’s needs unfolds, adjust the objectives accordingly.  
Our Solution
The main aim of this project is to assist the businesses to improve their website performance with the use of technologies like Google Analytics, Google Tag Manager and dashboards built on Whatagraph. 
Google Analytics: 
Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.
Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.
Google Tag Manager:
Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.
When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. 
Whatagraph Dashboards: 
The whatgraph dashboards previews the important metrics related to the website including conversions, events, number of users and performance about ads and campaigns by the website. This dashboard helps in drawing some of the useful insights for the website notifying the strengths,gains and areas of improvement. 
Project Deliverables
Main deliverables for the project are: 
Setup the Google Analytics and Google Tag Manager for the website. Tracking events on Google Analytics using Tags created in Google Tag Manager.Monthly Reporting of Analytics for businesses on Whatagraph dashboards or via presentations. LinkedIn and Facebook Pixel setup and validation for the website. Setup Goal Conversions for the website to track the important and valuable metrics from the website. 
Tools used
Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Whatagraph: To visually represent important metrics like impressions, clicks, goal completions and many more related to Ads management and Google Analytics. Clickup: This tool is used to manage tasks given. 
Skills used
Digital AnalysisData AnalysisDigital MarketingGoogle Analytics
What are the technical Challenges Faced during Project Execution
The main technical challenge faced was that any changes in Google Analytics are operational after 24 hrs. Thus, we can’t judge if the setup works as per required. 
How the Technical Challenges were Solved
We had to wait for 24 hours to check the setup. We could use real-time report as well to check the setup on-the spot. 
Business Impact
This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. 
Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.
Some of the common metrics that are important in digital analytics include:
Dashboard metrics:
Some examples are pages per visit, bounce rate, and average duration of each visit.
Most exited pages:
Pages with an exit rate of 75–100% show that you need to examine the problem with the content and improve upon it.
Most visited pages:
These pages will make the customers either exit or explore the website further.
Referring websites:
These are other websites that link to your website.
Conversion rate:
This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.
Frequency of visitors:
This tells you about the loyalty of the customers.
Days to the last transaction:
This refers to the time lapse between the first visit and the sale. The shorter the time taken, the better it is for your business.
Project Snapshots

Figure 1: Google Tag Manager Domains

Figure 2: Google Tags 

Figure  3: Google Analytics 

Figure 4: Google Analytics
Figure 5: Tracking Facebook Pixels for a website

Figure 6: Whatagraph dashboard

Figure 7: Whatagraph Dashboard(Conversions) 
Project website url
https://unite.ca/https://livelike.com/ http://essencelle.ca/ https://www.decorium.com/ https://www.everafterfest.com/2022-tickets/https://winagetaway.com/ 





Previous articleDashboard to track the analytics of the website using Google Analytics and Google Tag ManagerNext articleAn AI ML-based web application that detects the correctness of text in a given video Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2103,https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analytics-and-google-tag-manager/,Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager,"

Client Background
Client: A Automobile firm in India
Industry Type: Automobile
Services: Retail, Automobile
Organization Size: 1000+
Project Objective
The project objectives are as follows: 
Assisting the client with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.Dashboards on website analysis presenting the important metrics and analysis related to websites.
Project Description
This project includes assisting the client to study the user flow and behaviour flow of the users on the websites. It had one main website and three other sub websites to analyse the button clicks, impressions and understanding the user’s behaviour on the website. Many events were to be tracked and converted to a dashboard in Google Data Studio to make it simpler to understand. 
This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.
Our Solution
The main aim of this project is to display the event flow, user flow and behaviour flow through dashboards and analyse them to work on the areas of improvements. 
Google Analytics: 
Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.
Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.
Google Tag Manager:
Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.
When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. 
Google Data Studio Dashboards: 
The dashboards preview the important metrics related to the websites using graphs, tables to understand the trends, patterns in the users. 
The following steps were carried out for the project: 
 Get the important metrics for website performance like the number of users visiting the websites, the average session duration, graphs related to the user acquisition like number of new users vs the returning users. This is related to the main website.
For the sub websites, track the number of users clicking on specific buttons. Through this I understand the user flow. Compare between the number of users entering the website and those clicking on buttons. Track the metrics related to goal conversion like goal completions, goal conversion rate, goal completion rate and different goals and present it using visualisations.Provide data insights in the end providing scope of improvements and recommendations.
Project Deliverables
The main deliverable for this project were dashboards on Google Data Studio depicting important metrics related to website performance. There were three sub websites for which there were two types of views each. Each of the views had several buttons related to the product. The project was about finding the user flow and event flow on the views.
Tools used
Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Google Data Studio: To visually represent important metrics like impressions, clicks, goal completions using Google Analytics. 
Skills used
Digital AnalysisData AnalysisData VisualisationsGoogle Analytics
What are the technical Challenges Faced during Project Execution
The main technical challenge faced was that there were multiple events setup in Google Analytics for one event and thus identifying a particular one was difficult. 
How the Technical Challenges were Solved
We had to communicate with the client to clarify about the event names. Although this took some time but it was necessary since accurateness of data is very essential for the project.
Business Impact
This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. 
Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.
Some of the common metrics that are important in digital analytics include:
Dashboard metrics:
Some examples are pages per visit, bounce rate, and average duration of each visit.
Conversion rate:
This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.
Source/Medium Analysis: 
This analysis helps in understanding the traffic sources and medium on the website. This helps the businesses to work on strengthening the traffic sources to get better reach to the target audience.
Traffic Analysis: 
The overall traffic analysis for the website provides information regarding the important metrics like users,avg. session duration and goal completions according to different source/medium. This will help the  business to analyse different traffic channels performances.
Project Snapshots
Figure 1: Tracking of Buttons for Triber Virtual Studio

Figure 2: Triber Goal Conversions


Figure 3: Kiger 360 Experience Website Tracking

    Figure 4: Traffic Medium Analysis

Figure 5: Overview of Dashboard Metrics

Figure 6: Kiger Studio Experience Website
Project website url
Website URL: 
https://www.renault.co.in/
Dashboard URL: 







Previous articlePower BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web AppNext articleWebsite Tracking and Insights using Google Analytics, & Google Tag Manager Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2104,https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embedding-the-dashboard-to-web-app/,"Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App","

Client Background
Client: A leading tech firm in the USA
Industry Type: IT Services
Services: Consulting, Software, Marketing Solutions
Organization Size: 100+
Project Objective
Create a dashboard with Assets Performance With react App. So users can evaluate with Key metrics from data analytics and forecasting.
Project Description
The client requires two pages:
Screening Asset Performance Portfolio Investing according to criteria and sector-based.
Our Solution
By using Power BI We can achieve this requirement without any additional stack. It requires a subscription to enhance the report.Using Page Navigation and bookmarks to create reports like Web Application with React App.
Project Deliverables
Asset Report Page
Investor Page
Tools used
Power BIAzure AADMongo DB BI ConnectorODBC ConnectorDAX Studio
Language/techniques used
STAR SCHEMA
Skills used
DATA MODELLING.Performance Analyser.Vertipaq Analyser.
Databases used
Mongo DB
Web Cloud Servers used
AZURE 
What are the technical Challenges Faced during Project Execution
Time for loading pages is increased due to raw data.Cold start of Report taking more time than usual
How the Technical Challenges were Solved
From Snowflake to Star Schema  achieved performance of Report By using Performance Analyser debugging resolved many glitches and where it is happening.Extraction, Transformation makes data less complex and removing unwanted data from a website perspective makes data shrink and achieved 75% of Data Reduction.
Business Impact
Less coding with Power BI speeds the development process and achieves Best UX with less time.
Project Snapshots







Project website url
https://digital.bctriangle.com
Project Video







Previous articleNFT Data Automation (looksrare), and ETL toolNext articleDashboard to track the analytics of the website using Google Analytics and Google Tag Manager Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2105,https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/,"NFT Data Automation (looksrare), and ETL tool","

Client Background
Client: A leading tech firm in the USA
Industry Type: IT Services
Services: Blockchain, NFT
Organization Size: 10+
Project Objective
To scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on.
Project Description
Matthew Brown – extract all events, all time from this https://looksrare.org/explore/activity . We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available.
Our Solution
We provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created.
Project Deliverables
Webcrawler and database
Tools used
Python  Selenium  GBQ
Language/techniques used
Python  Selenium web scraper  Pandas Google big query Parallel processing.
Databases used
SQL 
Google BigQuery
Web Cloud Servers used
Google BigQuery
What are the technical Challenges Faced during Project Execution
The only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper.
How the Technical Challenges were Solved
Identifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.
Business Impact
Supplied upto 50 million rows data regarding NFTs.Provided a python solution with optimal functions and code to be used and automate them to save the data into a database on a daily basis.Caused a huge influx of data which can be used to make many insightful decisions regarding the nft market.
Project Snapshots


Project website url
https://looksrare.org/explore/activity




Previous articleOptimize the data scraper program to easily accommodate large files and solve OOM errorsNext articlePower BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2106,https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files-and-solve-oom-errors/,Optimize the data scraper program to easily accommodate large files and solve OOM errors,"

Client Background
Client: A leading tech firm in India
Industry Type: IT Services
Services: SAAS services, Marketing services, Business consultant
Organization Size: 100+
Project Description
Building a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government websites, multilateral banks, state and local government agencies, data aggregating websites, etc. 
Our Solution
We had tried multiple solutions to prevent the program from running out of memory. We used python pandas techniques to control the use of memory which worked for some files and did not work for others. Provided more solutions using vaex ,dask module and datatables.
Project Deliverables
Desired changes to the code and committing them to github.
Tools used
VscodePythonGithubSlack
Language/techniques used
Chunking dask Dataframevaex  datatable python.
Skills used
Cloud PythonTime complexity
What are the technical Challenges Faced during Project Execution
System specs requirement was the main issue during this project because the RAM available was too less and got used up quickly.
How the Technical Challenges were Solved
Team viewer to use remote desktop which had higher specs would be sufficient enough to solve the problem.
Business Impact
Provided various techniques to solve memory issues.Suggested parallel programming to decrease the execution time by 12% making getting the tender data at a much faster rate.
Project Snapshots

Project website url
https://github.com/Taiyo-ai/opentenders-euhttps://opentender.eu 





Previous articleMaking a robust way to sync data from airtables to mongoDB using python – ETL SolutionNext articleNFT Data Automation (looksrare), and ETL tool Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2107,https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-python-etl-solution/,Making a robust way to sync data from airtables to mongoDB using python – ETL Solution,"

Client Background
Client: A leading tech firm in the USA
Industry Type: IT Services
Services: SAAS services, Marketing services, Business consultant
Organization Size: 100+
Project Description
Equilo is a social impact start-up focused on gender equality and social inclusion. We need to link data in Airtable (1 million+ records spread across 20+ bases) to MongoDB (v3.x.x).Most of the data is backend data for our app, in which case the flow is only AT to MDB.Need to create a code that can calculate a scores by pulling from indicators in many different bases and putting result in new database.
Our Solution
Used Python and MongoDB module along with Airtable API to fetch all the data from airtables and push them to the database. Stayed in touch with the client through slack and asana completing daily tasks and applying a cronjob for the program to run on a scheduled time.

Project Deliverables
Python code for sync into their staging server and then to production.
Tools used
VScode MongoDBAirtable APISlackAsanaGithub
Language/techniques used
Python  MongoDbSQL
Skills used
Data extractionData handlingData storageComputational data queries
Databases used
AirtablesMongoDB
Web Cloud Servers used
Airtable
What are the technical Challenges Faced during Project Execution
Main challenge faced was regarding the new concept of Airtables and syncing up the data into mongodb in a very complex schema as proposed by the client. Dissimilar columns in mongoDB and Airtables for 100s of tables took lot of time.
Also insufficient information provided by client while coding and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem.
Not proper code management which could help next coders like me to complete the remaining stuff quickly.
How the Technical Challenges were Solved
These issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. For eg: whereabouts of the previous codes and people who run that code.
Business Impact
Helped them immensely making their backend to frontend integration seamless.Sped up their product development by 20% to calculate various different scores and visualize them on the frontend.
Project Snapshots


Project website url
https://www.equilo.io/




Previous articleGoogle Local Service Ads LSA API To Google BigQuery to Google Data StudioNext articleOptimize the data scraper program to easily accommodate large files and solve OOM errors Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2108,https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/,Incident Duration Prediction – Infrastructure and Real Estate,"

Client Background
Client: A leading research institution in the middle east
Industry Type:  Research
Services: R&D
Organization Size: 1000+
Project Objective
To complete a Research Paper draft by training various Machine Learning models which can predict the Incident Duration based on various parameters given in the dataset and summarising the results.
Project Description
Given a set of researches, need to analyse and compare various machine learning and deep learning models to predict the Incident Duration for the given dataset. The dataset contained Short durations as well as Long durations. Build models for each set of durations, compare and get the best out of all.
Our Solution
Here, we had to predict the traffic incident duration with some machine learning tools and techniques i.e. XGBoost, SVR and Deep Learning algorithm using tensor flow. First two models were run on Python Interpreter whereas Deep learning model was run on R studio, all the three with the same dataset and then we had compared these models based on their MAE (mean absolute error). Initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variables used in our research.
Project Deliverables
Python Script for each model.Documentation for Research Work.
Tools used
Python Interpreter
Language/techniques used
Language Used: Python
Libraries Used: pandas, sklearn, numpy, keras, pickle
Models used
XGBRegressorSVRSGDRegressorSequentialDecisionTreeRegressor
Skills used
Programming, Statistical Analysis
Project Snapshots













Previous articleStatistical Data Analysis of Reinforced ConcreteNext articleHow does Metaverse work in the Financial sector? Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2109,https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/,Statistical Data Analysis of Reinforced Concrete,"

Client Background
Client: A leading research institution in the middle east
Industry Type:  Research
Services: R&D
Organization Size: 1000+

Project Objective
Conducting statistical data analysis on the data provided for different types of reinforced concrete (using 3 different fibers – Steel, Date Palm and Polypropylene fibers) and also helping in preparing good research paper based on laboratory data.
Project Description
The project had two phase:
Phase 1:
In this phase, we had to do a comprehensive analysis on the data given and finally build statistical models for the variables present. The main motive was to understand the behaviour of concrete based on various parameters – Compressive strength, Flexural strength, water absorption capabilities of the concrete and many more. The analysis should include, but was not limited to:
Comparison of Mo (control mix) with all mixes at 28 days for each parameter testComparing all parameters for all specimens (all concrete mixes) with 28 days and also 6 months heat-cool and wet-dry all other expected analysis we could see you and do
Phase 2:
In this phase, we had to develop a structure for the research paper based on the results and analysis. The paper included sections – Abstract, Introduction ( literature, background and objective), Experimental program ( materials and methods), Results and discussion ( analysis and interpretation) and Conclusion ( summary, insights and remarks).
Our Solution
Providing a Comprehensive analysis for the concrete data – showcasing the key insights from it based on the parameters (compressive strength, etc). On the basis of results from the analysis, research paper was drafted which included all the deliverable.
Project Deliverable
A manuscript (drafted article) with the following:
AbstractIntroduction ( literature, background and objective)Experimental program ( materials and methods) Results and discussion ( analysis and interpretation)Conclusion ( summary, insights and remarks)References
Tools used
Tools used:
Jupyter – Notbebook (Python)NumpyPandasSklearnMatplotlibSeabornMS ExcelGoogle spreadsheets
Language/techniques used
PythonStatistical ModellingStatistical Inference
Models used
Statistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete mixes due to mixing of different fiber content and its effect on different parameters specified above.
Skills used
Coding – Python
Performing statistical analysis – extracting inferences
Building statistical models – through python or through Excel and its counterparts.
Databases used
No database was used.
Web Cloud Servers used
No Cloud server was used.
What are the technical Challenges Faced during Project Execution
The Challenges faced during project execution are:
Getting statistical models from seaborn libraries, there is no direct way to get the models from the graphs created from data.Building models in excel and validating it (didn’t know how, had to learn it before applying it).
How the Technical Challenges were Solved
I had to use different libraries for building the models, later on turned to MS excel and spreadsheet because they were building models and were also able to showcase it on the data itself. For this, I learned how to build models on the aforementioned software through YouTube and blogs.
Project Snapshots 











Project Video







Previous articleDatabase Normalization & Segmentation with Google Data Studio Dashboard InsightsNext articleIncident Duration Prediction – Infrastructure and Real Estate Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2110,https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboard-insights/,Database Normalization & Segmentation with Google Data Studio Dashboard Insights,"

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Market Research
Services: Marketing, Consultancy
Organization Size: 60+
Project Objective
To combine the different datasets.
To make dashboards for each and every dataset individually. 
Project Description
Phase – 1: In this project first of all we have to combine different datasets individually to make single file for each source.
Phase – 2: Make Good looking reports for each file individually.
Our Solution
We used pandas dataframe to combine different files to make single file for each source. We used Google Data Studio to make good looking and better reports with good UI.
Project Deliverables
We have provided a Google Data Studio report file as deliverable for the project.
Tools used
Python, Google Data Studio, Google Chrome
Language/techniques used
Python Programming and SQL queries editor.
Models used
SDLC model used in this project. We have used the SDLC model as analysis, design, implementation, testing and maintenance.
Skills used
Data cleaning, Data Pre-processing, Data Visualisation are used in this project.
Databases used
We have used the traditional file systems as database storage.
What are the technical Challenges Faced during Project Execution
Combining Data sets into single file.Making good looking UI dashboards.
How the Technical Challenges were Solved
I used pandas dataframe to combine different datasets and made a single file of every individual source. I used Google Data Studio to make dashboard for the project.
Project Snapshots 






Project Video







Previous articlePower BI dashboard to drive insights from complex data to generate business insightsNext articleStatistical Data Analysis of Reinforced Concrete Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2111,https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-business-insights/,Power BI dashboard to drive insights from complex data to generate business insights,"

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Market Research
Services: Marketing, Consultancy
Organization Size: 100+
Project Description
Phase – 1: In this project first of all we have made heatmap between two columns named Author and Data Source. Then after two combining two tables named NY_data and nodeid_views made the report of all of the data.
Phase – 2: Success of story was given by if pageviews is more than 35000, if pageviews lies between 3500-35000 the story was labelled as needs improvement and if it was below 3500 the story was labelled as failure. 
Phase – 3: The powerbi report was made to find different insights in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. The data is updated every week and the report is generated automatically.
Our Solution
We provided them Phase 1 in the powerbi sql editor by combining two tables using sql queries. For phase 2 we just used the power bi program tool and written a script in Python to calculate the success of story. For Phase 3 we used the internal features of Power BI to find insights of the data.
Project Deliverables
We have provided a PowerBI report file as deliverable for the project.
Tools used
Python, PowerBI, Google Chrome
Language/techniques used
Python Programming and SQL queries editor.
Models used
Waterfall model used in this project.
Skills used
Data cleaning, Data Pre-processing, Data Visualisation are used in this project.
Databases used
We have used the traditional file systems as database storage.
What are the technical Challenges Faced during Project Execution
Drawing heatmap in the PowerBI.Combining two tables on the basis of the pageviews.Converting the time series to data to 5 minute format.
How the Technical Challenges were Solved
We installed a new add on in the PowerBI to draw heatmap for the project and used the SQL editor to combine the tables on the basis of page views. We used python programming to convert the time series data to 5 minute time gap format.
Project Snapshots 







Project Video







Previous articleReal-time dashboard to monitor infrastructure activity and MachinesNext articleDatabase Normalization & Segmentation with Google Data Studio Dashboard Insights Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2112,https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines/,Real-time dashboard to monitor infrastructure activity and Machines,"

Client Background
Client: A leading tech firm in Europe
Industry Type:  IT
Services: Software Services
Organization Size: 30+
Project Objective
For the current project, we hope to develop a real-time dashboard (* it updates every several minutes). Currently, we have multiple Ubuntu machines that are sending messages every minute to Apache Pulsar.
Project Description
Developing a realtime updating dashboard to display the metadata of various machines on a server from pandio queue.
The dahboard must display the count of “inactive” , “active” and “down” servers with a table displaying the details of all the machines in different color scheme for each type of server/machine.
Our Solution
We used Django framework to develop the dashboard as it didn’t require the ec2 instance to be active on machine which was the problem with using streamlit.For communication between webpage and fetched data we used django channel .We used django background task module to make the fetching run forever in background.
Project Deliverables
Real time updating Dashboard with separate color scheme for different types of machines.Storing the historical data in sqlite3 db.
Tools used
DjangoWeb ChannelsD3 jsReddis server
Skills used
PythonDjango FrameworkDjango web channelsHTML/CSS + JS
Databases used
Django sqlite3 database.
Web Cloud Servers used
AWS
What are the technical Challenges Faced during Project Execution
Making the dashboard run forever using streamlitData updation in realtime when using django channels
How the Technical Challenges were Solved
Switched the entire dashboard to django frameworkWe redirected data to channels on local reddis server.
Project Snapshots









Project website url
Development hosted URL




Previous articleElectric Vehicles (EV) Load Management System to Forecast Energy DemandNext articlePower BI dashboard to drive insights from complex data to generate business insights Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2113,https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-demand/,Electric Vehicles (EV) Load Management System to Forecast Energy Demand,"

Client Background
Client: A leading energy consulting firm in the USA
Industry Type:  Energy
Services: Energy solutions, Consultancy
Organization Size: 100+
Project Objective
Create a Machine learning solution to manage electricity for electric vehicles.
Main Tasks:
Percentage probability of  user plugin his vehicle today by user’s plugin date historyReduce the probability of plugin time according to user’s plugin time history
Project Description
We need to calculate the date and time probability that the user will plugin his vehicle today based on his plugin date and plugin time history. We also need to decrease time probability based on the user’s past time range.
Our Solution
We converted the user’s plugin data into binary values like 0 if the user hasn’t plugged-in his vehicle on that day and 1 if he plugged-in. We identified the driven distance based on the amount of charge used between two plug-in times. Then we trained the Ridge Regression ML model for identifying each day driven kilometer. From these kilometres we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle.
For time probability we have used Probability Distribution Function (PDF) and Cumulative Distribution Function  (CDF). These functions will decrease probability according to the user’s time range.
Project Deliverables
2 python scripts to:
Train regression model every day.Use model weights to generate probability values.
Tools used
Google Colab, VS Code, Google Drive, and MS Excel.
Language/techniques used
Python programming language, Data Analytics with numpy and pandas, Data Visualization with matplotlib, Statistics and Mathematics, Machine learning with SKlearn.
Models used
Ridge Regression Model
Skills used
Data Analytics, Data Visualization, Machine learning, Python, Statistics
Databases used
local data from MS Excel Sheet
What are the technical Challenges Faced during Project Execution
There are a lot of challenges faced during project execution
At the start, we have only imaginary data so need to convert in a good format to apply machine learning models.Find the best machine learning model for the data.Decrease the time probability according to user’s time range 
How the Technical Challenges were Solved
We have converted the data into weekday’s binary values like marked 0 if not plugged-in vehicle on that day and 1 if plugged and calculated driven distance by amount of charge used between two plugin dates.Tried different regression based machine learning models like Random Forest Regressor, XGBoost Regressor, Ridge Regression and checked accuracies of all models and choosed best one.For decreasing time probability we used Probability Distribution Function (PDF) and Cumulative Distribution Function (CDF). These functions decrease probability according to the user’s time range.
Project Snapshots














Previous articlePower BI Data-Driven Map DashboardNext articleReal-time dashboard to monitor infrastructure activity and Machines Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2114,https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/,Power BI Data-Driven Map Dashboard,"

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Market Research
Services: Marketing, Consultancy
Organization Size: 60+
Project Objective
Change bubble colors dynamically.Make table and charts linked. If a user clicks on tables values, then the bubble chart on the map should be highlighted that relates to the table. 
Project Description
“I have a map visual. I would like to dynamically change the colours of some of the bubbles.”The report page has several filters and KPI Dashboard, whose metrics change dynamically when the user clicks a certain element. Similarly the map should also change dynamically relative to the filter.
Our Solution
Added the website data from Details table to the map visualization, it makes the bubbles get coloured dynamically according to the requirement for websites data.
Project Deliverables
The Power BI ( .pbix ) file updated with solution
Tools used
Power BI
Skills used
Power BIData VisualizationData Analysis
Databases used
The database that came in with the Power BI file received from client
What are the technical Challenges Faced during Project Execution
The map was not linkedMap Bubbles were not dynamic
How the Technical Challenges were Solved
Refactoring the data model and using appropriate keys to link the data togetherThat made Map to change according to Slicers/FiltersTo Change the colour, Bookmark buttons were used in the dashboard to bring up the dynamic colour changing with slicing (works after being published)
Project Snapshots


Project Video







Previous articleAI Conversational Bot using RASANext articleElectric Vehicles (EV) Load Management System to Forecast Energy Demand Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2115,https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/,Google Local Service Ads (LSA) Leads Dashboard,"

Client Background
Client: A leading law firm in USA
Industry Type:  Law
Services: Law practice
Organization Size: 40+
Project Objective:
For a better understanding, provide visualisations of the data on the LSA Dashboard.Learn how to enhance Rank and push the Ad to potential consumers by gaining data insights.
Project Description
Local Service Ads is a newer program by Google that allows advertisers to achieve a “Google Guaranteed” status in search engines when a visitor makes a search. Advertisers who participate in Google Local Service Ads will receive a larger ad space with their competitor’s local services ads and they will be able to feature their local businesses throughout organic search queries. 
There are various aspects that firms must concentrate on in order to win the Google services ad and so raise their ranking. These enhancements may be implemented if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future.
This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.
Our Solution
The solution for the project includes data insights through visualisations which will help businesses to better analyse the available data. This solution will help the businesses in improvising the factors to increase their potential customers and raise their respective ranks. 
It is divided into two parts: databases and data dashboard. The databases will store the important data retrieved from the LSA dashboard and use them to calculate some important metrics. The data dashboard will represent those metrics in form of graphs and data in form of tables. 
Project Deliverables
The project deliverables can be divided into two parts: 
Data in databases: The data is divided into three parts: Historical Account Data, Historical Phone Lead and Historical Message Lead. Using these three data, we calculate and store other important metrics like Cost per Acquisition, Conversion Rate, number of booked leads, number disputed leads, pending leads and approved leads. Google data studio dashboard: The dashboard will show the count of important metrics like total number of records, total interactions and different types of leads. It will represent different types of graphs portraying different kinds of information and tables containing major data like Lead data combined and Net monthly spent on Ads. 
Tools used
For extracting the data from the LSA Dashboard, we have made our own tool by python scripts. The automation tool will store data in the excel sheets and google bigquery for respective businesses on a day to day basis. PyCharm for compiling and running the code. JsonViewer for processing 
Language/techniques used
We have used the LSA API to extract data from the LSA Dashboard. Google Sheets API to store data in excel sheets. Bigquery API for storing data in google bigquery. The scripts for the automation tool were written in the Python programming language. 
Models used
Software Model: RAD(Rapid Application Development model) Model
In the RAD paradigm, less emphasis is placed on planning and more emphasis is placed on development activities. It aims to create software in a short period of time.
Advantages of RAD Model: 
Changing needs can be addressed.Progress may be quantified.Increases component reusability.Encourages responses from consumers.Integration from the start solves a lot of integration concerns.
Skills used
API Data AbstractionData VisualisationAutomation of toolsException Handling from PythonData PreprocessingData Wrangling
Databases used
Two types of databases: Google excel sheets and google bigquery. 
Web Cloud Servers used
Google BigQuery Cloud Database with up to 1 TB of free storage is being used.
What are the technical Challenges Faced during Project Execution
Some minor technical challenges were faced for clients with minimum data. For those, plotting graphs became difficult. 
How the Technical Challenges were Solved
We tried to process the data, remove the blank data spaces and plotted the graph with available data. 
Business Impact
It’s undeniable that Google’s Local Services ads (LSA) have changed the way home service businesses advertise online.
The pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized businesses a better shot at competing with national brands and massive budget operations.
To win with the Local Services the businesses need to take care of some factors where data comes to help. 
Dialling in your service area, Profile and Budget: The data from the message and phone leads help to know whether they are potential customers. If they are potential customers, their location and profile can help you in charging them or not charging the leads. Mark your JOBS as Booked: The dashboard will display the number of archived leads and booked leads. This count can help you analyse your performance and how you can work to increase your potential customers. Deal with disputes: The dashboard will also represent the disputed disputes and approved disputes which will help you to deal with the disputes. Net Monthly Ad Spend: This is an important metric which helps the firms to make better decisions for their expenditure. They can have an efficient control over their expenditure once they have proper data available. Other metrics related to finances include Cost per lead, Cost per Acquisition and Conversion rate. 
Project Snapshots 
Fig.1: Data Dashboard for individual businesses-1
Fig.2: Data Dashboard for individual businesses-2
Fig.3: Consolidated Dashboard
Fig.4: Historical Account Data
Fig.5: CPA and CPL datasheet
Fig.6: Lead Dispute Status





Previous articleHow Metaverse will change your life?Next articleAI Conversational Bot using RASA Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2116,https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/,AWS Lex Voice and Chatbot,"

Client Background
Client: A leading tech firm in USA
Industry Type:  IT
Services: eCommerce
Organization Size: 40+
Project Objective
Create a Voice and chatbot using AWS lex which can book flights, hotels, cars and book some fun activities in a city.
Project Description
We need to create a voice and chatbot using AWS lex and lambda function. The bot should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. We also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities. 
Our Solution
We have created aws lex intents and lambda functions for all bookings. Intents manage front ends like utterances (user can ask to the bot) and slots (bot replies with relevant questions). Lambda functions manage backend parts like which intent should be triggered if the user says “ book a flight” or “book a hotel” or “book a car”. For search results we have used some external APIs like Amadeus for flight, sabre for hotels and blablacar for car booking.  We have modified search results by using Data Analytics (for getting the cheapest and good star flight and hotel), Machine learning (for getting user’s preferences by analyzing user’s history) and NLP (Differentiate search results by text analysis) techniques so users can get the best search results. 
Project Deliverables
An aws lex voice and chatbot which can book flight, hotel, car and fun activities. This can be integrated with IOS applications. 
Tools used
AWS Lex, AWS Lambda, AWS Cognito, AWS EC2, Google colab, VS code, FAST API, Uvicorn.
Language/techniques used
python, machine learning, data analytics, NLP.
Models used
TfIdf-Vectorizer and cosine similarity
Skills used
Data Analytics, Machine learning, NLP, Python, AWS, REST APIs.
Databases used
MySQL
Web Cloud Servers used
AWS
What are the technical Challenges Faced during Project Execution
The first challenge we have faced is the integration of AWS lex and lambda functions. Amadeus and Sabre APIs data was not in a good format so we have to clean some data and organize it in a usable format.We need to make some APIs so we can pass flight or hotel parameters and the APIs will give flight or hotel related data. Create a book button in the bot for booking flights, hotel,s and car.
How the Technical Challenges were Solved
So the integration of AWS lex and lambda function was very tough for us. Because lex uses some intentes to show responses from the lambda function. So we have created different lex intents to pass messages to lex bot from lambda function. And put some good coding to the lambda function so different messages can be handled by different intents.For flight, hotel and car search results we were using some external apis like amadeus, sabre and blablacars apis. These APIs have a lot of data and are not in a format we need.  So first we cleaned data and then sorted data according to cheaper and best ratings results. We have used the best two results among all the results. We cannot use all the machine learning and data analytics part in aws lambda function. So what we did was we created some REST APIs which can handle all the data analytics and machine learning part and we hosted these APIs on AWS EC2 instance. We used these APIs in our lambda functions.So Creating a button in a chat bot or voice bot is always so different from providing text messages. For creating a button we used a response card structure in lambda function which can handle button and button related responses.
Project Snapshots

Project Video







Previous articleMetaBridges API Decentraland Integration – AR, VRNext articleAI/ML and Predictive Modeling Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2117,https://insights.blackcoffer.com/metabridges-api-decentraland-integration/,"MetaBridges API Decentraland Integration – AR, VR","

Client Background
Client: A leading tech firm in the USA
Industry Type:  IT
Services: Consulting, Software, Blockchain, Metaverse
Organization Size: 20+
Project Objective
To integrate with Metaverse environments with the help of EC2, S3 bucket and the Decentraland SDK.
Project Description
Move 3D model files from EC2 instance to S3 bucked using aws-sdk.
Our Solution
Configure  s3 bucket in aws account, create an user for s3 bucket api keys, and 
api secret. Put the api key, aapi secret, bucket name and bucket region in 
environment variable to use them in app. Install aws-sdk to implement s3 bucket.
Create a function to send file from nodejs server to s3 bucket.
Project Deliverables
Aws ec2 instance credentials, s3 bucket credentials. Code used in the project
Tools used
vs code editor, git bash terminal, google chrome web browser.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality 
Language/techniques used
Javascript language is used.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality  
Models used
dcl SDK (Decentraland sdk for nodejs), aws-sdk, awscli.
Skills used
Node js project setup, Dcl sdk setup, Aws ec2 instance setup with aws cli,
S3 bucket connection with aws-sdk. cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality
Databases used
No database is used
Web Cloud Servers used
AWS cloud server is used
What are the technical Challenges Faced during Project Execution
Making the application port in ec2 instance available globaly.
How the Technical Challenges were Solved
Search few blogs and videos for the solution. And make it done by doing some change in 
Security group in ec2 instance.
Business Impact
 As Decentraland is a platform based of NFT so main part of business is related to NFT and cryptocurrency.  
Project Snapshots








Project Video







Previous articleMicrosoft Azure chatbot with LUIS (Language Understanding)Next articleAWS Lex Voice and Chatbot Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2118,https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/,Microsoft Azure chatbot with LUIS (Language Understanding),"

Client Background
Client: A leading retail firm in the USA
Industry Type:  Retail
Services: e-commerce, retail business
Organization Size: 100+
Project Objective
To create an advanced chatbot using Microsoft Azure cognitive service to take orders from customer on behalf of a pizza restaurant and give order summary as end result to the user. 
Project Description
The project uses MS Azure LUIS service for language understanding to receive order details from a customer and provide an order summary. Also display various menu options to the customer in a dynamic method.
Our Solution
Our solution is to create a chatbot on MS Azure platform using their LUIS service in bot-framework composer environment. Use dynamic hero cards to display menu so that user can get a better experience.
Project Deliverables
Chatbot
Tools used
Bot Framework composerBot emulatorMS Azure LUIS services
Language/techniques used
Bot framework composer Natural language processing
Models used
MS Azure LUISMS Azure QnAMS Azure speed SDK
Skills used
Deep learningWeb developmentCloud tech
Web Cloud Servers used
Microsoft Azure web platform
What are the technical Challenges Faced during Project Execution
Monthly quota for LUIS authoring service was reachedTracking multiple items ordered by userAccessing relevant images for each menu item
How the Technical Challenges were Solved
Switching to a more suitable pricing tier which would have to eventually switch to when move onto production phaseCreating custom functions and intents for different trackersUsing open license images from internet
Project Snapshots 










Project website url Demo








Previous articleDo All Social Media Is Owned By Meta?Next articleMetaBridges API Decentraland Integration – AR, VR Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2119,https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investments/,"Impact of news, media, and press on innovation, startups, and investments","

Client Background
Client: A leading research institution in the word
Industry Type:  Research, R&D
Services: R&D
Organization Size: 1000+
Project Objective
Make data ready for predictive modelling. 
Making Google Data Studio dashboard.
Project Description
Phase – 1: In this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data.
Phase – 2: Finding co-relation between the pitchbook data and the other output files.
Phase – 3: Making dashboard in Google Data Studio for the project.
Our Solution
We used pandas and numpy to clean the data and make useful for it to be used in predictive modelling. We have found the co-relation between the tempa msa pitchbook data and the output files like textual file, ai_ml_tm file etc. We have made the dashboard using the Google Data Studio.
Project Deliverables
We have provided a excel file consisting of clean data and the Google Data Studio report.
Tools used
Python, Google Data Studio, Google Chrome
Language/techniques used
Python Programming 
Models used
Waterfall model used in this project.
Skills used
Data cleaning, Data Pre-processing, Data Visualisation are used in this project.
Databases used
We have used the traditional file systems as database storage.
What are the technical Challenges Faced during Project Execution
Cleaning the data was the major challenge faced while executing the project. The data has a lot of noise. It was difficult to find which data was useful and which data is not useful in this project. Secondly the co relation between the output files and pitchbook data. There was nothing common between both the datasets. So was difficult to find co-relation between them. 
How the Technical Challenges were Solved
We used pandas dataframe to clean the data and make it ready for predictive modelling and used the Google Data studio to find insights between the different datasets.
Project Snapshots 







Project Video







Previous articleAWS QuickSight Reporting DashboardNext articleHow Metaverse is Shaping the Future? Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2120,https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/,AWS QuickSight Reporting Dashboard,"

Client Background
OverviewAs a Singapore and Australia based startup, Drive lah (known as Drive mate in Australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. All trips on Drive lah are comprehensively insured through our insurance partners so car owners don’t have to worry about their insurance. The idea is simple: car ownership is expensive in Singapore (per month yet only use the car 5% of the time – cars are mostly parked. With Drive lah you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. Renters can rent those cars when they are not used by their owners at good value.In a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, Drive lah is envisioning to take the lead in distance travel and simplifying car access
Websitehttp://www.drivelah.sgCompany size11-50 employeesFounded2019
Project Objective
Automating the process to get updated Metrics every week.
Evaluate the following Performance Metrics which will be used on AWS Quick Sight for Performance Evaluations:
Total CancellationsCancellations by HostWeekly Guest Success Rate.Monthly Active User’s {MAUs}Monthly Active Listings {MALs}Total Approved & Live ListingsApproved & Live InstantBookingsApproved & Live Dl GoDelivery Booking ListingsWeekly Active Listings {WALs}Successful HDMUnsuccessful HDMBooking Acceptance RateTotal Requested TripsNew Listings Made LivePercentage of Live Listings Made ActiveMap Location Metrics Table with Postal Districts.DL Live Cars & DL L3M Active Cars Host Experience Team Weekly DashboardNew Weekly Listings DashboardTwo Transaction Metrics
Build Code for extracting Daily Agent Activity Report on Daily Basis.
Our Solution
For Performance Metrics, we suggested that we will Code for each Metric & will store them in a Table on AWS RDS which will be directly synced to the AWS Quick Sight for Performance Evaluations.
For Automating the process to get updated Tables of Metrics every week, we suggested to use a Virtual Machine on which we can upload all code files & can run a Cron Job for each file to automatically get updated on specified time every week.
Tools used
Jupyter NotebookPyCharmMySQL WorkbenchAWS Quicksight
Language used
Python
Database Used
Amazon Relational Database Service (RDS)
What are the technical Challenges Faced during Project Execution?
Tried with AWS Lambda Function to update tables on AWS RDS but Lambda Function was unable to run complete code.
How the Technical Challenges Were Solved?
Suggested to use a Virtual Machine on which we can upload our Code Files & can run Cron Job for automatically updating tables on regularly basis.
Project Snapshots
Metrics from Listings Table:

Host Experience Metric:

New Live Listings of Last 7 Days:

Line Chart of Total Cancellations & Cancellations by Host:

Line Chart of Monthly Active Users (MAU’s):

Area Chart of Percentage of Live Listings Made Active:

Line Chart of Number of DL GO Listings & Number of Instant Booking Listings:

Line Chart of Monthly Active Listings (MAL’s):

Line Chart of New Listings Made Live:

Vertical Bar Chart of Total Approved & Live Listings:

Project Video Link 







Previous articleGoogle Data Studio Dashboard for Marketing, ads and Traction dataNext articleImpact of news, media, and press on innovation, startups, and investments Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2121,https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/,"Google Data Studio Dashboard for Marketing, ads and Traction data","

Client Background
OverviewBankiom – the super banking app for MENA on a mission to make managing your finances easier.
☞ Open an account on your phone and get a virtual card in 3 minutes or less☞ Manage all your bank accounts from one app and one control panel☞ Save money and grow your wealth
Websitehttp://www.bankiom.comCompany size2-10 employeesFounded2019SpecialtiesBanking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech
Project Objective
Build a dashboard unifying all the platforms that we use: Google Ads, FB ads, Appsflyer, Mixpanel
Project Description
We want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel):– App settings in Appsflyer– SDK Installation, test it (+ instruction for devs)– Ad sources setup in ad accounts (Facebook, Google Ads, etc)– Ad sources setup in Appsflyer– In-app conversions mapping– Conversion set up in ads sources– One link, smart script, and deep link setup– SKAD Network for IOS app
Our Solution
Built dashboard for each data source like Google Ads, Facebook Ads for tracking installs, channel spend, cost per install for both Android and IOS.
Then, we made a dashboard for tracking the retention rates of customers and other events that they execute on the app like transfer money, user registration, connect banks. The data for these events was fetched from MixPanel.
These dashboards were made using Google Data Studio.
Project Deliverables
We need to deliver dashboards for tracking the ads data from Google and Facebook and to track the events which the users perform on their app and for this data was collected from MixPanel.
Tools used
Following Tools were used for successful execution of the project
Google Data StudioAdveronix Mixpanel ApiBigQueryGCP
Language/techniques used
Code was written to create the pipeline to fetch MixPanel data through mixpanel Api and store it in bigquery. So, the code was written in Python.
Skills used
Following Skills were used to complete the project
Data PreparationData VisualizationPythonAPIBigQueryGoogle Cloud Platform
Databases used
For storing the data of the project Google Sheets and Google BigQuery were used.
Web Cloud Servers used
Web Cloud server used in this project was Google Cloud Platform.
What are the technical Challenges Faced during Project Execution?
Technical Challenges faced during the execution of the project was to understand how the api of the mixpanel works and how to connect it to Google BiqQuery. Another technical challenge that we faced was to find a free resource to connect the facebook ads data to data studio.
How the Technical Challenges were Solved
To solve the technical challenges we went through the documentation of the mixpanel api to get a understanding of how the things work. Based on that we built the pipeline to connect the mixpanel data to big query. The other technical challenge of finding a free resource to connect the facebook ads to datastudio for free was solved by researching for the various connectors available and we found an add on named ‘Adveronix’ which could connect the facebook ads data to google sheets which can eaily be connected to data studio.
Project Snapshots




















Project website url
https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f
Project Video







Previous articleGangala.in: E-commerce Big Data ETL / ELT Solution and Data WarehouseNext articleAWS QuickSight Reporting Dashboard Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2122,https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/,Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse,"

Client Background
Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America
Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …
Industry Type:  eCommerce
Services: e-commerce, retail business
Organization Size: 100+
Project Title
Gangala.in: E-commerce site gathering data of different products from various sources and providing it on a single platform
Project Objective
Provide up-to-date data of any given product on the website along with 3-5 prices of that product from different sites for the customer to compare and buy. 
Project Description
A platform in which users can get price data of any product from multiple sites. The client provided us with raw data. We were tasked with building a pipeline for the data, build API’s to get product data such as price and update them and make sure that all the data is available for the front end team to access.  
Our Solution
We built them a pipeline to process and clean the raw data provided. We built API’s to fetch the updated data of the products. Neo4j was used as the intermediary data and mongoDB was used as our primary database. We also process the images of each product and remove any unwanted texts from it and add the client’s watermark. 
Project Deliverables
A fully-updated database with up to date data on all the products and each product having atleast 3-5 prices from different sites. 
Tools used
Numpy package Json package csv package concurrent futures package (for multithreading)Py2neo package (to connect to neo4j using python)
Language/techniques used
Python Cypher Query Language (CQL)APOC Queries
Databases used
Neo4jMongoDBDataiku OdooDSS
Web Cloud Servers used
Linode cloud servers 
What are the technical Challenges Faced during Project Execution
We were asked to process 3million products per day and this was a challenge as the VM’s we used were not able to handle the load. 
How the Technical Challenges were Solved
We were able to overcome the challenge by using Asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as well
Project website url
https://gangala.in/




Previous articleBig Data solution to an online multivendor marketplace eCommerce businessNext articleGoogle Data Studio Dashboard for Marketing, ads and Traction data Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2123,https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-business/,Big Data solution to an online multivendor marketplace eCommerce business,"

Client Background
Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America
Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …
Industry Type:  eCommerce
Services: e-commerce, retail business
Organization Size: 100+
Project Objective
To give User experience of easy and convenient Shopping by searching all the products like any medicines , Clothes , Gadgets etc in a single Website without going through all the E-Commerce Sites and make shopping easy and get the most affordable and best product.
Project Description
It’s an E-Commerce Sites that’s helps customer to compare different  products that were available on different E-Commerce Sites like Flipkart , Amazon , Netmeds etc.It’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.The gives the user a great and friendly Experience in Buying any Products.It’s Also have some Unique Similar Products Recommendation Based on user search and also have a ChatBot That’s solves User Query .It’s uses Big data and Rest API that’s help the projects for regular updates and regular fetching of the new products.
Our Solution
In BlackCoffer We create the flow of the Big Data and all Backend Solution That is requires for this futuristic E-Commerce Sites.We Create Pipelines for the data of all the products and their price and url fetch from different E-Commerce Sites using Custom made APIs And perform many data cleaning, data transformation and data validation techniques to make sure the standard of data to be used by Our Sites .We also get Additional Feature from the scraped data By using Different APIs . We also create automation and custom python scripts that helps us to achieve some outstanding data related tasks.
Project Deliverables
Python script for performing ETL and Cypher Query for big data Handling.
Tools used
Jupyter NotebookDSSVS Code
Language/techniques used
PythonNo SQlCypherETL
Models used
Similar Price APIWhatsapp Chat APISimilarity Server to get similar products
Skills used
Data EngineeringData AnalysisPython ProgrammingRest APIs
Databases used
DSSNEO4JMongoDB
Web Cloud Servers used
LinodeAWS 
What are the technical Challenges Faced during Project Execution
Data Cleaning : -The Scraped that will be used by our sites is coming from different sources and also it’s not’s that clean to be used by sites .This is the very first problem every data scientist faced during the whole process.Data Merging :-  The data is scraped from around 140 sources that’s why it’s very difficult to maintain the attributes that should be used by all the sources and we can get a clean and sufficient amount of data to process.Data Validation :-  There are many records that have null values and missing values that disturb the users experience a lot .That should be handle with very care.
How the Technical Challenges were Solved
Data Cleaning : – For Data cleaning we used Python Data Frame and Pandas and data structure and handles the data cleaning and optimize our data for get correct data format and useful data.Data Merging : –  For data Merging and data transformation we used pandas that help to get the appropriate data that can used further And also make Python pipelines for future updation.Data Validation :- For that data validation we use some fundamental property and feature selection that’s help us to make the appropriate data format and records to be used In our sites.
Project Snapshots










Project website url
https://gangala.in/




Previous articleCreating a custom report and dashboard using the data got from Atera APINext articleGangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2124,https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-atera-api/,Creating a custom report and dashboard using the data got from Atera API,"

Client Background
Client: A leading Marketing firm in USA
Industry Type:  Marketing
Services: Marketing, consulting, ads, business solutions
Organization Size: 20+
Project Description
Atera.com is used as our RMM, we have an agent on every machine. Which tracks the if a machine goes down, initial response time etc.., The website doesn’t provide any standard reports, So we needed to create a custom report.
Our Solution
Importing the data from Atera API into JupyterUsing Web Scraping download the JSON dataConvert the JSON data to Data Frame and download it into PC.Clean the data with only required columnsUpload the data into google sheets.Connect google sheets and google data studioCreate the dashboard with the data
Tools used
Python (Pandas, requests)Google SheetsGoogle Data Studio
Skills used
AnalyticsProgramming Language
Databases used
Contacts.csvCustomers.csvTickets.csvAlerts.csv
What are the technical Challenges Faced during Project Execution?
I found it difficult on downloading the data.
How the Technical Challenges were Solved
Once I figured I have been using the wrong Authorization key to login I was able to solve the issue, and convert the curl command into python
Project Snapshots









Project website url
https://datastudio.google.com/reporting/5e61aecb-a420-41cc-afba-d0ca37f69132
Project Video







Previous articleAzure Data Lake and Power BI DashboardNext articleBig Data solution to an online multivendor marketplace eCommerce business Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2125,https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/,Azure Data Lake and Power BI Dashboard,"

Client Background
OverviewStone is a video bibliographic tool for journalists and other researchers.
It allows users to capture, annotate and share their journeys through digital and physical space, producing verifiable logs and generating monetizeable video highlight reels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story.
Our mission is to address distrust and disinformation with transparency and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work.
Research is valuable. Make it Visible.
Write In Stone.
Websitehttp://www.writeinstone.comCompany size2-10 employeesHeadquartersBlackheath, New South WalesFounded2017SpecialtiesResearch Transparency, Trust, Video Content, Journalism, Proof Of Work, and Bibliographic Standards
Project Objective
Working on Microsoft Azure Analytics ServicesVerifying that indicators are being gathered in an intended manner, in line with GDPR provisionsBuilding and analyzing dashboards and, specifically, conversion funnels
Project Description
To determine whether the already implemented indicators in are in intended fashion (separated by where these indicators are placed in the currently constituted funnel)Implement New IndicatorsResearch LoggedAverage Number of Highlights per ProjectTotal Hours of Content ProducedTotal Hours of Content WatchedDaily unique visitors engaging with Stone, including the landing page, public research page(s), and the research portalAssess the dashboard set up in Azure, refine the existing dashboard, and determine whether an alternative is preferable. Review, refine, and optimize the WIS conversion funnel(s)
Our Solution
Built a Power BI dashboard as per the requirement. Also built a separate dashboard for the metric data from Azure.
Project Deliverables
Power BI dashboard which contains indicators funnels, new indicators(Research logged, Average number of Highlights per projects, Total hours of content watched etc), visualizations  extracted from metric data.
Tools used
Power BIAzure
Language/techniques used
Power BIDAXKusto QueryAzure
Skills used
Data collectionData AnalysisData cleaningFeature engineeringQueryingVisualization
Databases used
Azure database
Web Cloud Servers used
Azure
What are the technical Challenges Faced during Project Execution
Difficulty in data collection. 




Previous articleAdvantages and Disadvantages of E-learning during the COVID-19 for students and teachersNext articleCreating a custom report and dashboard using the data got from Atera API Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2126,https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/,Google Data Studio Pipeline with GCP/MySQL,"

Client Background
Client: A leading IT firm in Europe
Industry Type:  IT
Services: e-commerce, retail business, marketing, Consulting
Organization Size: 100+
Project Objective
Creating a Data Pipeline to sync live data from FieldPulse to Google Data Studio using GCP/MySQL.
Project Description
There is a Virtual Machine up and running and MySQL in Google Cloud(GCP). Get the following live data from FieldPulse to Google Data Studio(GDS) for making Business Dashboard in GDS –
Job DataTag DataTeam Member DataTeam Data
Such that if data changes in FieldPulse , GDS Dashboard should update automatically.
Our Solution
For fetching data from FieldPulse –
Data Pipeline (FieldPulse to GCP MySQL) :  We have created a Data Pipeline that uses web scraping to fetch data from FieldPulse. It makes a GET request to the FieldPulse API , and the API returns raw data. Convert this into json format then in Dataframe. Now , create new tables in GCP MySQL and insert/update the data accordingly.Insertion & Updation of Data : 
Insertion : If any data fetched from Fieldpulse is not present in their respective database table , then  insert that data in the table.Updation : If any data fetched from Fieldpulse is present in their respective database table , then update that data in the table.Deploy the above Data Pipeline in GCP VM instance :  Deploy the above data pipeline in GCP VM so that data gets updated every hour from FieldPulse to MySQL.
For getting data from GCP MySQL to Google Data Studio(GDS) :
Connecting GCP MySQL to Google Data Studio :  Connect GCP MySQL to GDS as follows –
Open a new reportClick on add dataChoose MySQL connectorEnter following fields :
Host Name or IP  :  xxx.xxx.xxx.xxxDatabase             :   xyzUsername            :   xyzPassword             :   ********** Enable SSL
Upload server-ca.pem certificateUpload client-cert.pem certificateUpload client-key.pem certificate
Click AuthenticateAdd whatever table you wantBuild Visualization
Project Deliverables
Below are the services that we provided to client after completion of this project –
Deployed Data Pipeline in GCP :  A Data Pipeline connecting FieldPulse(https://webapp.fieldpulse.com/) to GCP MySQL that is deployed on a client’s GCP VM instance. It updates the data in MySQL every hour. It extracts the following data tables from FieldPulse –
Job DataTag DataTeam Member DataTeam DataMaintaining a log file in Google Cloud :  There is a log file in cloud to resolve unexpected error quickly if any , log file stores following details –
last pipeline synced timeError type if anyError location if anyWork Order Data :
Job idWork order no.Tags titlesStart_timeJob_typeCreated ByStatusInvoice_statusAssigned teams nameProject_idAssignment_countAssignable_typeNotesCustomer_notesCustomer_first_nameCustomer_last_nameLocationAssigned_team_members nameEnd_timecreated_atJob Tag Data :
Tag idsCompany_idMongo_idTitle (Tag name)TypeColorCreated_atUpdated_atdeleted_atSetup to Connect GCP MySQL to Google Data Studio(GDS) :  Provided a setup to connect GCP MySQL to GDS easily. Client can access his live data from MySQL to GDS and make visualizations out of it. 
Tools used
Google Colab
Language/techniques used
PythonWeb ScrapingMySQL
Skills used
Programming in Python  Data Structure & Algorithm Web ScrapingFile HandlingGoogle CloudGoogle Data Studio
Databases used
MySQL
Web Cloud Servers used
Google Cloud Platform (GCP)
What are the technical Challenges Faced during Project Execution
Getting Data from FieldPulse : As there is no open source package/library in Python for accessing Fieldpulse API , we struggled a lot to get the desired data from FieldPulse.Setting Up Connection from GCP MySQL to GDS :  Due to firewall and VPN , connection was not set up as IP address changes while using VPN. It was showing an error every time someone tries to connect to MySQL from their Google Studio account.
How the Technical Challenges were Solved
Getting Data from FieldPulse : We did use web scraping for this. We explored all the API addresses. We connected to each possible address and got the data then explored the data. Made a list of addresses which contains data of our interest. Also data is stored in a scattered and cascaded manner in FieldPulse with ids. So , we had to fetch a lot of extra tables and then join multiple tables to get a desired data table.Setting Up Connection from GCP MySQL to GDS :  To resolve this issue , we did as below –
set up the IP address in GCP MySQL security to 0.0.0.0 , so that any system can access it. (VPN issue resolved)Enabled the SSL in GCP MySQL. (to prevent it from unauthorized access)
Project Video







Previous articleQuickBooks dashboard to find patterns in finance, sales, and forecastsNext articleAI and its impact on the Fashion Industry Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2127,https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecasts/,"QuickBooks dashboard to find patterns in finance, sales, and forecasts","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: e-commerce, retail business, marketing
Organization Size: 100+
Project Objective
Build a fully Integrated BI Platform in PowerBI using native connectors and APIs(QuickBooks and Airtable) to pull real time data from many sources.
Project Description
For building a fully integrated BI Platform , the data has to come from the following sources to feed it to PowerBI –
·         QuickBooks :  An accounting software that accepts real-time business payments ,  manage and pay bills, manage organisation’s deposits/expenses , customers ,and payroll functions. The following data/tables has to be fetched from Quickbooks –
o   Customer
o   Invoices
o   Product & Services
o   Payments
o   Expenses
o   Deposits
o   Accounts
o   Vendors
o   Departments
o   Classes
·         Airtable : An online database hybrid platform for creating and sharing relational databases with friendly user interfaces. The following databases with multiple data table has to be fetched from Airtable –
o   Marketing Data Analytics Base (Google Ads , Facebook Ads)
o   Payroll Tracking (Payroll , Hours Log)
 This Quickbook and Airtable real time data has to go to the powerBI service (https://app.powerbi.com). Then create useful visualisation and dashboards based on plan and feedback from the executive team. All visuals in dashboards should automatically update without any intervention to make it fully integrated.
Our Solution

Collecting data tables from data sources :
Data Pipeline(QuickBooks to Airtable) – We have built a Data Pipeline in Python that uses quickbooks API(https://pypi.org/project/python-quickbooks/) to get raw data tables from QuickBooks and uses Airtable API (https://api.airtable.com/v0/base_key/Table_name?api_key=YOUR_API_KEY) to write/update data in Airtable. It fetches all the below raw tables after making requests to QuickBooks API –
Customers , Invoices , Expenses , DepositsAccounts , Departments , Vendors etc.
After getting these raw data tables , pipeline converts it into DataFrame , then writes/updates it into Airtable. 
The Pipeline is deployed in a server that runs every night , it fetches the data from QuickBooks API and writes/updates to Airtable.
Airtable to PowerBI – As there is no connector available to sync data from Airtable to PowerBI. We have used pagination using DAX queries to get data from Web Sources i.e. Airtable API. Pagination fetches the data page by page  using a source and offset technique set by the Airtable API developers. It successfully fetches all the below bases from Airtable API –
Marketing Data Analytics Data (Google Ads , Facebook Ads)Payroll Data (Payroll , Hours Log)
Scheduled Refresh :  To refresh visualization/dashboard (If incoming data from Airtable API has updated) , set refresh time in powerBI service.
Preprocessing of  Data – We have used DAX queries to prepare and process the raw data coming from Airtable like –
Split data , typecast dataFilter data (fill missing values , delete irrelevant rows etc.)Create visualizations/Dashboards – We have used following techniques to create visualizations –
Used M code queries to extract useful/desired dataUsed measure to perform calculations on dataUse a calculated table to create a relationship between two tables.Used data joining (Union , Intersection) to get desired data
Project Deliverables
Below are the services that we provided to client after completion of this project –
Deployed Data Pipeline :  A Data Pipeline connecting QuickBooks to Airtable to sync in the following data tables –
CustomersInvoicesProduct & ServicesExpenseDepositsPaymentsAccountsVendorsDepartmentsClassesQuickBooks Data Dashboard : It contains following visualizations –
KPIs –
Total RevenueTotal SpendTotal ProfitProfit MarginNo. of Customer
Line Charts –
Revenue/Expense over days
Bar Charts –
Revenue & Expenses by BusinessesProfit/loss by BusinessesRevenue & Expense by ClassProfit/loss by Class
Pie Chart
Expenses by CategoryPaid/Unpaid Invoices
Tables –
[Class , Business , Revenue , Spend , Profit , Profit Margin)[Customer , Balance , Due(in days)][Customer , Balance , OverDue][Account , QuickBooks Balance]
Filters/Slicer –
Transaction DateBusinessClassMarketing Analytics (Facebook Ads) Dashboard –
KPIs –
All ImpressionsTotal ReachTotal Link ClicksAverage CPMAmount Spent on AdsTotal BudgetBudget Left
Line Charts –
Avg. Frequency Over DaysAvg. CPC over daysImpressions , Reach and Page Engagement over daysLink Clicks by day and Account NameResults , Cost per Results over daysAd set Budget and Amount Spent Over days
Bar Charts –
Ad set Budget and Amount Spent by Account Name
Gauge –
Daily Avg. LinksCount of Ongoing Campaigns
Tables –
Top Compeigns [Account name , Compeign name , Link Clicks , Impressions , Reach , Avg. Frequency , Social Impressions]
Filters/Slicer –
Account nameDate RangeMarketing Analytics (Google Ads) Dashboard –
KPIs –
Total ImpressionsTotal ClicksTotal ConversionsTotal CostDaily Avg. CostDaily Avg. CTR Daily Avg. Conversion RateDaily Avg. Cost per Conversion
Line Charts –
Clicks and Conversions over daysAvg. CPC over days by Day and Google Ad AccountClicks per Impressions by Day and Google Ad AccountImpressions by Day and Google Ad AccountCost by Day and Google Ad AccountClicks by Day and Google Ad Account
Gauge –
Avg. Daily new Conversions
Pie Chart –
Count of Google Ad Accounts
Tables –
Top Ads [Ad name , Ad Group , Conversions][Google Ad Account , Impressions , Clicks , Conversions]
Filters/Slicer –
Date RangeGoogle Ad Account NamePayroll Dashboard –
KPIs –
$ Total Payroll$ Avg. RateCount of InvoiceTotal Payroll Time (in hrs.)Avg. TurnArroundTime (in Days)Total Hours
Line Charts –
Avg. Rate over DaysAvg. daily Pay Amount
Bar Chart –
Payroll time by Employee$ Payroll by EmployeeHours by EntityTotal hours by Employee
Pie Chart –
Paid/Unpaid Invoices
Tables –
Payroll [Employee , Count of Invoice , Total Due , Paid Before/After Due Date]
Filters/Slicer –
Date RangeEmployee nameEntity name
Tools used
PowerBI
Language/techniques used
PythonPagination
Skills used
Programming in Python  Data Structure & Algorithm API Integration (QuickBooks , Airtable) File Handling PowerBI(with DAX , M code queries) Data Analytics
What are the technical Challenges Faced during Project Execution?
QuickBooks Refresh Token Expired Issue : As stated in QuickBooks Developer Guide , we need refresh token to access QuickBooks API and it expires after 101 days. But that is not the case , it usually expires within 2 to 4 days depending on how frequently we access the API. In that case our deployed Pipeline does not work if the token expired.Getting data from Airtable to PowerBI : As PowerBI has no Airtable data source connector to fetch data from Airtable , we did use Web Source connector using Airtable data web links. It only fetches the 1st page that is 100 rows from Airtable base because Airtable API gives only 100 rows/request.Dynamic Data Source Refresh Issue :  As the URL of Airtable bases data changes based on the size of data. PowerBI recognizes it as Dynamic Data Source , hence it gives the error “Dynamic Data Source Refresh Error” in PowerBI Service.
How the Technical Challenges were Solved
QuickBooks Refresh Token Expired Issue :  As the token may expire anytime after 2 days , so to resolve this we have added a gui element in Pipeline so that if token expires a pop up will appear asking for a new refresh token , until the consumer enters a valid new token from their QuickBook developer account , a pop up will keep coming and pipeline will be paused. Once the user enters a new token , the pipeline will continue working.

Getting data from Airtable to PowerBI : To resolve this issue , we have used Pagination technique as below –
First request Airtable API with proper URL , api_key and blank_offset (data_url?API_KEY=api_key?OFFSET=blank_offset)This request returns first 100 rows of data and a new offset valueNow replace the previous offset value with a new offset value in the URL , and again make an API request.This request will return the next 100 rows of data and a new offset.Do this until you get a null offset (null offset means , all data has been fetched)
This is how we get all the data of any size from Airtable bases.
Dynamic Data Source Refresh Issue : The above mentioned Pagination technique converts dynamic URLs of Airtable bases data into static URLs. So PowerBI gives no error as it has been converted to a static data source. Now the client can refresh the dashboard manually by clicking the refresh button or can set automatic refresh daily at some given time. 
Project Snapshots




Project Video
https://www.youtube.com/watch?v=iemcyRtWPNU&ab_channel=Blackcoffer




Previous articleMarketing, sales, and financial data business dashboard (Wink Report)Next articleGoogle Data Studio Pipeline with GCP/MySQL Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2128,https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/,"Marketing, sales, and financial data business dashboard (Wink Report)","

Client Background
Client: A leading retail firm in Australia
Industry Type:  Retail
Services: e-commerce, retail business, marketing
Organization Size: 100+
Project Objective
Bringing in data from many sources(Google Analytics , ServiceM8 and Xero etc.) and making Business Dashboard KPIs in Wink Report.
Project Description
For building Business Dashboards in Wink Report , collect data from the following sources – 
ServiceM8XeroFacebookGoogle AdsCommuniqa
Explore/analyze the underlying data tables from each Data Source. Make useful reports using different tables from different data sources based on client’s requirement. Set up formulas in each report to calculate desired fields. Add a custom visualization to each report for making dashlets. Add dashlets to newly created dashboards. 
Our Solution
For collecting the data from the sources (ServiceM8 , Xero , Facebook , Google Ad) native connectors have been used , available in the Wink Report. It fetches the following data/tables from around the given data sources –
ServiceM8 Connector –
AssetsClientInvoicesJob AllocationsJobsMaterialsPaymentsXero Connector –
Bank Transaction ItemsBudget Vs ActualEmployeesPaymentsPayslipProductsPurchase OrdersPurchase InvoicesSales InvoicesTransactionFacebook Connector –Facebook Ad InsightsGoogle Ads Connector –Ad InsightsGoogle Analytics Connector –
eCommerce CampaignTotals
Data Pipeline : For collecting data from Communiqa website (https://www.communiqa.com.au/) , web scraping has been used as there is no connector available for Communiqa to Wink Report. By scraping Communiqa , we get the following data –
Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc.
Then , we have merged different tables from different sources to get desired reports. Store all reports belonging to the same dashboard in a separate folder. Do this for all the dashboard , then setup formula for calculating desired fields. Add appropriate visualization to each report for each folder. Then , finally add all dashlets belonging to the same folder to a newly created dashboard.
Project Deliverables
Below are the services that we provided to client after completion of this project –
Data Pipeline(Communiqa to Wink Report) :  A Data Pipeline connecting Communiqa to Wink Report to sync in the following data tables –CSR calls [Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc]Company Performance Dashboard : It contains following visualizations –
KPIs –
Sales This MonthLeads Booked TodaySales TodayRevenue This MonthCash Payment This MonthConversion RateOpen Warranty Jobs
Bar Charts –
Scheduled Jobs by CategorySales by MonthRevenue by Month
Tables –
Open Jobs from Last month[Job Id , Opened Date , Status, Invoice Amount, Amount Paid]
Filters/Slicer –
Date RangeJob StatusDate Grouping(Daily/Monthly/Yearly)Lead Generation Dashboard –
KPIs –
Total Website Traffic this monthAverage Daily Website Traffic this monthNo. of Conversion this monthTotal Marketing Investment this monthMarketing Budget TrackingCost per Acquisition
Line Charts –
Link Clicks and conversion by monthTotal marketing spend by month
Bar Chart –
Lead Generation Count by Source
Pie Chart –
Lead Generation Source by Invoice Amount
Filters/Slicer –
Date RangeJob StatusLead Conversion Dashboard –
KPIs –
All Employees monthly Sales TargetAll Employees monthly Conversion Rate
Filters/Slicer –
Date RangeJob StatusCompany Leads/Target Dashboard –
KPIs –
Total Hi-pages Lead todayTotal Hi-pages Lead this monthTotal OneFlare Lead todayTotal OneFlare Lead this monthTotal Google Ads Lead todayTotal Google Ads Lead this monthTotal Facebook Ads Lead todayTotal Facebook Ads Lead this monthCompany Daily Sales TargetCompany Monthly Sales Target
Filters/Slicer –
Date RangeJob Status
Tools used
Wink Report
Language/techniques used
PythonWeb Scraping
Skills used
Data AnalyticsData VisualizationProgramming in Python  Data Structure & Algorithm Web ScrapingFile Handling 
What are the technical Challenges Faced during Project Execution
Merging reports from different data sources : Faced the issue of making the cross report from different data sources.Take live parameter input daily in Dashboards from User : Taking live user parameter input daily to feed in Wink report Dashboard. So that dashboard KPIs can change accordingly.
How the Technical Challenges were Solved
Merging reports from different data sources : Resolved this issue by using merge report configuration. Using this we were able to join tables from different data sources like – Left join , Right join , Union etc.Take live parameter input daily in Dashboards from User : To resolve this issue , we added a custom field in reports with input tag. Users can enter their parameter in this custom field and all dashlets in the dashboard would update automatically.
Project Snapshots 







Project Video







Previous articleReact Native Apps in the Development PortfolioNext articleQuickBooks dashboard to find patterns in finance, sales, and forecasts Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2129,https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/,React Native Apps in the Development Portfolio,"

Here are the list of react native apps developed by the team and the resources:
https://itunes.apple.com/us/app/truckmap-truck-gps-routes/id1198422047?mt=8
https://play.google.com/store/apps/details?id=com.truckmap.truckmap
https://play.google.com/store/apps/details?id=com.verifai.standalone
https://apps.apple.com/nl/app/verifai/id1504214033
https://apps.apple.com/de/app/meetlist-lokale-aktivit%C3%A4ten/id1439183715
https://play.google.com/store/apps/details?id=de.mlug.meetlist
https://play.google.com/store/apps/details?id=com.payroo.employee
https://play.google.com/store/apps/details?id=com.vahcare
https://play.google.com/store/apps/details?id=com.candorivf




Previous articleA Leading Law Firm in the USA, Website SEO & OptimizationNext articleMarketing, sales, and financial data business dashboard (Wink Report) Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2130,https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/,"A Leading Law Firm in the USA, Website SEO & Optimization","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Connect website to Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.
Fix SEO of the website.
Project Description 
Connecting website to Google Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.
Fixing SEO of the website.
Our Solution
Website connected to Google Search Console, Google Analytics and Facebook Pixel successfully.
Fixed the 
meta description errorbroken link error404 error, etc.
Tools used
SquarespaceGoogle Tag ManagerGoogle AnalyticsGoogle Search Console
Language/techniques used
JavaScript
Skills used
SquarespaceGoogle Tag ManagerGoogle AnalyticsGoogle Search ConsoleJavaScript
Project Snapshots
Project website URL
https://www.keepingorlandomoving.com/




Previous articleA Leading Hospitality Firm in the USA, Website SEO & OptimizationNext articleReact Native Apps in the Development Portfolio Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2131,https://insights.blackcoffer.com/a-leading-hospitality-firm-in-the-usa-website-seo-optimization/,"A Leading Hospitality Firm in the USA, Website SEO & Optimization","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Working On-page SEO of the pages to make it user-friendly and feasible for crawlers to make the site indexing better.
Project Description
Firstly, exploring the Liverez as it was a new platform then, performing intermediate SEO like page titles and description, completing word count, alt. text and removing duplicate page title and description.
Our Solution
To increase the organic traffic of the site and improve the insights.
Project Deliverables
There was a bit of improvement in the traffic of the site.
Tools used
Brightlocal.com, Yoast SEO, Grammarly
Language/techniques used
Basic HTML
Skills used
ON-page SEO
Project Snapshots





Project website url
https://www.missionbeach.com/




Previous articleA Leading Firm in the USA, Website SEO & OptimizationNext articleA Leading Law Firm in the USA, Website SEO & Optimization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2132,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-website-seo-optimization/,"A Leading Firm in the USA, Website SEO & Optimization","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Fixing On-Page SEO of the website
Project Description
Fixing On-Page SEO contains things like title, meta description, image-alt text, broken links, 404 error page, multiple h1 tag in one page, duplicate title/description, dynamic URL, sparse content page (word count <500), etc. 
Our Solution
Fixed all the possible solutions that we can do for improving the SEO health score. Fixed, image-alt text error, title, meta description, broken links, dynamic URL, 404 error page, sparse content pages, contact information on all pages, connecting website with Google search console.
Tools used
AhrefsWordPressGoogle Search Console
Language/techniques used 
HTMLRedirection plugin
Skills used
HTMLWordPressGoogle Search Console
Project Snapshots










Project website URL
URL https://www.jupiteroutdoorcenter.com/

Home





Previous articleA Leading Musical Instrumental, Website SEO & OptimizationNext articleA Leading Hospitality Firm in the USA, Website SEO & Optimization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2133,https://insights.blackcoffer.com/a-leading-musical-instrumental-website-seo-optimization/,"A Leading Musical Instrumental, Website SEO & Optimization","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Connect website to Google Tag Manager.
Remove error.
Project Description
Remove all previously added code and add new code for connecting to Google Tag Manager.
Remove 5xx error from the website.
Our Solution
Website connected to Google Tag Manager successfully.
Removed 5xx error.
Tools used
Google Tag ManagerWordPress
Language/techniques used
JavaScript
Skills used
WordPress
Google Tag Manager
Project website URL

URL: https://www.hamiltonpianoco.com/




Previous articleA Leading Firm in the USA, SEO and Website OptimizationNext articleA Leading Firm in the USA, Website SEO & Optimization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2134,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-seo-and-website-optimization/,"A Leading Firm in the USA, SEO and Website Optimization","

Client Background
Client: A leading marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Connect website to Search Console. Add Call Rail Code
Project Description
Connecting website to Google Search Console through Google Tag Manager.
Connect website with CallRail.
Our Solution
Website connected to Google Search Console successfully.
Added CallRail code to the website.
Tools used
kvCoreGoogle Tag ManagerGoogle Search ConsoleCallRail
Language/techniques used
JavaScript
Skills used: 
kvCoreGoogle Tag ManagerGoogle Search ConsoleCallRailJavascript
Project Snapshots



Project website URL: 
https://www.12stonesnwa.com/




Previous articleImmigration Datawarehouse & AI-based recommendationsNext articleA Leading Musical Instrumental, Website SEO & Optimization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2135,https://insights.blackcoffer.com/immigration-datawarehouse-ai-based-recommendations/,Immigration Datawarehouse & AI-based recommendations,"

Client Background
Client: A leading business school worldwide
Industry Type:  R&D
Services: R&D, Innovation
Organization Size: 100+
Project Objective
Objective of this project is to research and collect news article data sourcing from Canada, based on the keyword. 
Project Description
There were 3 phases of the project. 
Phase 1– Data collection and selectionData related to anyone coming to Canada (new comers)Data related to anyone coming to Canada (new comers) Canadian policy to new comersi.e. from any country to CanadaData containing News, press, think tanks, government policy documents, or research institutions releasing the news or press aboutThe news source should be limited to Canada onlyTime span- 2005 to 2021Output- Excel having URLs or the documents along with the source type, keywords, and date on which that article is posted.
Phase 2– Documents text data extraction Develop tool to collect and extract data from each URL.Clean and save the texts in the text documents
Phase 3– Textual AnalysisSentiment AnalysisAnalysis of readabilityTopic modelling 
Our Solution
We provide them with completed Phase 1 in an excel sheet and ongoing samples for Phase 2. Also work for Phase 3 has been started in between to complete the Project as soon as possible in a best way.
Project Deliverables
There is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from Phase 2.
Tools used
Python, PyCharm, Jupyter Notebook, Microsoft Excel, Google Chrome is used to complete different phases of this project
Language/techniques used
Python programming language is used to do Web Scraping, Automation, Data Engineering in this project.
Models used
SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.
We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.

Figure 1 SDLC Iterative Waterfall Model
Skills used
Data scraping, cleaning, pre-processing and creating data pipelines are used in this project.
Databases used
We used the traditional way of storing the data i.e file systems. 
What are the technical Challenges Faced during Project Execution
There were a lot of challenges we faced during the project execution. 
As on the internet, raw data is available to us. So, to search for the important data specifically related to Canada only, with a lot of keywords was a challenging part for us.Then, if we somehow manage to do the task by automating it upto some extent only, we are required to find the dates of the articles, news, think tanks, documents etc, that was also a challenging part.While working on Phase 2, we need to scrape the data from the URLs, so sometimes, the news articles were removed from the website, which we earlier took in our datasets which cause problems in extracting the data.Then cleaning the webpages was also challenge for us, because this project is for research, so data is important to us. So, it was difficult to take only that data from website which we require and are most important.
How the Technical Challenges were Solved
Below are the points used to solve the above technical challenges-
We used sitemaps of websites to find different articles that we require according to the keywords, manual research was done to find out which URL will solve the purpose. Manual checking of results of automation tools, that we created, was done.To find the dates of the articles, we wrote multiple regular expressions, that will find the match for the dates that we need, also manual checking was done after that.To scrape removed webpages, we used WayBack machine or google archives, which stores all the deleted webpages.To clean the data, we filtered out various HTML tags, classes, ids by using regex, manual research.
Project Snapshots















Previous articleLipsync Automation for Celebrities and InfluencersNext articleA Leading Firm in the USA, SEO and Website Optimization Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2136,https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/,Lipsync Automation for Celebrities and Influencers,"

Client Background
Client: A leading tech firm in India
Industry Type:  Entertainment
Services: B2C
Organization Size: 100+
Project Objective
To change the lipsing of the original video with the new replaced audio.
Project Description
We needed to create an output video that will have the new lipsing according to the new replaced audio. Also we will have to change the actual audio with the new audio with automated editing.
Our Solution
We have created two different files which will perform 2 different operations 1st will replace the original audio with new and extract only video from original. 2nd will take the muted video and replaced audio and we will get the output of the new replaced audio lipsync. This is done by pre-defined model Wav2Lip on github.
Project Deliverables
2  google colab notebooks
Tools used
github
Google drive
Language/techniques used
Python 3.6
moviepy
ffmpeg
Models used
Wav2lip
Skills used
Python programming
Data science
Databases used
Provided by the company (Hrithik Roshan video files)
Project Snapshots





Project website url
https://colab.research.google.com/drive/18mlREpLmV9hj-uDfufkGJ_-m_E37Hct9?usp=sharing
https://colab.research.google.com/drive/1FZHvcVKyJxOUkUFI2auPt3vTOu4jh09K?usp=sharing




Previous articleKey Audit Matters Predictive ModelingNext articleImmigration Datawarehouse & AI-based recommendations Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2137,https://insights.blackcoffer.com/key-audit-matters-predictive-modeling/,Key Audit Matters Predictive Modeling,"

Client Background
Client: A leading business school worldwide
Industry Type:  R&D
Services: Research & Innovation
Organization Size: 10000+
Project Objective
Do regression modeling on the data provided, cross-country determinants of Key Audit Matters (KAMs) and its usefulness to Investors and Debt Market Participants
Project Description
USEFULNESS TO EQUITY MARKETS
Examine whether the number and content of KAMs varies with country-level determinants. Explore whether the usefulness of KAMs to investors varies with country level variables such as type of law, enforcement etc. Examine whether adoption of the expanded auditor’s report associated with change in audit quality? Examine whether the content in the auditor’s report improves audit quality. Does this vary across countries?Is the adoption of the expanded auditor’s report associated with a change in audit fees?Explore whether the content of auditor report moderates the usefulness of KAMs to investors (also check by country-level variables)Can the number and content of KAMs be used to predict restatements (2017 onwards)? 
In order to do the analysis and hypothesis testing, create a mapping to divide the audits into sub category and category according to the sub category and category provided in the question document. Clean the data before proceeding and calculate variables ABRET, ABVOL, CAR and CAAR according to the description provided.
Our Solution
Created a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.
Calculation of variable ABRET and ABVOL is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. Cleaning is done on the data by removing the repetitive entries from the dataset and then selected the data around the date for which the variable is to be calculated. Similarly, calculated ABVOL in which extracted the data around the annual report filing date and mean value for 40 days interval that ends 21 days before earning announcement dates.
Couldn’t proceed because dataset provided by the client was incomplete in order to calculate ABRET.
Language/techniques used
R language to create mapping for the key audit matters and save data set for question 1.
Python pandas library to deal with dates and extract data around annual report filing date.
Skills used
Data mapping, data cleaning, data manipulation, debugging
Databases used
Key audit matter
GDP rule law
Audit fee
Trading data
Earning date 
Report filing date
What are the technical Challenges Faced during Project Execution
Dataset provided by the client was too big and made my system slow when the data is loaded in the environment. Too many datasets and variables made it bit difficult to understand and time taking.
How the Technical Challenges were Solved
Calculated the number of unique identifiers in the large dataset and sorted those. Then selected the data for 1 unique identifier and sorted dates for it and append it to the dataframe and saved group of such unique identifiers to reduce the size of the dataset and performed the calculations in loop. 
To tackle the difficulty of understanding the data I made a document tracking all the columns or variables present in the data. 




Previous articleSplitting of Songs into its Vocals and InstrumentalNext articleLipsync Automation for Celebrities and Influencers Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2138,https://insights.blackcoffer.com/splitting-of-songs-into-its-vocals-and-instrumental/,Splitting of Songs into its Vocals and Instrumental,"

Client Background
Client: A leading Entertainment firm in the USA
Industry Type:  Entertainment
Services: Music
Organization Size: 100+
Project Objective
The objective of this project is to split a song into its vocals and instrumental.
Project Description
The project aims at taking a Hindi language song as input and separating the vocals(lyrics) from the instrumental music of the song. Save both the vocals and instrumental files separately as output.
Our Solution
I have used Python programming language for this project. The use of a Python library called Spleeter developed by Deezer has been made to achieve our goal.
Spleeter is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavor of separation :
Vocals (singing voice) / accompaniment separation (2 stems)Vocals / drums / bass / other separation (4 stems)Vocals / drums / bass / piano / other separation (5 stems)
2 stems and 4 stems models have high performance on the musdb dataset. Spleeter is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.
Project Deliverables
Python tool that takes Hindi song as input and gives two audio files as output: vocals file and instrumental file.
Language/techniques used
Python
Models used
2 Stems model
Skills used
Advanced Python programming
Project Snapshots 





Previous articleAI and ML technologies to Evaluate Learning AssessmentsNext articleKey Audit Matters Predictive Modeling Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2139,https://insights.blackcoffer.com/ai-and-ml-technologies-to-evaluate-learning-assessments/,AI and ML technologies to Evaluate Learning Assessments,"

Client Background
Client: A leading EduTech firm in the USA
Industry Type:  EduTech
Services: Educations. Training
Organization Size: 1000+
Project Objective
Confirmation / Identification of data that can be used / obtained without bias.Understanding the Actions that are required to be performed post analytics.Converting the data into metrics using formulae that can be used to conduct the analysis.  
Project Description
It is a culture management platform that uses learning as the fundamental mode of communication. The platform requires an Analytics portion that captures a variety of data related to the interaction of the learner with content, assessments, engagements and forums to create personalized learning plans for each user to increase the effectiveness of learning and its retention which together make an impact on the overall productivity of the learner and the organization.
Our Solution
We helped the client in deciding the data required for the analysis process. We came up with the appropriate models for various tasks and interpretations of how the data will be collected and analysed for the initial response, final response, retention, proficiency, and learning intent of the user. We designed the models in such a way that one can perform seamlessly grading for each question type (based on difficulty level) and at a different hierarchical level (sub-section, section, training, and so on). We knew that each user has its unique aptitude level (basic, intermediate, and advanced) and keeping that in my mind, we incorporated those aptitude levels in our analytics too. Moreover, we integrated the grade and time factor into the analysis so that more points are allotted for comparatively tough questions and quick responses, respectively.
Project Deliverables
MS Excel sheet, Google spreadsheets with proper tables and visualizations. 
Tools used
Jupyter notebook, MS Excel, Google Spreadsheets. 
Language/techniques used
Python.
Skills used
Data science and analytics.
Databases used
Generated our data through data simulation.
What are the technical Challenges Faced during Project Execution?
Data analytics is all about analysing and finding patterns in the data that already exist or are getting generated in real-time. However, this project is in the budding stage, and we had no data to start our analysis. Moreover, this project is novel, and the dataset that meets our requirements was nearly impossible to find online. 
How the Technical Challenges were Solved
We performed data simulation techniques and tried to generate the data as authentic as possible using some libraries in python and random functions in spreadsheets. We also generated the data manually at a small scale, but we made sure that we are including every human factor in it.
Project Snapshots (Minimum 10 Pictures)














Previous articleDatawarehouse, and Recommendations Engine for AirBNBNext articleSplitting of Songs into its Vocals and Instrumental Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2140,https://insights.blackcoffer.com/datawarehouse-and-recommendations-engine-for-airbnb/,"Datawarehouse, and Recommendations Engine for AirBNB","

Client Background
Client: A leading hotels chain in the USA
Industry Type:  Real Estate, Hospitality
Services: Hostpitality
Organization Size: 1000+
Project Objective
To download the data from the servers using Cyberduck on the daily basis and perform data engineering on it. 
Project Description
Firstly, download the property and forward files from the serverSecondly, From the property master file a new data set was created with the conditions that the Bedrooms from Property file should be 5 or more or Max Guests from Property File should be 16 or more and City from Property File should be Sevierville or Pigeon Forge or Gatlinburg.In the forward file only those with status = R were kept and the other data was removed.Finally, forward file was merged with the new data set on ‘Property ID’ i.e., keeping only those forward data with the common ‘Property ID’ and City, Bedrooms, Max Guests columns from the new dataset was added to the forward file.
Our Solution
We created a Python Script which performs the task and create property and forward master files, which we deliver to client on weekly basis.
Project Deliverables
Two csv files named property master file and forward master file to be delivered weekly after applying various steps.
Tools used
PyCharm, PowerBi, Cyberduck, Microsoft Excel.
Language/techniques used
Python Programming Language is used to create scripts performing Data Manipulation in different files.
Models used
SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.
We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.

Figure 1 SDLC Iterative Waterfall Model
Skills used
Skills such as Data Pre-processing, cleaning, and data manipulation are used in this project.
Databases used
We used traditional way of storing the data i.e file systems.
Web Cloud Servers used
Cyberduck, which is a libre server and cloud storage browser for Mac and Windows with support for FTP, SFTP, WebDAV, Amazon S3 etc, was used in this project with Amazon S3 servers.
What are the technical Challenges Faced during Project Execution?
Data to be processed was very big in size, so space complexity was a challenge in this project
How the Technical Challenges were Solved
To solve the space complexity issues, we tried PowerBi, but now time complexity arises. Then we did processing in chunks, by reducing file sizes to avoid memory errors.
Project Snapshots (Minimum 10 Pictures)

















Previous articleReal Estate Data WarehouseNext articleAI and ML technologies to Evaluate Learning Assessments Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2141,https://insights.blackcoffer.com/real-estate-data-warehouse/,Real Estate Data Warehouse,"

Client Background
Client: A leading Real Estate firm in the EU
Industry Type:  Real Estate
Services: Real Estate
Organization Size: 1000+
Project Objective
The objective of this project is to build a data warehouse from a website given search and filter criteria.
Project Description
The objective of this project is to collect data from a website given search and filter criteria.
Data Brief:
Crawl all the information for the property adverts once a week and store them in a database. Data language: English
Filters:
Federal States
Contains a list of the federal states in Germany to Crawl:
https://en.wikipedia.org/wiki/States_of_Germany
Categories to Crawl
Mieten Wohnung

Kaufen Wohnung

Kaufen Anlageobjekte

Kaufen Grundstück

Our Solution
We have developed a Python tool that crawls and scrapes all the apartment listings for all the states in Germany under each category namely: mieten wohnungen, kaufen wohnungen, kaufen anlageobjekte and kaufen grundstuck. The Scrapy library has been used to crawl and scrape. Beautiful soup could have also been used for the scraping purpose, but for the sake of consistency, Scrapy has been used for both purposes.
Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.
Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.
Four Spiders have been created for each category to be scraped. Every spider crawls all the states in Germany and scrapes all the apartment listings for important data. Every spider creates a separate JSON file to store all its data. This data is then converted to CSV using another python script called “conversion”.
The python tool has been completely automated and only needs the “Controller” script to be run. The script also has the capability of running every two weeks automatically. 
Project Deliverables
Four CSV files (one for each category):
Mieten Wohnungen.csv
Kaufen Wohnungen.csv
Kaufen Anlageobjekte.csv
Kaufen Grundstuck.csv
Language/techniques used
PythonWeb Crawling & Scraping
Skills used
Data ScrapingData CrawlingAdvanced Python programming
Project Snapshots











Previous articleTraction Dashboards of Marketing Campaigns and PostsNext articleDatawarehouse, and Recommendations Engine for AirBNB Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2142,https://insights.blackcoffer.com/traction-dashboards-of-marketing-campaigns-and-posts/,Traction Dashboards of Marketing Campaigns and Posts,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project ObjectiveFor the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used and also view the data according to Impressions and Likes? 
Project Description
We are testing AWS Comprehend. I performed a key phrase analysis of our LinkedIn posts. We have an output file. Now we need your help to visualize the data so that we can interpret it.I also have the original export file from LinkedIn. I want to answer this business question:For the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used?
I want to match up Engagement Rate with key phrase analysis. The business question is this: For the LinkedIn posts that received the highest engagement, what were the most common keywords, phrases and hashtags used?
Beyond matching to Engagement Rate, please check if there is a way to also view the data according to Impressions and Likes.
Our Solution
Data Driven Dashboards which will give the summary of Most used words, keywords, Phrases and also Analysis of Posts as per their interaction with their audience.
Project Deliverables
Two Dashboard Links in which 
First dashboard represents Key Phrase analysis of the output by AWS Comprehend.
Second Dashboard represents the Linked In data Analysis 
Tools used
Python, Google Data studio
Language/techniques used
Python 
Skills used
Python and Data Studio
Databases used
MongoDB
Web Cloud Servers used
Google Data Studio
What are the technical Challenges Faced during Project Execution
One of the major problem was to match the output of AWS Comprehend data with the data of excel sheet to find out which posts received maximum interactions and make a dashboard out of it.
How the Technical Challenges were Solved
Working on the output.json file in code editor and comparing it to the Linked In data sheet to check the accuracy of the output file with each post.
Project Snapshots (Minimum 10 Pictures)









Project website Url
1 Key Phrase Analysis Dashboard 
https://datastudio.google.com/reporting/efbabbff-55ba-4326-8133-78ae304aeb99
2 Linked IN Data Analysis Dashboard 
https://datastudio.google.com/reporting/3525e1c1-6c4f-4613-b260-d6e975fe1652





Previous articleGoogle Local Service Ads (LSA) Data WarehouseNext articleReal Estate Data Warehouse Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2143,https://insights.blackcoffer.com/google-local-service-ads-lsa-data-warehouse/,Google Local Service Ads (LSA) Data Warehouse,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Automated tool to extract daily review data from Local Service Ads dashboard for all clients.
Project Description
Extracts data from a company’s Google LSA page for the last 24 hours
The data is uploaded to the Bigquery database called “LSA_Review_db”.The script runs once a day and is deployed to Heroku by the name “lsa-daily-reviews”.The script runs for all companies in the Google sheet “LSA Review Automation master file”.The following data is uploaded:DateCompany NameLocationTotal ReviewsVerified ReviewsOverall StarReviewer NameReview DateReviewer StarReviewer Comment
Our Solution
Get list of companies to monitor along with their LSA URL
Use Selenium automated browsing to open the review page for each company.
Web scrape the data from the review page
Prepare report
Upload to database
Project Deliverables
An automated tool that runs daily and extracts and uploads review data for all companies.
Tools used
Selenium
Heroku
Sheets API
BigQuery
Language/techniques used
Python
Skills used
Data extraction, cleaning and summarising. Web scraping.
Databases used
BigQuery –  LSA_Review_db
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Using Selenium to automate web browsing since it takes a large amount of RAM.
How the Technical Challenges were Solved
Using the proper type of dynos and managing their allotment to lower both costs as well as memory usage.




Previous articleGoogle Local Service Ads Missed Calls and Messages Automation ToolNext articleTraction Dashboards of Marketing Campaigns and Posts Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2144,https://insights.blackcoffer.com/google-local-service-ads-missed-calls-and-messages-automation-tool/,Google Local Service Ads Missed Calls and Messages Automation Tool,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
A real time tool to send a report of missed calls and messages to the client.
Project Description
Extracts data from CallRail database for the last 5 minutes
All the calls which are marked as “missed” and all messages in the data are sent in the form of a report to the client.The script runs every 5 minutes and is deployed to Heroku by the name “missed-messages”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Company NameDateTimeCustomer NameContact No.Customer LocationCall TypeIn case of messages:Company NameDateTimeCustomer NameContact No.No. of messagesDirection (Inbound/Outbound)Content
Our Solution
To provide data real time, schedule the tool to check for data every 5 minutes.
Extract data from CallRail
Filter out all answered calls
Prepare report
Get email ids from sheets
Send email through SendGrid
Project Deliverables
An automated tool which provides real time updates to the client along with all information about the call.
Tools used
Heroku
CallRail API
SendGrid
Sheets API
Language/techniques used
Python
Skills used
Data extraction, cleaning and summarising
Databases used
Google Big Query
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Sending correct reports only to the companies which are active
How the Technical Challenges were Solved
Using Google Sheet’s cell formatting in Python




Previous articleMarketing Ads Leads Call Status Data Tool to BigQueryNext articleGoogle Local Service Ads (LSA) Data Warehouse Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2145,https://insights.blackcoffer.com/marketing-ads-leads-call-status-data-tool-to-bigquery/,Marketing Ads Leads Call Status Data Tool to BigQuery,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Prepare a daily report for the companies and upload it to BigQuery database. Data is from callrail and contains all call information about a company.
Project Description
Extracts data from CallRail database for the last 24 hoursThe data is uploaded to the Bigquery database called “Call_Status_From_CallRail”.The script runs once a day and is deployed to Heroku by the name “lsa-call-status-db”.The script runs for all companies in the CallRail database.The following data is uploaded:Company NameStatusLocationCustomer NameCall DateCall TimeContact NoCall StatusCall Lead
Our Solution
Use CallRail API to get data from database.
Run script daily
Filter out excess data
Prepare report
Upload to BigQuery
Project Deliverables
A working deployed automated tool that runs once a day in the morning hours and uploads the data to BigQuery database. Tool is monitored daily.
Tools used
Heroku
CallRail API
BigQuery
Sheets API
Language/techniques used
Python
Skills used
Data extraction, cleaning, and summarising
Databases used
BigQuery –  Call_Status_From_CallRail
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Ensuring proper data upload to database
How the Technical Challenges were Solved
Proper monitoring of tool post-deployment.




Previous articleMarketing Analytics to Automate Leads Call Status and ReportingNext articleGoogle Local Service Ads Missed Calls and Messages Automation Tool Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2146,https://insights.blackcoffer.com/marketing-analytics-to-automate-leads-call-status-and-reporting/,Marketing Analytics to Automate Leads Call Status and Reporting,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Prepare a daily report for the companies and upload it to Google Sheets. Data is from callrail and contains all call information about a company.
Project Description
Extracts data from CallRail database for the last 24 hours
The data is uploaded to the Google sheet “Call status record”The script runs once a day and is deployed to Heroku by the name “call-status-to-sheets”.The script runs for all companies in the CallRail database.The following data is uploaded:Company NameStatusLocationCustomer NameCall DateCall TimeContact NoCall StatusCall Lead
Our Solution
Use CallRail API to get data from database.
Run script daily
Filter out excess data
Prepare report
Upload to Google Sheets
Project Deliverables
A working deployed automated tool that runs once a day in the morning hours and uploads the data to Google Sheets. Tool is monitored daily.
Tools used
Heroku
CallRail API
BigQuery
Sheets API
Language/techniques used
Python
Skills used
Data extraction, cleaning and summarising
Databases used
Google Sheets –   Call status record
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Ensuring proper amendment of data to sheets without overwrite
How the Technical Challenges were Solved
Proper monitoring before final deployment




Previous articleCallRail, Analytics & Leads Report AlertNext articleMarketing Ads Leads Call Status Data Tool to BigQuery Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2147,https://insights.blackcoffer.com/callrail-analytics-leads-report-alert/,"CallRail, Analytics & Leads Report Alert","

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Prepare an annual report for the companies and upload it to database. Data is from callrail and contains call analytics.
Project Description
Extracts data from CallRail database for the last 1 year.The data is uploaded to BigQuery database “lead_report_alert_callrail”The script runs once a year and is deployed to Heroku by the name “lead-report-alert”.Currently, the script is programmed to run for only 2 companies (on a trial basis) – Capital Law Firm and Wilshire Law Firm.The following data is uploaded:Company NameNo. of calls answeredNo. of calls missedNo. of calls abandonedNo. of calls to voicemailTotal Calls
Our Solution
Use CallRail API to get data from database.
Set time window to be one year.
Filter out excess data
Prepare report
Upload to BigQuery
Project Deliverables
A working deployed automated tool that runs once a year in the morning hours and uploads the data to BigQuery. Tool is in prototype phase and hence is operational for 2 companies.
Tools used
Heroku
CallRail API
BigQuery
Language/techniques used
Python
Skills used
Data extraction, cleaning and summarising
Databases used
BigQuery –  lead_report_alert_callrail
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Working on a large amount of data since a year’s data contains hundred of thousands of records
How the Technical Challenges were Solved
Optimized code for faster processing.




Previous articleMarketing Tool to Notify Leads to Clients over Email and PhoneNext articleMarketing Analytics to Automate Leads Call Status and Reporting Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2148,https://insights.blackcoffer.com/marketing-automation-tool-to-notify-lead-details-to-clients-over-email-and-phone/,Marketing Tool to Notify Leads to Clients over Email and Phone,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Prepare a daily report for data from Local Service Ads dashboard and email to client.
Project Description
Extracts data from the LSA dashboard for the last 24 hours.
The data is sent to the client email in the form of a daily report using SendGrid.The script runs every morning and is deployed to Heroku by the name “lead-details-to-email”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Number of LeadsCost Per LeadLead TypeDispute amount to be approvedDispute amount approvedCost per Call
Our Solution
Use LSA API to extract data.Clean the data to make it readable and dispose the data not needed.Get the email id of each company from the given SheetSend an email to the client using SendGridDeploy to Heroku
Project Deliverables
A working deployed automated tool that runs everyday in the morning hours and sends a report to the client. Tool is monitored everyday.
Tools used
Heroku
LSA API
SendGrid
Sheets API
Language/techniques used
Python
Skills used
Data extraction, cleaning, and summarising
Databases used
Data is not stored and is sent directly to the client
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Ensuring a company’s data does not go to another company
How the Technical Challenges were Solved
Testing on multiple dummy email ids




Previous articleData ETL: Local Service Ads Leads to BigQueryNext articleCallRail, Analytics & Leads Report Alert Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2149,https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/,Data ETL: Local Service Ads Leads to BigQuery,"

Client Background
Client: A leading Marketing firm in the USA
Industry Type:  Marketing
Services: Marketing consulting
Organization Size: 100+
Project Objective
Upload daily data from Google Local Service Ads dashboard to BigQuery database.
Project Description
Extracts data from LSA dashboard for the last 24 hours.
The data is uploaded to BigQuery database “lsa_lead_daily_data”The script runs every morning and is deployed to Heroku by the name “lead-details-to-db”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Number of LeadsCost Per LeadLead TypeDispute amount to be approvedDispute amount approvedCost per Call
Our Solution
Use LSA API to extract data.Clean the data to make it readable and dispose the data not needed.Upload data to a BigQuery database everyday at a fixed time.Deploy to Heroku to run the script everyday.
Project Deliverables
A working deployed automated tool that runs everyday in the morning hours and uploads a report to database. Tool is monitored everyday. 
Tools used
Heroku
LSA API
BigQuery API
Sheets API
Language/techniques used
Python
Skills used
Data extraction, cleaning and summarising
Databases used
BigQuery –  lsa_lead_daily_data
Web Cloud Servers used
Heroku
What are the technical Challenges Faced during Project Execution
Making sure that the data uploaded is for the right company.
How the Technical Challenges were Solved
Monitoring daily logs and uploads for some time and making sure data was correct




Previous articleMarbles Stimulation using pythonNext articleMarketing Tool to Notify Leads to Clients over Email and Phone Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2150,https://insights.blackcoffer.com/marbles-stimulation-using-python/,Marbles Stimulation using python,"

Client Background
Client: A leading consulting firm in the USA
Industry Type: IT Consulting
Services: Consultanting
Organization Size: 100+
Project Objective
For all 4 cases, use a random number generator that will give you numbers between 1 & a million [1,000,000].  Whatever generator you use, make sure to adjust the numbers so that they are between 1 & 1,000,000 distributed randomly. 

For all tasks, we will have 5 colors, for example in Task 1, when the random number selected is between 1 & 5857 choose a bright color that is easily visible [I have called it Br. Clr. 1], for numbers between 5858 & 8678 choose another bright color [Br. Clr. 2], for numbers between 8679 & 11500 choose B (Blue), for numbers between 11501 & 50,000 choose R (Red), and > 50,000 choose G (Green). Simulate these 4 Task scenarios and represent them in a Table (1000 x 32) and collect statistics at the end. Replicate the simulation exercises for each Task with 3 different initial seed numbers. Likewise for 16 other Tasks.
Our Solution
Task involves creating 20 excel files running a Python Script in Jupyter Notebook which contains certain integer ranges indicating certain values and some other criteria the Random number range [1 to 1 million].
There are 20 Different tasks which have different conditions based on which need to form. Simulate these 20 Tasks and represent them in a Table (1000 x 32) and collect statistics at the end. Replicating the simulation exercises for each Task with 3 different initial seed numbers.Then using the Find and Replace tab of excel to make it in the correct format with proper color. Data Representation in particular format and formatting colors, Text based on condition passed within excel.
Project Deliverables
Excel File
Tools used
JupyterNBSublime TextMS Excel
Language/techniques used
Python 
Models used
No Software model is being Used to Solve this Project
Skills used
Python programmingMS Excel Formatting
Databases used
No database were used stored complete data in MS Excel
Web Cloud Servers used
No cloud servers were used for this project
What are the technical Challenges Faced during Project Execution
Formatting Excel Files 
How the Technical Challenges were Solved
Formatting Excel Files Discovered a lot of Shortcuts Available within Excel to deal with Data Representation in particular format and learned about formatting colors, Text based on condition passed within excel.Replication and Selecting Rows and Columns with shortcuts and in simplest way possible, transposing selected data and many more.
Project Snapshots 

  Figure 1: Sample Output File for Task 12 stimulation 3
In total there were 16 conditional tasks all of them had 3 stimulation which needed to be performed.





Previous articleStocktwits Data StructurizationNext articleData ETL: Local Service Ads Leads to BigQuery Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2151,https://insights.blackcoffer.com/stocktwits-data-structurization/,Stocktwits Data Structurization,"

Client Background
Client: A leading financial institution in the USA
Industry Type: Financial services & Consulting
Services: Financial consultant
Organization Size: 100+

Project Objective
>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).
>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.
>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.
>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.
> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.
Project Description
During the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.
Our Solution
Worked on Accessing Json Data, done tree Analysis on Json Sample data.
Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.
Created a list of all the chunked files of Json Data & Concat all the files in that list.
The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.
Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.
For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.
The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.
Project Deliverables
Categorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.
Tools used
● Jupyter Notebook
● Anaconda
● Notepad++
● Sublime Text
● Brackets
● JsonViewer
Language/techniques used
● Python Programming
Models used
My project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.
● Software Model : RAD(Rapid Application Development model) Model
● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.
● Advantages of RAD Model:
o Changing requirements can be accommodated.
o Progress can be measured.
o Iteration time can be short with use of powerful RAD tools.
o Productivity with fewer people in a short time.
o Reduced development time.
o Increases reusability of components.
o Quick initial reviews occur.
o Encourages customer feedback.
o Integration from very beginning solves a lot of integration issues
Skills used
● Data Mining
● Data Wrangling
● Data Visualization
● Python Programming including OOPs and Exception Handling
Databases used
No Databases were used, all the data was stored on Google Drive and Local Device.
Web Cloud Servers used
No Cloud Server were used
What are the technical Challenges Faced during Project Execution
● Handling Huge Data and Data Cleaning
● JSON Data Serialization.
● Solving Complex Nested JSON among the data provided.
How the Technical Challenges were Solved
● Handling Huge Data and Data Cleaning
Solved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.
● JSON Data Serialization
Solved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.
● Solving Complex Nested JSON among the data provided.
Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. 
Project Snapshots 

Figure 1 Sample Input Dataframe After Converting Outer JSON

Figure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing




Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2152,https://insights.blackcoffer.com/sentimental-analysis-on-shareholder-letter-of-companies/,Sentimental Analysis on Shareholder Letter of Companies,"

Client Background
Client: A leading financial firm in the USA
Industry Type: Financial services & Consulting
Services: Financial consultant
Organization Size: 100+
Project Objective
Project “Sentimental Analysis on Shareholder Letter of Companies” objective was to Predict the Sentiments columns Shareholder Letter in terms of Polarity and Subjectivity finally classification of data into positive, negative and neutral tone.
Project Description
The project ‘Sentimental Analysis on Shareholder Letter of US Companies’ task involved data cleaning on shareholder letters of different companies which includes lemmatization, lower case conversion, removing special character, \n , \t , punctuations, numbers & single character and tokenization. To generate polarity and subjectivity columns for the letter 1 & letter 2 columns using the Textblob library of NLTK. Based on the polarity categorizing it into positive, neutral  &  negative. 
Our Solution
Letter Text Length VariationContraction mapping on datasetReplacing missing value with some neutral tone string like None so that cleaning doesn’t generate any issue. Data Cleaning and Preprocessing which involves : 
            i.  Lemmatisation 
            ii. lower case conversion 
            iii.  Removing Special character 
            iv.  Removing \n , \t etc
             v.  remove punctuations, numbers & single character removal
            vi.  forming list of letter data using tqdm
Tokenization and word count.Used Textblob Library which is part of NLTK for Sentiment analysis.Created Polarity and Subjectivity column for the Letter1 & Letter2 columnsBased on the polarity of letter 1 created a letter1_type column with values “positive” , “neutral”  &  “negative” category.
Project Deliverables
Output iPython FilePreprocessed Dataset
Tools used
● Jupyter Notebook
● Anaconda
● Notepad++
● Sublime Text
● Brackets
● Python 3.4
Language/techniques used
PythonMachine LearningNLP (Natural Language Processing)
Models used
My project ‘Sentimental Analysis on Shareholder Letter of Companies’ developed with a software model which makes the project high quality, reliable and cost effective.
● Software Model : Waterfall Model
● For Project ‘Sentimental Analysis on Shareholder Letter of US Companies’ is a Waterfall Model as our model is not forming the loop from end to the start using Textblob which predicts Sentiments, Polarity and Subjectivity as the output following the Waterfall Model.
Skills used
Pandas OperationsData Chunking and IntegrationData Visualization
Databases used
No Database is used to complete this project.
Web Cloud Servers used
No Web cloud Server was required for this work.
What are the technical Challenges Faced during Project Execution
I have worked before on tasks similar to this so there were no challenges faced but the data cleaning was a bit different and required time to complete.
How the Technical Challenges were Solved
As Discussed no technical Challenges were faced during this project.
Project Snapshots 

Figure 1: Input Data Schema

Figure 2: Output Data Schema

Figure 3: Sample Input Dataset
figure 3 is pandas dataframe which was fetched from google cloud database there were 7 columns and 13290 rows.

Figure 4: Sample Output Dataset
figure 4 is output pandas dataframe after data cleaning and modeling of sentiment identification there are 13 columns and 13290 rows.

Figure 5: Sentiments assignment based on polarity
figure 5 represents the identification of sentiments and tone based on polarity and subjectivity. polarity>0 then sentiment type is positive,  if the polarity<0 sentiment type is negative and if the polarity=0 sentiment type is neutral.

Figure 6:  Histogram Representation of Length of Shareholder Letter 1
figure 6 is histogram plot between length of shareholder letter 1 among the final output dataset.

Figure 7:  Histogram Representation of Length of Shareholder Letter 2
figure 7 is Histogram plot between length of shareholder letter 2 among the final output dataset.

Figure 8: Flow Chart




Previous articlePopulation and Community Survey of AmericaNext articleHow is AI used to solve traffic management? Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2153,https://insights.blackcoffer.com/population-and-community-survey-of-america/,Population and Community Survey of America,"

Client Background
Client: A leading marketing firm in the USA
Industry Type: Marketing services & Consulting
Services: Marketing consultant
Organization Size: 100+
Project Objective
Project ‘Population and Community Survey of America’ objective were to perform Data Abstraction, Data Structurization, Data Preprocessing, Data Cleaning, and Combining Data from all the years listed and finally presenting insights of the data by Exploratory Data Analysis.
Project Description
For Project ‘Population and Community Survey of America’ task involved fetching json and unformatted csv data from numerous web links further needed to process data, handling nested JSON, data conversion of JSON data in dataframe, performing certain pandas operation for feature selection and structuring data. Concat all this data into one csv file then handle missing value by mapping with another dataset finally perform certain data visualization and exploratory data analysis.
Our Solution
Module 1: Data Abstraction
The process of data abstraction involves collecting data from numerous web links from Year 2005 to 2017 and viewing the data using JSON viewer in tree format.
Module 2: Data Chunking and Integration
Was unable to process data in pandas so performed data chunking with chunksize 10000 rows at a time for year 2005 likewise performed for all other years data till 2017 and finally combined all the dataframes into one containing all data from year 2005 to 2017.
Module 3: Handling Complexity of Nested Data & format the Unformatted CSV Files
Handling unformatted CSV in proper comma separated format so that data frame can be formed. Dataframe produced after merging for all the years from 2005 to 2017 contains a lot of nested JSON data among certain attributes so performed normalization of nested Json forming new_columns naming them based on their attributes key.
2.2.4 Module 4: Data Cleaning and Preprocessing
Involves handling missing value, contraction mapping with another dataset to fill the missing State_Zip_Code column, handling inf and -inf within the dataset for some attributes and forming a new column population_ratio based on passing formula among other attributes.
2.2.5 Module 5: Data Analysis
This step involves forming a correlation matrix to understand the relation between numeric attributes. performed Exploratory Data Analysis on strong correlated attributes to understand pattern/relation between them. 
Project Deliverables
After completion of Project we provided:
Final Preprocessed CSV Files Three iPython files: Preprocessed dataset from year 2010 to 2015Preprocessed dataset from year 2008 to 2017 Data Visualization and EDA.
Tools used
● Jupyter Notebook
● Anaconda
● Notepad++
● Sublime Text
● Brackets
● Python 3.4
● JSON Viewer
Language/techniques used
● Python
● ETL Techniques
● Advanced Excel Formatting 
Models used
My project ‘Population and Community Survey of America’ developed with a software model which makes the project high quality, reliable and cost effective.
● Software Model : RAD(Rapid Application Development model) Model
● This Project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.
● Advantages of RAD Model:
o Changing requirements can be accommodated.
o Progress can be measured.
o Iteration time can be short with use of powerful RAD tools.
o Productivity with fewer people in a short time.
o Reduced development time.
o Increases reusability of components.
o Quick initial reviews occur.
o Encourages customer feedback.
o Integration from very beginning solves a lot of integration issues
Skills used
Pandas OperationsData Chunking and IntegrationData VisualizationExploratory Data Analysis
Databases used
No Database is used in this project, only used Google Drive for Storing and Transferring Data.
Web Cloud Servers used
No Web Server is Used
What are the technical Challenges Faced during Project Execution
Data Cleaning and Filling out Missing Values by Data mapping with another dataset as the Data was not in proper format in the another dataset.
How the Technical Challenges were Solved
Data Cleaning was done using a few built in pandas operations to deal with Missing Values, Ordering Data Columns, Data Formatting, Changing of data types and many more. Filling of remaining Missing Data from columns using Outer Join among the datasets and using Map Function of Python.
Project Snapshots

  Figure 1: Input Data Schema for Year 2008

  Figure 2: Output Data Schema from Year 2005 to 2017

Figure 3: Dataset for Year 2008
figure 3 is pandas dataset of year 2008 which has 169595 rows and 25 columns which was fetched from authenticated survey web portal, data obtained were in JSON format which were converted into pandas dataframe likewise there are dataframes created from year 2005 to 2017.

Figure 4:  Output Preprocessed Dataset
figure 4 is an output preprocessed dataset from 2005 to 2017 which has 26,41,363 rows and 25 columns.

Figure 5: Describing Numeric Data of Preprocessed Dataset

Figure 6: Bar plot of attribute state_name
figure 6 represents the bar plot among the state_name on the final output dataset from year 2005 till 2017.

Figure 7: KDE Graph for all numeric population data column of dataset
figure 7 represents the Kernel Density Estimate Plot(KDE) among all Population estimate data columns for the Preprocessed Dataset. KDE plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions. Plotted many more graphs apart this between highly correlated attributes like pair plot, box plot, line plot etc.

  Figure 8: Flow Chart




Previous articleGoogle LSA API Data Automation and DashboardingNext articleSentimental Analysis on Shareholder Letter of Companies Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2154,https://insights.blackcoffer.com/google-lsa-api-data-automation-and-dashboarding/,Google LSA API Data Automation and Dashboarding,"

Client Background
Client: A leading marketing firm in the USA
Industry Type: Marketing services & Consulting
Services: Marketing consultant
Organization Size: 100+
Project Objective
For this project objective was to perform API Data Abstraction using Google LSA API in GCP, Automation of data fetching and storing in BigQuery on daily basis, Storing Historical data for all active companies, Fetching Customer Report then storing data on daily basis in BigQuery also storing Historical data for all companies, Perform Linear Regression Modelling on Historical data for all companies and storing the modeling Summary in google sheet in a structured manner, Basecamp Automation with LSA Daily Data, Creating 4 BI Dashboard in Data Studio for Live, Historical, Modelling and Customer Report data for all companies. 
Project Description
For this project task was to obtain an account report and detailed lead report for a specific dates and customer_id using Google Local Service Ads API Service in Google Cloud Platform. Further need to integrate with Google BigQuery database storing MCC data for all companies on a daily basis then storing Historical data for all active companies. Also notifying clients through email and passing messages containing daily account data in a message format to BaseCamp Message Board and Campfire of respective company projects through its API all with python programming, further deploying the script on Heroku Server for automating all this task. Then Creating BI Dashboard in Data Studio connecting with BigQuery and Creating Live Dashboard, Historical Dashboard for all companies. 
On historical data for all companies, Linear Regression Modelling needs to perform and to create Modelling Dashboard for all companies in Data Studio. Further needs to do  Exploratory Data Analysis for all companies on Historical Data. 
To Store Customer Account Report for message lead and phone lead on a daily basis, Script needs to be created and deployed in Heroku and also need to store Historical data for these companies and Finally Create Data Studio Dashboard on it.
Creating Sales Representation Dashboard for two Companies which involves multiple Reports and blending of multiple data sources from Big Query.
Our Solution
>> Module 1: API Data Abstraction
Which first includes generation of the access token and refresh token with the scope of Google AdWord API for the authentication and connecting with Google LSA API. Then fetching daily data in JSON format for particular account name based on customer_id assigned in API URL while fetching data. Likewise generating a script that would Handle data generation for all other active accounts based on their customer id.
>> Module 2: Data Imputation and Storing
Converting the JSON data to the pandas data frame forming a list of data frame for all the active accounts by looping them then deriving certain more attributes based on their handling the missing and inf values. Finally storing the data in Google Big Query database within the respective table for all accounts using Bigquery API.  
>> Module 3: Data Storing in BigQuery and Notification Automation
The task was to automate notifications sent to email and to Basecamp and the data transferred to the database on a daily basis by deploying the script to Heroku Server setting time parameters based on the New York time zone.
>> Module 4: Automation tools created till now:
i. LSA_AccountReport_daily_BigQuery tool: For Automation of Account Report for all companies on a daily basis. Scheduling it at 1:00 am in the Los Angeles Timezone.
ii. LSA_AccountReport_Historical_API tool:  For Storing Historical Data for companies for the last few Years till the end date which we set.
iii. Basecamp_lsa_automation: This is used to pass the lsa data in a message format to Campfire for respective companies groups and store lsa data combined for all companies to Messageboard and Campfire at one Automation Python Group in Basecamp.
iv. LSA_DateRange Tool: Used to store missed out data for all the companies for a few sets of days or months as per the need.
v. LSA_MainSheet_AutoUpdation tool: For Auto updation of main sheet  ‘LSA Client Lead’  Google Sheet. As Daily Data are fetched on the basis of this list so it is required to auto update this sheet for all the new companies entered would store information of those like company name, account id and database name.
vi. LSA_daily_CustomerReport tool: Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ on daily basis.
vii. Historical_LSA_CustomerReport tool:  Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ storing historical data for year 2021.
>> Module 5: Data Studio BI Dashboards Created:
i. Historical Dashboard
ii. Live Dashboard
ii. Customer Report Dashboard
iii. Modelling Report Dashboard
iv. Sales Representation Dashboard
Project Deliverables
Data Studio Dashboard Main SheetAll Codes for the Deployed tools and for Modelling EDA and Test Purpose .
Tools used
● PyCharm
● Jupyter Notebook
● Anaconda
● Heroku
● Notepad++
● Google Sheet API
● Google LSA API on GCP
● Google BigQuery
● Sublime Text
● Brackets
● JsonViewer
Language/techniques used
● Python
● SQL
Models used
My project ‘Google Adword LSA API Reports automation into Google Big Query database and Basecamp’ developed with a software model which makes the project high quality, reliable and cost-effective.
● Software Model: RAD(Rapid Application Development model) Model
● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.
● Advantages of RAD Model:
o Changing requirements can be accommodated.
o Progress can be measured.
o Iteration time can be short with the use of powerful RAD tools.
o Productivity with fewer people in a short time.
o Reduced development time.
o Increases reusability of components.
o Quick initial reviews occur.
o Encourages customer feedback.
o Integration from the very beginning solves a lot of integration issues
Skills used
● API Data Abstraction
● Data Mining and Statistical Modelling
● Data Wrangling
● Deployment for Automation
● Data Visualization
● SQL
● Machine Learning
● Python Programming including OOPs and Exception Handling
Databases used
● Google Firestore (Just for Testing Purpose)
● Google BigQuery
Web Cloud Servers used
Google BigQuery Cloud Database with up to 1 TB of free storage is being used. 
What are the technical Challenges Faced during Project Execution
● Scheduling Automation of Python Script.
● Data Exceptions and Duplication in BigQuery Tables.
● Refresh token Expiration After 7 Days.
● Data Exception due to Inactive companies or not Updation of LSA Main sheet. 
● Basecamp ProjectId Issue for transferring Data to multiple companies projects.
● Data Studio Time Series Plot data mismatch due to multiple account id.
How the Technical Challenges were Solved
● Scheduling Automation of Python Script.
Python Library BlockingScheduler were used and the Timezone variable ‘TZ’ was set to Los Angeles in Heroku
● Data Exceptions and Duplication in BigQuery Tables.
       Structuring SQL Query to deal with all the database issues which were being used in BigQuery to solve those issues.
● Refresh token Expiration after 7 Days.
Initially ‘Auth Playground’ was used for generating Refresh token which was getting expired after every 7 Days so to last it longer for more than a year we are now using the refresh token which was generated using Python script where proper token endpoints and many other headers were defined before generating the refresh token.
● Data Exception due to Inactive companies or not Updation of LSA Main sheet. 
Data Exception occurred while API data abstraction for few of the companies which were solved by adding more nested try and except statements after understanding issues also ‘LSA Clients Lead’ main sheet was not being updated by other members due to which we missed out data for few of the companies which were solved by creating script which will automatically update the mainsheet when an error occurred.
● Basecamp ProjectId Issue for transferring Data to multiple companies projects.
This issue was solved by creating Basecamp Main sheet where data was fetched now by mapping the account id of fetched data using LSA Main sheet and project id of all the basecamp companies.
● Data Studio Time Series Plot data mismatch due to multiple account id.
Solved by adding many parameters like setting the metrics which will do a summation of all the companies on a particular day for all the account id.





Previous articleHealthcare Data AnalysisNext articlePopulation and Community Survey of America Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2155,https://insights.blackcoffer.com/healthcare-data-analysis/,Healthcare Data Analysis,"

Client Background
Client: A leading healthcare tech firm in the USA
Industry Type: Healthcare Consulting
Services: Management consultant
Organization Size: 100+
Project Objective
The main objective of this project is to find the pattern in the vital signs of patients who were admitted to the hospital in past. And from this pattern, we get some ranges that help us to give early warnings.
Project Description
We are more interested in non-survivor patients’ vital signs as compare to survivor patients. we find patterns in vital signs that could better determine that patient died (ex. if Sp02 is below 70, patient in 95% of cases died, if Sp02 is below 50%, the death rate is 99.9%) or we can take correlations which can help us to find better patterns to define death cases.
Data The dataset which was used for analysis here is taken from the mimic website. But the dataset is not in the correct format which we want, after some manipulation, we get the data ready for the analysis.
Our Solution
Approach
To protect patient confidentiality date and time is shifted to future that’s not the actual time so from shifted time column we create an extra column hour which tells us the time passed in hours since first observation in ICU.After all manipulation our final dataset contain vital signs values for each observation of patients with time in separate column and also the label fo Death (0 or 1) in another column.There are two options to deal with missing valuesDrop all rows which contain null values.2.Fill the missing values by some method using pandas. 
I can’t go with 1st option because a major part of the data has missing values. so, I decided to go with the second option and fill missing values with the average of upper and lower values. But before that, I filtered the data and take only those patients’ data who died in a hospital or survive.
Project Deliverables
After performing EDA which also include the removal of some impossible outliers, we come up with a result of Analysis.
This result helps to build an early warning system which predict the condition of patients on the basis of their score, calculated on their condition using vital sign values.
Tools used
Google Colab Notebook
Language/techniques used
Python
Skills used
Data visualizationData analysisPandasNumpySeaborn
Databases used
SQL
MongoDB
Web Cloud Servers used
Google Cloud
Project Snapshots









Project website url
https://colab.research.google.com/drive/1mo7i32BoEVb0Ac6_CWwJd7_HVbliktx0?usp=sharing




Previous articleELK Stack – Elastic QueriesNext articleGoogle LSA API Data Automation and Dashboarding Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
bctech2156,https://insights.blackcoffer.com/budget-sales-kpi-dashboard-using-power-bi/,"Budget, Sales KPI Dashboard using Power BI","

Project Description
Weekly Data – clustered bar chart for weekly Budget & Actual value , weekly Total Budget & Actual value (completed)YTD Data – clustered bar chart for monthly Budget & Actual value , monthly Total Budget & Actual value (completed)Sales History – stacked chart for yearly sales with each month sales , total yearly sale (completed)Dashlet – weekly data – Total weekly Budget , Total weekly Actual , % weekly Budget (completed)Dashlet – YTD data – Total YTD Budget , Total YTD Actual , % YTD Budget (completed)Dashlet – Sales History – Total Sales (completed)Filters – select Area , select City , select Years (completed)

Data Visualization Deliverables
PresentationMapDashboardAPI Integration

Data Visualization Tools
KibanaGoogle Data StudioMicrosoft ExcelMicrosoft Power BI
Data Visualization Languages
JavaScriptSQLPythonDAX

Demo







Previous articleBenefits of Big Data in Different fieldsNext articleELK Stack – Elastic Queries Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

 



 

Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application 

 



 

Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment 

  

 
"
bctech2157,https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/,"Amazon Buy Bot, an Automation AI tool to Auto-Checkouts","

Client Background
Client: A leading consulting firm in the USA
Industry Type: Consulting
Services: Management consultant
Organization Size: 100+
Project Objective
The main objective of this project is to build the automation tool to buy product on amazon.
Project Description
This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium.
Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again.
Our Solution
A simple python code which uses selenium web driver to do all work.
Project Deliverables
Python Code
Tools used
Selenium Webdriver
Language/techniques used
Python
Skills used
Web Scraping
Selenium
Project Snapshots









Previous articlePredictive Modelling, AI, ML Dashboards in Power BINext articleHow does Big Data Help in Finance and the Growth of Large Firms? Ajay Bidyarthy  
RELATED ARTICLESMORE FROM AUTHOR




 

Healthcare AI ChatBot using LLAMA, LLM, Langchain 

 



 

AI Bot Audio to audio 

 



 

Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development 

  

 
"
